{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Master_Thesis-Algorithmic_Bias_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1xgO861smhhAVw-ALtedP9mnmWIFwVSvY",
      "authorship_tag": "ABX9TyODG5hwh4MpLC1AKzZNFxg8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NilsBahr94/Churn-Analysis-CRM/blob/master/Master_Thesis_Algorithmic_Bias_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LslOkLJNLSY-"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5GE201KpLiKY"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0J4SEBlzMik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install relevant libraries\n",
        "!pip install plotnine  \n",
        "!pip install pandas\n",
        "!pip install plotly\n",
        "!pip install datatable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MFZNjOwEsaEr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "23df0b22-1bf2-4400-acb2-60cc2027b6f6"
      },
      "source": [
        "# Basic Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotnine import *\n",
        "from pandas import DataFrame\n",
        "\n",
        "# Classifier Libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import collections\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\n",
        "# Import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Import accuracy_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Learning Curve\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "# Model Evaluation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "# Setup classifiers \n",
        "# a) Linear\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "# b) Nonlinear\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# c) Others\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Data Visualization with ggplot2\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from plotnine import *\n",
        "from plotnine.data import mpg\n",
        "\n",
        "# Model Evaluation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-kWQTpKzC0D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "ab5c4c5d-51fa-492e-82a9-7c8893001e63"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ89yoNTe1AE",
        "colab_type": "text"
      },
      "source": [
        "## Function Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KErf0aaNiQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# HYPERPARAMETER TUNING\n",
        "\n",
        "def hyperparameter_tuning(df_train_input, df_train_label, cv):\n",
        "\n",
        "  from sklearn import model_selection\n",
        "\n",
        "  # Create the hyperparameter grid\n",
        "  # Important: Keys in the dictionary must be valid hyperparameters \n",
        "  param_grid = {\"learning_rate\": [0.2],\n",
        "                \"n_estimators\": [50, 100, 150],\n",
        "                \"max_depth\": [3, 6, 9],\n",
        "                \"min_child_weight\": [1, 3, 5], \n",
        "                \"reg_lambda\": [1, 1.2]}\n",
        "\n",
        "  xgb_grid_search = XGBClassifier(objective= 'binary:logistic', nthread=4)\n",
        "\n",
        "  # Learning Curve for Slice \n",
        "  from sklearn.metrics import make_scorer\n",
        "  from sklearn.metrics import f1_score\n",
        "\n",
        "  # Define model\n",
        "  f1 = make_scorer(f1_score)\n",
        "\n",
        "  # Dummy Coding\n",
        "  df_train_input_dummy = pd.get_dummies(df_train_input)\n",
        "\n",
        "  grid_rf_class = sklearn.model_selection.GridSearchCV(estimator = xgb_grid_search,\n",
        "                                                      param_grid = param_grid, \n",
        "                                                      scoring= f1,\n",
        "                                                      n_jobs = 2, \n",
        "                                                      cv = cv,\n",
        "                                                      refit = True,\n",
        "                                                      return_train_score = False)\n",
        "  \n",
        "  # Fit model\n",
        "  grid_rf_class.fit(df_train_input_dummy, df_train_label)\n",
        "\n",
        "  print(grid_rf_class.best_params_)\n",
        "  print(grid_rf_class.best_estimator_) \n",
        "\n",
        "  return(grid_rf_class)\n",
        "\n",
        "def eda_descr_stats(data, disc_feature, disc_min_value, label, second_disc_feature=\"\"):\n",
        "\n",
        "  # 1. Sensitive Feature\n",
        "  print(f\"1. Sensitive Attribute: One or more of the following features are sensitive ones: {data.columns}.\")\n",
        "  print(f\"1. Sensitive Attribute: These are the individual values for the sensitive attribute: {data[disc_feature].unique()}.\")\n",
        "\n",
        "  # 2. Binary Target Feature\n",
        "  print(\"2. Binary Target Variable: The Binary Target Feature has the following values and counts:\")\n",
        "  print(data.groupby([label]).agg({label: 'count'}))\n",
        "\n",
        "  # 3. Total Number of Predictor Features\n",
        "  print(f\"3. The Total Number of Predictor Features is: {data.shape[1]}.\")\n",
        "\n",
        "  # 4. Total Number of Training Examples\n",
        "  print(f\"4. The Total Number of Training Examples is: {data.shape[0]}.\")\n",
        "\n",
        "  # 5. Total Number of Training Examples in the Minority Group \n",
        "  is_min = data[disc_feature].isin([disc_min_value])\n",
        "  print(f\"5. The Total Number of Training Examples in the Minority Group is: {len(data[is_min].index)}.\")\n",
        "\n",
        "  # 6. Sample Size Disparity\n",
        "  # Absolute number of members of different \"races\"\n",
        "  print(f\"6. Sample Size Disparity: The Absolute numbers of members of different races are as follows:  {data[disc_feature].value_counts(dropna=False, sort=True)}.\")\n",
        "  # Percentage of members of different \"races\"\n",
        "  print(f\"6. Sample Size Disparity: The Percentages of the number of members of different races are as follows: {data[disc_feature].value_counts(normalize=True, dropna=False, sort=True)}.\")\n",
        "\n",
        "  # 7. Class Balance\n",
        "  print(\"7. Class Balance: The Class Balance looks as follows:\")\n",
        "  print(data[label].value_counts(dropna=False))\n",
        "  print(data[label].value_counts(normalize=True, dropna=False))\n",
        "\n",
        "  # 8. Coarseness of Features\n",
        "  print(\"8. Coarseness of Features: Details on missing values of features in the dataset:\")\n",
        "  print(data.apply(lambda x: x.isna().sum()))\n",
        "  print(data.groupby(disc_feature).apply(lambda x: x.isna().sum()))\n",
        "  print(data.groupby(disc_feature).apply(lambda x: x.isna().mean()))\n",
        "\n",
        "  # 9. Severity of Outliers for Numeric Features\n",
        "  print(\"9. Severity of Outliers for Numeric Features\")\n",
        "  ax = sns.boxplot(data=data, orient=\"h\", palette=\"Set2\")\n",
        "  print(ax)\n",
        "\n",
        "  # (10. Cross-sectional sample size disparity)\n",
        "  if second_disc_feature:\n",
        "    print(\"10. Cross-sectional sample size disparity\")\n",
        "    data.groupby([second_disc_feature, disc_feature]).agg({label: 'count'})\n",
        "\n",
        "# Define Data Preprocessing Function\n",
        "\n",
        "def fair_preprocess(data, label, neg_class, pos_class):\n",
        "\n",
        "  ''' Applies binary encoding on the values of the label feature.\n",
        "\n",
        "  Returns two datasets in the following order: data_input_features and data_label. '''\n",
        "\n",
        "  # Binary encoding\n",
        "  data[label] = data[label].replace({neg_class: 0, pos_class: 1})\n",
        "\n",
        "  # Create separate dataset version for input features and label\n",
        "  data_input_features = data.drop(columns=[label])\n",
        "  data_label = data[label]\n",
        "\n",
        "  return data_input_features, data_label\n",
        "\n",
        "\n",
        "# Function to create datasets with different minority group sizes\n",
        "\n",
        "def create_datasets(min_data: pd.DataFrame, maj_data: pd.DataFrame, training_sizes: list):\n",
        "    datasets = []\n",
        "    for training_size in training_sizes:\n",
        "      if abs(training_size) <= len(min_data.index):\n",
        "        dataset_min_sample = min_data.sample(n=training_size)\n",
        "        dataset = pd.concat((dataset_min_sample, maj_data))\n",
        "        datasets.append(dataset)\n",
        "      else:\n",
        "        break\n",
        "    print(len(datasets))\n",
        "    print([df.shape[0] for df in datasets])\n",
        "    return datasets\n",
        "\n",
        "def metrics_to_df(list_dfs, label, model, cv, discr_feature, min_value, maj_value):\n",
        "\n",
        "  # Import relevant modules\n",
        "  import numpy as np\n",
        "  from sklearn.model_selection import cross_val_predict\n",
        "  from sklearn.model_selection import cross_validate\n",
        "  import sklearn.metrics\n",
        "  from sklearn.metrics import make_scorer\n",
        "  from sklearn.metrics import f1_score\n",
        "  from sklearn.metrics import recall_score\n",
        "  from sklearn.metrics import confusion_matrix\n",
        "  from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "  # Initialize lists for metrics \n",
        "  # COMPLETE\n",
        "  rows_compl_list = [] \n",
        "  f1_compl_list = []\n",
        "  f1_avg_train_score_list = []\n",
        "  tpr_compl_list = []\n",
        "  # MINORITY\n",
        "  rows_min_list = [] \n",
        "  f1_min_list = []\n",
        "  tpr_min_list = []\n",
        "  fpr_min_list = []\n",
        "  prob_y_1_min_list = []\n",
        "  # MAJORITY\n",
        "  rows_maj_list = []\n",
        "  f1_maj_list = []\n",
        "  tpr_maj_list = []\n",
        "  fpr_maj_list = []\n",
        "  prob_y_1_maj_list = []\n",
        "\n",
        "\n",
        "  for dataset_var in list_dfs:\n",
        "\n",
        "      # Define Input and target columns\n",
        "      df_train_input = dataset_var.drop(columns=[label])  # Input\n",
        "      df_train_label = dataset_var[label]                 # Target\n",
        "\n",
        "      # Apply dummy coding\n",
        "      df_train_input = pd.get_dummies(df_train_input)\n",
        "\n",
        "      ## TRAIN & TEST\n",
        "      # Predict \n",
        "      y_train_pred = cross_val_predict(model,\n",
        "                                       df_train_input,\n",
        "                                       df_train_label,\n",
        "                                       cv = cv)\n",
        "\n",
        "      # Append prediction labels to original dataset\n",
        "      dataset_var['y_pred'] = y_train_pred\n",
        "\n",
        "      # Create dataset for MINORITY group \n",
        "      is_black = dataset_var[discr_feature].isin([min_value])\n",
        "      df_check_black = dataset_var[is_black] \n",
        "\n",
        "      # Create dataset for MAJORITY group\n",
        "      is_white = dataset_var[discr_feature].isin([maj_value])\n",
        "      df_check_white = dataset_var[is_white] \n",
        "\n",
        "      ## METRICS\n",
        "      # Get metrics for the COMPLETE dataset\n",
        "      # NUMBER OF ROWS\n",
        "      rows_compl = len(dataset_var.index)\n",
        "      rows_compl_list.append(rows_compl)   \n",
        "      # F1 test scores\n",
        "      f1_compl = f1_score(df_train_label, y_train_pred) \n",
        "      f1_compl_list.append(f1_compl) \n",
        "      # F1 training scores\n",
        "      f1 = make_scorer(f1_score)\n",
        "      cross_val_results = cross_validate(estimator = model, \n",
        "                                         X = df_train_input, \n",
        "                                         y = df_train_label, \n",
        "                                         cv = cv, \n",
        "                                         scoring = f1, \n",
        "                                         return_train_score=True)\n",
        "      f1_avg_train_score = np.mean(cross_val_results[\"train_score\"])\n",
        "      f1_avg_train_score_list.append(f1_avg_train_score)\n",
        "      # Recall\n",
        "      tpr_compl = recall_score(df_train_label, y_train_pred)\n",
        "      tpr_compl_list.append(tpr_compl)\n",
        "\n",
        "      # Get metrics for the MINORITY group\n",
        "      # NUMBER OF ROWS\n",
        "      rows_min = len(df_check_black.index)\n",
        "      rows_min_list.append(rows_min)\n",
        "      # F1\n",
        "      f1_min = f1_score(df_check_black[label], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))  # or labels = [0,1]\n",
        "      f1_min_list.append(f1_min)\n",
        "      # TPR/RECALL\n",
        "      tpr_min = recall_score(df_check_black[label], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "      tpr_min_list.append(tpr_min)\n",
        "      # FPR/SPECIFICITY\n",
        "      tn_min, fp_min, fn_min, tp_min = confusion_matrix(df_check_black[label], df_check_black[\"y_pred\"], labels=[0,1]).ravel()\n",
        "      fpr_min = fp_min / (fp_min+tn_min)\n",
        "      fpr_min_list.append(fpr_min)\n",
        "      # Cond. Prob. P(y_min=1|minority)\n",
        "      filter_race_black_y_1 = dataset_var[discr_feature].isin([min_value]) & dataset_var[\"y_pred\"].isin([1])\n",
        "      filter_race_black = dataset_var[discr_feature].isin([min_value])\n",
        "      prob_y_1_min = len(dataset_var[filter_race_black_y_1].index) / len(dataset_var[filter_race_black].index)\n",
        "      prob_y_1_min_list.append(prob_y_1_min)\n",
        "\n",
        "      # Get metrics for the MAJORITY group\n",
        "      # NUMBER OF ROWS\n",
        "      rows_maj = len(df_check_white.index)\n",
        "      rows_maj_list.append(rows_maj)\n",
        "      # F1\n",
        "      f1_maj = f1_score(df_check_white[label], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "      f1_maj_list.append(f1_maj)\n",
        "      # TPR/RECALL\n",
        "      tpr_maj = recall_score(df_check_white[label], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"])) \n",
        "      tpr_maj_list.append(tpr_maj)\n",
        "      # FPR/SPECIFICITY\n",
        "      tn_maj, fp_maj, fn_maj, tp_maj = confusion_matrix(df_check_white[label], df_check_white[\"y_pred\"], labels=[0,1]).ravel()\n",
        "      fpr_maj = fp_maj / (fp_maj+tn_maj)\n",
        "      fpr_maj_list.append(fpr_maj)\n",
        "      # Cond. Prob. P(y_maj=1|majority)\n",
        "      filter_race_white_y_1 = dataset_var[discr_feature].isin([maj_value]) & dataset_var[\"y_pred\"].isin([1])\n",
        "      filter_race_white = dataset_var[discr_feature].isin([maj_value])\n",
        "      prob_race_white_y_1 = len(dataset_var[filter_race_white_y_1].index) / len(dataset_var[filter_race_white].index)\n",
        "      prob_y_1_maj_list.append(prob_race_white_y_1)\n",
        "\n",
        "  # Store metrics for different iterations in Data Frame\n",
        "  results_df = pd.DataFrame({'rows_complete': rows_compl_list,\n",
        "                             \"rows_minority\": rows_min_list,\n",
        "                             \"rows_majority\": rows_maj_list, \n",
        "                             'f1_complete': f1_compl_list,\n",
        "                             \"f1_complete_train\": f1_avg_train_score_list,\n",
        "                             'f1_minority': f1_min_list,\n",
        "                             'f1_majority': f1_maj_list,\n",
        "                             \"tpr_complete\": tpr_compl_list,\n",
        "                             'tpr_minority': tpr_min_list,\n",
        "                             \"tpr_majority\": tpr_maj_list,\n",
        "                             \"fpr_minority\": fpr_min_list,\n",
        "                             \"fpr_majority\": fpr_maj_list,\n",
        "                             \"prob_yhat_1_minority\": prob_y_1_min_list,\n",
        "                             \"prob_yhat_1_majority\": prob_y_1_maj_list})\n",
        "\n",
        "  # Calculate new metric columns and append to df \n",
        "  results_df[\"rel_share_min_of_maj\"] = (results_df[\"rows_minority\"] / results_df[\"rows_majority\"])\n",
        "  # FAIRNESS METRICS\n",
        "  # Average Absolute Odds Difference -> The closer to 0, the fairer.\n",
        "  results_df[\"aver_abs_odds_diff\"] = 0.5*(abs(results_df[\"fpr_minority\"] - results_df[\"fpr_majority\"])+abs(results_df[\"tpr_minority\"] - results_df[\"tpr_majority\"])) \n",
        "  # Statistical Parity Difference -> The closer to 0, the fairer.\n",
        "  results_df[\"stat_parity_diff\"] =  results_df[\"prob_yhat_1_minority\"] - results_df[\"prob_yhat_1_majority\"]\n",
        "  # Equal Opportunity Distance -> The closer to 0, the fairer.\n",
        "  results_df[\"equal_opport_dist\"] = results_df[\"tpr_minority\"] - results_df[\"tpr_majority\"]  \n",
        "  # Disparate Impact -> The closer to 1, the fairer.\n",
        "  results_df[\"disparate_impact\"] =  results_df[\"prob_yhat_1_minority\"] / results_df[\"prob_yhat_1_majority\"]\n",
        "\n",
        "  return(results_df)\n",
        "\n",
        "# Learning Curve https://www.kaggle.com/grfiv4/learning-curves-1\n",
        "\n",
        "import numpy  as np\n",
        "import pandas as pd\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, \n",
        "                        ylim=None, cv=None, \n",
        "                        scoring=None, obj_line=None,\n",
        "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    \n",
        "    \"\"\"\n",
        "    Generate a simple plot of the test and training learning curve.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    title : string\n",
        "        Title for the chart.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    ylim : tuple, shape (ymin, ymax), optional\n",
        "        Defines minimum and maximum yvalues plotted.\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "          - None, to use the default 3-fold cross-validation,\n",
        "          - integer, to specify the number of folds.\n",
        "          - An object to be used as a cross-validation generator.\n",
        "          - An iterable yielding train/test splits.\n",
        "\n",
        "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
        "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
        "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validators that can be used here.\n",
        "\n",
        "    scoring : string, callable or None, optional, default: None\n",
        "              A string (see model evaluation documentation)\n",
        "              or a scorer callable object / function with signature scorer(estimator, X, y)\n",
        "              For Python 3.5 the documentation is here:\n",
        "              http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
        "              For example, Log Loss is specified as 'neg_log_loss'\n",
        "              \n",
        "    obj_line : numeric or None (default: None)\n",
        "               draw a horizontal line \n",
        "               \n",
        "\n",
        "    n_jobs : integer, optional\n",
        "        Number of jobs to run in parallel (default 1).\n",
        "        \n",
        "        \n",
        "    Citation\n",
        "    --------\n",
        "        http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
        "        \n",
        "    Usage\n",
        "    -----\n",
        "        plot_learning_curve(estimator = best_estimator, \n",
        "                            title     = best_estimator_title, \n",
        "                            X         = X_train, \n",
        "                            y         = y_train, \n",
        "                            ylim      = (-1.1, 0.1), # neg_log_loss is negative\n",
        "                            cv        = StatifiedCV, # CV generator\n",
        "                            scoring   = scoring,     # eg., 'neg_log_loss'\n",
        "                            obj_line  = obj_line,    # horizontal line\n",
        "                            n_jobs    = n_jobs)      # how many CPUs\n",
        "\n",
        "         plt.show()\n",
        "    \"\"\"\n",
        "    \n",
        "    from sklearn.model_selection import learning_curve\n",
        "    import numpy as np\n",
        "    from matplotlib import pyplot as plt\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, scoring=scoring, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std  = np.std(train_scores, axis=1)\n",
        "    test_scores_mean  = np.mean(test_scores, axis=1)\n",
        "    test_scores_std   = np.std(test_scores, axis=1)\n",
        "    plt.grid()\n",
        "\n",
        "    # plt.style.use('seaborn')\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "             label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "             label=\"Cross-validation score\")\n",
        "\n",
        "    if obj_line:\n",
        "        plt.axhline(y=obj_line, color='blue')\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    return plt\n",
        "\n",
        "# Minority Line Chart - Function Definition\n",
        "\n",
        "def min_metrics_line_chart(metric_df, title):\n",
        "\n",
        "  import plotly.graph_objects as go\n",
        "  \n",
        "  # Create traces\n",
        "\n",
        "  # Performance Metrics\n",
        "  fig = go.Figure()\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"f1_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='F1 Minority'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"tpr_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='TPR/Recall Minority'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"fpr_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='FPR Minority'))\n",
        "  # Fairness Metrics\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"aver_abs_odds_diff\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='Avg. Abs. Odds Difference'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"stat_parity_diff\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='Statistical Parity Difference'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"equal_opport_dist\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='Equal Opportunity Distance'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"disparate_impact\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='Disparate Impact'))\n",
        "\n",
        "  # Edit the layout\n",
        "  fig.update_layout(title={'text': title,\n",
        "                           'y':0.9,\n",
        "                           'x':0.5,\n",
        "                           # 'xanchor': 'center',\n",
        "                           'yanchor': 'top'},\n",
        "                    xaxis_title='Rows Minority',\n",
        "                    yaxis_title='Metric Score', \n",
        "                    font=dict(size=14))\n",
        "\n",
        "  fig.show()\n",
        "\n",
        "# Majority <-> Minority Line Chart - Function Definition\n",
        "\n",
        "def maj_min_metrics_line_chart(metric_df, title):\n",
        "\n",
        "  import plotly.graph_objects as go\n",
        "\n",
        "  # Create traces\n",
        "\n",
        "  # Performance Metrics\n",
        "  fig = go.Figure()\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"f1_majority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='F1 Majority'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"f1_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='F1 Minority'))\n",
        "  # TPR\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"tpr_majority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='TPR/Recall Majority'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"tpr_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='TPR/Recall Minority'))\n",
        "  # FPR\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"fpr_majority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='FPR Majority'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"fpr_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='FPR Minority'))\n",
        "\n",
        "  # Edit the layout\n",
        "  fig.update_layout(title={'text': title,\n",
        "                           'y':0.9,\n",
        "                           'x':0.5,\n",
        "                           # 'xanchor': 'center',\n",
        "                           'yanchor': 'top'},\n",
        "                    xaxis_title='Rows Complete',\n",
        "                    yaxis_title='Metric Score', \n",
        "                    font=dict(size=14))\n",
        "\n",
        "  fig.show()\n",
        "\n",
        "def reduce_mem_usage(props):\n",
        "    start_mem_usg = props.memory_usage().sum() / 1024**2 \n",
        "    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n",
        "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
        "    for col in props.columns:\n",
        "        if props[col].dtype != object:  # Exclude strings\n",
        "            \n",
        "            # Print current column type\n",
        "            print(\"******************************\")\n",
        "            print(\"Column: \",col)\n",
        "            print(\"dtype before: \",props[col].dtype)\n",
        "            \n",
        "            # make variables for Int, max and min\n",
        "            IsInt = False\n",
        "            mx = props[col].max()\n",
        "            mn = props[col].min()\n",
        "            \n",
        "            # Integer does not support NA, therefore, NA needs to be filled\n",
        "            if not np.isfinite(props[col]).all(): \n",
        "                NAlist.append(col)\n",
        "                props[col].fillna(mn-1,inplace=True)  \n",
        "                   \n",
        "            # test if column can be converted to an integer\n",
        "            asint = props[col].fillna(0).astype(np.int64)\n",
        "            result = (props[col] - asint)\n",
        "            result = result.sum()\n",
        "            if result > -0.01 and result < 0.01:\n",
        "                IsInt = True\n",
        "\n",
        "            \n",
        "            # Make Integer/unsigned Integer datatypes\n",
        "            if IsInt:\n",
        "                if mn >= 0:\n",
        "                    if mx < 255:\n",
        "                        props[col] = props[col].astype(np.uint8)\n",
        "                    elif mx < 65535:\n",
        "                        props[col] = props[col].astype(np.uint16)\n",
        "                    elif mx < 4294967295:\n",
        "                        props[col] = props[col].astype(np.uint32)\n",
        "                    else:\n",
        "                        props[col] = props[col].astype(np.uint64)\n",
        "                else:\n",
        "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
        "                        props[col] = props[col].astype(np.int8)\n",
        "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
        "                        props[col] = props[col].astype(np.int16)\n",
        "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
        "                        props[col] = props[col].astype(np.int32)\n",
        "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
        "                        props[col] = props[col].astype(np.int64)    \n",
        "            \n",
        "            # Make float datatypes 32 bit\n",
        "            else:\n",
        "                props[col] = props[col].astype(np.float32)\n",
        "            \n",
        "            # Print new column type\n",
        "            print(\"dtype after: \",props[col].dtype)\n",
        "            print(\"******************************\")\n",
        "    \n",
        "    # Print final result\n",
        "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
        "    mem_usg = props.memory_usage().sum() / 1024**2 \n",
        "    print(\"Memory usage is: \",mem_usg,\" MB\")\n",
        "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n",
        "    return props, NAlist"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z0MC-9BGLofv"
      },
      "source": [
        "# Case Studies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66N6XRfQf1Pb",
        "colab_type": "text"
      },
      "source": [
        "## 0) Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xIamwLuVcSP",
        "colab_type": "text"
      },
      "source": [
        "#### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZJiBYwBVqOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "best_classifier_adult = hyperparameter_tuning(df_3_adult_train_input, df_3_adult_train_label, cv = 5)\n",
        "\n",
        "# Resulting Model\n",
        "\n",
        "# {'learning_rate': 0.3, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 100, 'reg_lambda': 1}\n",
        "# XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "#               colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "#               learning_rate=0.3, max_delta_step=0, max_depth=6,\n",
        "#               min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
        "#               nthread=None, objective='binary:logistic', random_state=0,\n",
        "#               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "#               silent=None, subsample=1, verbosity=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZJ5ojlvVm59",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "b7d1943e-8480-4f3f-f53b-5501733fcb66"
      },
      "source": [
        "best_classifier_compas = hyperparameter_tuning(df_compas_train_input, df_compas_train_label, cv = 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-947c25bc7585>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_classifier_compas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameter_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_compas_train_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_compas_train_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_compas_train_input' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BvyZkFRVn8H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "3ceaffda-7696-40ee-fd81-4771dddac835"
      },
      "source": [
        "best_classifier_homicide = hyperparameter_tuning(df_homicide_train_input, df_homicide_train_label, cv = 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TerminatedWorkerError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-24e7ab38772a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_classifier_homicide\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameter_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_homicide_train_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_homicide_train_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-ca97542c01a2>\u001b[0m in \u001b[0;36mhyperparameter_tuning\u001b[0;34m(df_train_input, df_train_label, cv)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0;31m# Fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m   \u001b[0mgrid_rf_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train_input_dummy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_rf_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker. The exit codes of the workers are {SIGKILL(-9)}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeGjO1NFVyDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_classifier_credit = hyperparameter_tuning(df_credit_train_input, df_credit_train_label, cv = 5)\n",
        "\n",
        "{'learning_rate': 0.3, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 50, 'reg_lambda': 1.2}\n",
        "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=3,\n",
        "              min_child_weight=1, missing=None, n_estimators=50, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1.2, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DTSOwjLd9IU",
        "colab_type": "text"
      },
      "source": [
        "#### Learning Curves: Compare diff. algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKTdADBDbp14",
        "colab_type": "text"
      },
      "source": [
        "Selection of classification method:\n",
        "\n",
        "- **Linear** machine learning algorithms often have a high bias but a low variance.\n",
        "    1. Logistic Regression\n",
        "    2. Linear Discriminant Analysis\n",
        "    3. Partial Least Squares Discriminant Analysis\n",
        "- **Nonlinear** machine learning algorithms often have a low bias but a high variance.\n",
        "    1. Nonlinear Discriminant Analysis\n",
        "    2. Neural Networks\n",
        "    3. Flexible Discriminant Analysis\n",
        "    4. Support Vector Machines \n",
        "    5. K-Nearest Neighbors\n",
        "    6. Naive Bayes\n",
        "- Others\n",
        "    1. Basic Classification Trees\n",
        "    2. Random Forest\n",
        "    3. Boosted Trees - XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHM2sG6QNt0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Setup classifiers \n",
        "# a) Linear\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "# b) Nonlinear\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# c) Others\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "\n",
        "# Define different classification algorithms\n",
        "random_forest = RandomForestClassifier(n_estimators = 100, max_leaf_nodes = 12)\n",
        "# knn = KNeighborsClassifier(n_neighbors = 5)\n",
        "log_reg = LogisticRegression()\n",
        "svm = SVC(C = 1.0, kernel = \"rbf\")\n",
        "\n",
        "# Store different models in a list\n",
        "models = [random_forest, log_reg, svm]\n",
        "\n",
        "# Define other input arguments\n",
        "f1 = make_scorer(f1_score) # theoretically, set (zero_division=1)\n",
        "sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300,\n",
        "             350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750,\n",
        "             2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000, 4250, 4500,\n",
        "             4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Plot learning curve for different classifcation algorithms\n",
        "for model in models:\n",
        "  plt.subplot\n",
        "  plot_learning_curve(estimator = model, \n",
        "                      title = f\"{model} Learning Curve\", \n",
        "                      X = df_3_adult_train_input, y = df_3_adult_train_label, \n",
        "                      cv = 10, \n",
        "                      scoring = f1, \n",
        "                      ylim = (0, 1), \n",
        "                      train_sizes = sizes)\n",
        "\n",
        "# Plot different subplots \n",
        "\n",
        "# for model, i in [(RandomForestClassifier(), 1), (KNeighborsClassifier(),2)]:\n",
        "#     plt.subplot(1,2,i)\n",
        "#     learning_curves(estimator = model, \n",
        "#                     data = df_3_adult_dummies, \n",
        "#                     features = df_3_adult_train_input.columns, \n",
        "#                     target= \"Over-50K\", \n",
        "#                     train_sizes = train_sizes, \n",
        "#                     cv= 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhsVjAHMqceC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def data_preprocess_fair(full_dataset, \n",
        "#                          outcome_feature, \n",
        "#                          privileged_outcome_label, \n",
        "#                          discriminatory_feature, \n",
        "#                          majority_label,\n",
        "#                          minority_label)\n",
        "\n",
        "\n",
        "## 1) Convert the labels of the target variable into a suitable  format (binary integer)\n",
        "# Encoding Binary \n",
        "df_3_adult[\"Over-50KVariables\"] = df_3_adult[\"Over-50K\"].apply(lambda val: 1 if val == \">50K\" \n",
        "                                                               else 0)\n",
        "\n",
        "# Check if encoding was successful \n",
        "df_3_adult[\"Over-50K\"].dtypes\n",
        "\n",
        "## 2. Prepare data for training (input features and label).\n",
        "# Features of complete dataset\n",
        "# print(df_3_adult.columns)\n",
        "\n",
        "# Input features\n",
        "df_3_adult_train_input = df_3_adult.drop(columns=[\"Over-50K\"])\n",
        "# print(df_3_adult_train_input.columns)\n",
        "\n",
        "# Target feature\n",
        "df_3_adult_train_label = df_3_adult[\"Over-50K\"]\n",
        "# print(df_3_adult_train_label)\n",
        "\n",
        "## 3) Prepare data as such that machine learning classification algorithm can handle that properly (e.g. standardization, normalization, feature scaling, dummy encoding)\n",
        "# Dummy \n",
        "df_3_adult_train_input = pd.get_dummies(df_3_adult_train_input)\n",
        "\n",
        "## 4) Filter dataset based on certain feature values which identify subpopulations and create different dataset versions based on that\n",
        "# Setup slices of the dataset\n",
        "is_black = df_3_adult[\"Race\"].isin([\"Black\"])\n",
        "is_white = df_3_adult[\"Race\"].isin([\"White\"])\n",
        "is_female = df_3_adult[\"Sex\"].isin([\"Female\"])\n",
        "is_male = df_3_adult[\"Sex\"].isin([\"Male\"])\n",
        "\n",
        "# Create filtered version of the dataset\n",
        "# Minority group\n",
        "df_3_adult_black = df_3_adult[is_black]\n",
        "df_3_adult_female = df_3_adult[is_female]\n",
        "# Majority group\n",
        "df_3_adult_white = df_3_adult[is_white]\n",
        "df_3_adult_male = df_3_adult[is_male]\n",
        "\n",
        "# print(df_3_adult.shape)\n",
        "# print(df_3_adult_black.shape)\n",
        "# print(df_3_adult_white.shape)\n",
        "# print(df_3_adult_female.shape)\n",
        "# print(df_3_adult_male.shape)\n",
        "\n",
        "\n",
        "  # \"\"\"\n",
        "  #   Convert the target label in an binary number format, create two different datasets sets \n",
        "\n",
        "  #   Parameters\n",
        "  #   ----------\n",
        "  #   estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "  #       An object of that type which is cloned for each validation.\n",
        "\n",
        "  #   title : string\n",
        "  #       Title for the chart.\n",
        "\n",
        "  #   X : array-like, shape (n_samples, n_features)\n",
        "  #       Training vector, where n_samples is the number of samples and\n",
        "  #       n_features is the number of features.\n",
        "\n",
        "  #   y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "  #       Target relative to X for classification or regression;\n",
        "  #       None for unsupervised learning.\n",
        "\n",
        "  #   ylim : tuple, shape (ymin, ymax), optional\n",
        "  #       Defines minimum and maximum yvalues plotted.\n",
        "\n",
        "  # \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxrRXcX700-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost\n",
        "from xgboost import XGBClassifier\n",
        "model = xgboost.XGBClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NdqX0VzxMCuy"
      },
      "source": [
        "## 1) US Adult Income Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CpPHz6cXJhv",
        "colab_type": "text"
      },
      "source": [
        "UCI Adult dataset, also known as \"Census Income\" dataset, contains information, extracted from the 1994 census data about people with attributes such as age, occupation, education, race, sex, marital-status, native-country, hours-per-week etc., indicating whether the income of a person exceeds $50K/yr or not. It can be used in fairness-related studies that want to compare gender or race inequalities based on people’s annual incomes, or various other studies [6]. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og1N6Co27mxY",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCNF_2o9_ALP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATA IMPORT\n",
        "\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# Set the path to the CSV containing the dataset to train on.\n",
        "csv_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "\n",
        "# Set the column names for the columns in the CSV. If the CSV's first line is a header line containing\n",
        "# the column names, then set this to None.\n",
        "csv_columns = [\"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital-Status\",\n",
        "               \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital-Gain\", \"Capital-Loss\",\n",
        "               \"Hours-per-week\", \"Country\", \"Over-50K\"]\n",
        "\n",
        "# Read the dataset from the provided CSV and print out information about it.\n",
        "df_3_adult = pd.read_csv(csv_path, names=csv_columns, skipinitialspace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q76uhuRlwiAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_3_adult.groupby([\"Over-50K\"]).agg({\"Over-50K\": 'count'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxt_BWxnmvKs",
        "colab_type": "text"
      },
      "source": [
        "### Initial Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqOxLfCdywjg",
        "colab_type": "text"
      },
      "source": [
        "Shrink the size of the Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX8QYrtg2oH_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "9b29eaa7-8516-4ebe-e562-3c1e29311a3d"
      },
      "source": [
        "df_3_adult.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Age                int64\n",
              "Workclass         object\n",
              "fnlwgt             int64\n",
              "Education         object\n",
              "Education-Num      int64\n",
              "Marital-Status    object\n",
              "Occupation        object\n",
              "Relationship      object\n",
              "Race              object\n",
              "Sex               object\n",
              "Capital-Gain       int64\n",
              "Capital-Loss       int64\n",
              "Hours-per-week     int64\n",
              "Country           object\n",
              "Over-50K          object\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoBkOfk12Vc_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "f3c02998-9401-4bbd-b9a5-7c6cad47d799"
      },
      "source": [
        "# Check which columns are suitable for conversion in \"category\" data format\n",
        "## We should stick to using the category type primarily for object columns where less than 50% of the values are unique.\n",
        "df_3_adult_copy = df_3_adult.select_dtypes(include=['object']).copy()\n",
        "df_3_adult_copy.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Workclass</th>\n",
              "      <th>Education</th>\n",
              "      <th>Marital-Status</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>Relationship</th>\n",
              "      <th>Race</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Country</th>\n",
              "      <th>Over-50K</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>32561</td>\n",
              "      <td>32561</td>\n",
              "      <td>32561</td>\n",
              "      <td>32561</td>\n",
              "      <td>32561</td>\n",
              "      <td>32561</td>\n",
              "      <td>32561</td>\n",
              "      <td>32561</td>\n",
              "      <td>32561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>9</td>\n",
              "      <td>16</td>\n",
              "      <td>7</td>\n",
              "      <td>15</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>42</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Private</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>22696</td>\n",
              "      <td>10501</td>\n",
              "      <td>14976</td>\n",
              "      <td>4140</td>\n",
              "      <td>13193</td>\n",
              "      <td>27816</td>\n",
              "      <td>21790</td>\n",
              "      <td>29170</td>\n",
              "      <td>24720</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Workclass Education      Marital-Status      Occupation Relationship  \\\n",
              "count      32561     32561               32561           32561        32561   \n",
              "unique         9        16                   7              15            6   \n",
              "top      Private   HS-grad  Married-civ-spouse  Prof-specialty      Husband   \n",
              "freq       22696     10501               14976            4140        13193   \n",
              "\n",
              "         Race    Sex        Country Over-50K  \n",
              "count   32561  32561          32561    32561  \n",
              "unique      5      2             42        2  \n",
              "top     White   Male  United-States    <=50K  \n",
              "freq    27816  21790          29170    24720  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5sRmQEk25ju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reduce the size of the numeric columns\n",
        "\n",
        "df_3_adult, NAlist = reduce_mem_usage(df_3_adult)\n",
        "print(\"_________________\")\n",
        "print(\"\")\n",
        "print(\"Warning: the following columns have missing values filled with 'df['column_name'].min() -1': \")\n",
        "print(\"_________________\")\n",
        "print(\"\")\n",
        "print(NAlist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFcFYqsb26SJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for col in [\"Workclass\", \"Education\",\"Marital-Status\",\"Occupation\",\"Relationship\",\"Race\",\"Sex\",\"Country\",\"Over-50K\"]:\n",
        "    df_3_adult[col] = df_3_adult[col].astype('category')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl76RtG5KyC4",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR2CNkgLlA-f",
        "colab_type": "text"
      },
      "source": [
        "1. Sensitive Feature \t\n",
        "  - Features are included which could lead to discrimination (e.g. Race, Sexual Orientation, Color, National Origin, Citizenship, Familiar Status, Pregnancy, Disability Status) and on the basis of which groups in the data can be assigned to the majority or minority group.  \n",
        "\n",
        "2. Binary Target Feature Y ∈ {0, 1}: \n",
        "  - The target variable should reflect allocative harms (= allocation of limited opportunities, resources, or information).\n",
        "\n",
        "3. Total Number of Predictor Features\t\n",
        "\n",
        "  - Number of predictor features on the basis of which a mapping to a target variable is carried out. Partly representative of the complexity of the prediction problem at hand.\n",
        "\n",
        "4. Total Number of Training Examples \t\n",
        "  - Total number of training examples in the dataset.\n",
        "\n",
        "5. Total Number of Training Examples in the Minority Group\n",
        "  - Number of training examples per value of a sensitive attribute.\n",
        "\n",
        "6. Sample Size Disparity\t\n",
        "  - Sample size disparity is represented by the difference between the relative share of the minority and the majority group.\n",
        "\n",
        "7. Class Balance\t\n",
        "  - Percentage of examples with 1 and 0 in the majority group, in the minority groupas well as in the complete dataset.\n",
        "\n",
        "8. Coarseness of Features\t\n",
        "  - Features might be less reliably collected for minority groups. \n",
        "  - Represented by percent of missing values for the subpopulations per feature for the examined training data interval \n",
        "\n",
        "9. Feature Importance\t\n",
        "  - The predictive power of the features represent how much contribution the features have brought in average in order to derive at the final predictions. \n",
        "  - The distribution of the predictive power of the features should be even for different datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffuJd_WpxZcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_3_adult.info)\n",
        "print(df_3_adult.describe())\n",
        "print(df_3_adult.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CierIp3nTNln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eda_descr_stats(data = df_3_adult, disc_feature= \"Race\", disc_min_value=\"Black\", label = \"Over-50K\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gvxB8Eg_SFP",
        "colab_type": "text"
      },
      "source": [
        "### Data Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzjd5IvaEJk-",
        "colab_type": "text"
      },
      "source": [
        "###### 0) Preprocess data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-HHPEI2vf-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute preprocessing function\n",
        "df_3_adult_train_input, df_3_adult_train_label = fair_preprocess(data = df_3_adult, \n",
        "                                                                 label = \"Over-50K\", \n",
        "                                                                 neg_class = \"<=50K\", \n",
        "                                                                 pos_class = \">50K\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3_u077HEWs0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check whether binary encoding was successful and seperate datasets were created\n",
        "print(df_3_adult.groupby([\"Over-50K\"]).agg({\"Over-50K\": 'count'}))\n",
        "print(df_3_adult_train_input.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY190l6ZkNN3",
        "colab_type": "text"
      },
      "source": [
        "###### 1) Define hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77-9BqfcHMLd",
        "colab_type": "text"
      },
      "source": [
        "Get Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8er2vO90UkZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_rf_class_adult = hyperparameter_tuning(df_train_input = df_3_adult_train_input, df_train_label= df_3_adult_train_label)\n",
        "\n",
        "print(grid_rf_class_adult.best_params_)\n",
        "print(grid_rf_class_adult.best_estimator_) # print in order to check actual final hyperparameters\n",
        "\n",
        "cv_results_df = pd.DataFrame(grid_rf_class_adult.cv_results_)\n",
        "best_row = cv_results_df[cv_results_df[\"rank_test_score\"] == 1]\n",
        "print(best_row)\n",
        "print(cv_results_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AAaPCPTImax",
        "colab_type": "text"
      },
      "source": [
        "Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU8Qe9FuUCHf",
        "colab_type": "text"
      },
      "source": [
        "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAzBhMm1Nx23",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "86e804cb-5b07-4ac7-a917-ea1b956f7d08"
      },
      "source": [
        "# HYPERPARAMETER TUNING - XGBoost\n",
        "\n",
        "from sklearn import model_selection\n",
        "\n",
        "param_grid = {\"learning_rate\": [0.3],\n",
        "              \"n_estimators\": [80],\n",
        "              \"max_depth\": [5], \n",
        "              \"min_child_weight\": [1], \n",
        "              \"reg_lambda\": [1]}\n",
        "\n",
        "xgb_grid_search = XGBClassifier(objective= 'binary:logistic', nthread=4)\n",
        "cv=3\n",
        "\n",
        "# Learning Curve for Slice \n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define model\n",
        "f1 = make_scorer(f1_score)\n",
        "\n",
        "# Dummy Coding\n",
        "df_3_adult_train_input = pd.get_dummies(df_3_adult_train_input)\n",
        "\n",
        "grid_rf_class = sklearn.model_selection.GridSearchCV(estimator = xgb_grid_search,\n",
        "                                                     param_grid = param_grid, \n",
        "                                                     scoring= f1, \n",
        "                                                     cv = cv,\n",
        "                                                     refit = True,\n",
        "                                                     return_train_score = False)\n",
        "\n",
        "# Fit model\n",
        "grid_rf_class.fit(df_3_adult_train_input, df_3_adult_train_label) \n",
        "\n",
        "print(grid_rf_class.best_params_)\n",
        "print(grid_rf_class.best_estimator_) # print in order to check actual final hyperparameters\n",
        "\n",
        "# cv_results_df = pd.DataFrame(grid_rf_class.cv_results_)\n",
        "# best_row = cv_results_df[cv_results_df[\"rank_test_score\"] == 1]\n",
        "# print(best_row)\n",
        "# print(cv_results_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'learning_rate': 0.3, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 80, 'reg_lambda': 0.9}\n",
            "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
            "              learning_rate=0.3, max_delta_step=0, max_depth=5,\n",
            "              min_child_weight=1, missing=None, n_estimators=80, n_jobs=1,\n",
            "              nthread=4, objective='binary:logistic', random_state=0,\n",
            "              reg_alpha=0, reg_lambda=0.9, scale_pos_weight=1, seed=None,\n",
            "              silent=None, subsample=1, verbosity=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njeCn37qQpS-",
        "colab_type": "text"
      },
      "source": [
        "###### 2) Create datasets with diff. minority group sizes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTkqDkW4QrWV",
        "colab_type": "text"
      },
      "source": [
        "First, take the whole dataset with the majority and the minority class. Then, filter the dataset such that the majority class is fixed in size and only a certain number of training examples that are considered for the minority class is considered. Change this number step-by-step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGE2KGthLrrm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "2617740d-591a-4de4-cd50-1e0cc1a023bb"
      },
      "source": [
        "# Determine number of training examples for the examples that are at risk of being discriminated\n",
        "print(df_3_adult.groupby(['Race', \"Over-50K\"]).agg({\"Over-50K\": 'count'}))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                             Over-50K\n",
            "Race               Over-50K          \n",
            "Amer-Indian-Eskimo 0              275\n",
            "                   1               36\n",
            "Asian-Pac-Islander 0              763\n",
            "                   1              276\n",
            "Black              0             2737\n",
            "                   1              387\n",
            "Other              0              246\n",
            "                   1               25\n",
            "White              0            20699\n",
            "                   1             7117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-GcZxMQEaxl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "aee8ff42-7467-4495-d801-5a317cea0fd4"
      },
      "source": [
        "# Define arguments\n",
        "\n",
        "training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Creating dfs for minority and majority group\n",
        "is_black = df_3_adult[\"Race\"].isin([\"Black\"])\n",
        "is_white = df_3_adult[\"Race\"].isin([\"White\"])\n",
        "df_3_adult_black = df_3_adult[is_black]  # Minority\n",
        "df_3_adult_white = df_3_adult[is_white]  # Majority\n",
        "\n",
        "# Execute function\n",
        "list_dfs_adult = create_datasets(min_data = df_3_adult_black, maj_data = df_3_adult_white, training_sizes = training_sizes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31\n",
            "[27821, 27826, 27836, 27846, 27856, 27866, 27891, 27916, 27941, 27966, 27991, 28016, 28066, 28116, 28166, 28216, 28266, 28316, 28416, 28516, 28616, 28716, 28816, 29066, 29316, 29566, 29816, 30066, 30316, 30566, 30816]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nyn0FoHBJmAm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "06f6e988-ba95-49c1-cba5-bea9938ae245"
      },
      "source": [
        "df_test = list_dfs_adult[15]\n",
        "print(df_test.groupby([\"Over-50K\"]).agg({\"Over-50K\": 'count'}))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          Over-50K\n",
            "Over-50K          \n",
            "0            21046\n",
            "1             7170\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDkzaawcuCKZ",
        "colab_type": "text"
      },
      "source": [
        "###### 3) Create dataframe with diff. metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBSkMj5qSUX_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c65ed8d-383c-4002-b6d5-735a7d4c90d1"
      },
      "source": [
        "# Define arguments\n",
        "list_dfs = list_dfs_adult\n",
        "label = \"Over-50K\"\n",
        "adult_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=5,\n",
        "              min_child_weight=1, missing=None, n_estimators=80, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "\n",
        "# model = grid_rf_class # <- Best model from hyperparameter tuning\n",
        "cv = 10\n",
        "discr_feature = \"Race\"\n",
        "min_value = \"Black\"\n",
        "maj_value = \"White\"\n",
        "\n",
        "# Execute function\n",
        "results_df_adult = metrics_to_df(list_dfs = list_dfs_adult, label = label, model = adult_model, cv = cv, \n",
        "                                 discr_feature = discr_feature, min_value = min_value, maj_value = maj_value)\n",
        "\n",
        "results_df_adult"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning:\n",
            "\n",
            "F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning:\n",
            "\n",
            "F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rows_complete</th>\n",
              "      <th>rows_minority</th>\n",
              "      <th>rows_majority</th>\n",
              "      <th>f1_complete</th>\n",
              "      <th>f1_complete_train</th>\n",
              "      <th>f1_minority</th>\n",
              "      <th>f1_majority</th>\n",
              "      <th>tpr_complete</th>\n",
              "      <th>tpr_minority</th>\n",
              "      <th>tpr_majority</th>\n",
              "      <th>fpr_minority</th>\n",
              "      <th>fpr_majority</th>\n",
              "      <th>prob_yhat_1_minority</th>\n",
              "      <th>prob_yhat_1_majority</th>\n",
              "      <th>rel_share_min_of_maj</th>\n",
              "      <th>aver_abs_odds_diff</th>\n",
              "      <th>stat_parity_diff</th>\n",
              "      <th>equal_opport_dist</th>\n",
              "      <th>disparate_impact</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>27821</td>\n",
              "      <td>5</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.714187</td>\n",
              "      <td>0.756365</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.714187</td>\n",
              "      <td>0.653225</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.653225</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.060534</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.212180</td>\n",
              "      <td>0.000180</td>\n",
              "      <td>0.356879</td>\n",
              "      <td>-0.212180</td>\n",
              "      <td>-0.653225</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>27826</td>\n",
              "      <td>10</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.711844</td>\n",
              "      <td>0.760895</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.711844</td>\n",
              "      <td>0.649431</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.649431</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.060244</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.210994</td>\n",
              "      <td>0.000360</td>\n",
              "      <td>0.354838</td>\n",
              "      <td>-0.210994</td>\n",
              "      <td>-0.649431</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>27836</td>\n",
              "      <td>20</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.713683</td>\n",
              "      <td>0.756079</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.713596</td>\n",
              "      <td>0.653884</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.653787</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.061404</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.212971</td>\n",
              "      <td>0.000719</td>\n",
              "      <td>0.203809</td>\n",
              "      <td>-0.112971</td>\n",
              "      <td>0.346213</td>\n",
              "      <td>0.469548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>27846</td>\n",
              "      <td>30</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.712046</td>\n",
              "      <td>0.757311</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.712077</td>\n",
              "      <td>0.653609</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.653646</td>\n",
              "      <td>0.040000</td>\n",
              "      <td>0.062660</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.213870</td>\n",
              "      <td>0.001079</td>\n",
              "      <td>0.038153</td>\n",
              "      <td>-0.080536</td>\n",
              "      <td>-0.053646</td>\n",
              "      <td>0.623433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>27856</td>\n",
              "      <td>40</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.712786</td>\n",
              "      <td>0.756766</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.712917</td>\n",
              "      <td>0.653096</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.653365</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.061742</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.213115</td>\n",
              "      <td>0.001438</td>\n",
              "      <td>0.190887</td>\n",
              "      <td>-0.163115</td>\n",
              "      <td>-0.320032</td>\n",
              "      <td>0.234615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>27866</td>\n",
              "      <td>50</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.710958</td>\n",
              "      <td>0.757652</td>\n",
              "      <td>0.842105</td>\n",
              "      <td>0.710768</td>\n",
              "      <td>0.653150</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.652944</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.063385</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.214229</td>\n",
              "      <td>0.001798</td>\n",
              "      <td>0.092721</td>\n",
              "      <td>-0.034229</td>\n",
              "      <td>0.147056</td>\n",
              "      <td>0.840222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>27891</td>\n",
              "      <td>75</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.707116</td>\n",
              "      <td>0.758356</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.707108</td>\n",
              "      <td>0.648470</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.648588</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.063916</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.213510</td>\n",
              "      <td>0.002696</td>\n",
              "      <td>0.078474</td>\n",
              "      <td>-0.146844</td>\n",
              "      <td>-0.093032</td>\n",
              "      <td>0.312241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>27916</td>\n",
              "      <td>100</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.708273</td>\n",
              "      <td>0.758556</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>0.708231</td>\n",
              "      <td>0.648008</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.648026</td>\n",
              "      <td>0.011236</td>\n",
              "      <td>0.062563</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>0.212360</td>\n",
              "      <td>0.003595</td>\n",
              "      <td>0.031495</td>\n",
              "      <td>-0.132360</td>\n",
              "      <td>-0.011662</td>\n",
              "      <td>0.376719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>27941</td>\n",
              "      <td>125</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.709747</td>\n",
              "      <td>0.754835</td>\n",
              "      <td>0.685714</td>\n",
              "      <td>0.709811</td>\n",
              "      <td>0.648164</td>\n",
              "      <td>0.705882</td>\n",
              "      <td>0.648026</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0.061162</td>\n",
              "      <td>0.144000</td>\n",
              "      <td>0.211317</td>\n",
              "      <td>0.004494</td>\n",
              "      <td>0.031732</td>\n",
              "      <td>-0.067317</td>\n",
              "      <td>0.057856</td>\n",
              "      <td>0.681440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>27966</td>\n",
              "      <td>150</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.711309</td>\n",
              "      <td>0.755391</td>\n",
              "      <td>0.608696</td>\n",
              "      <td>0.711490</td>\n",
              "      <td>0.651235</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.651258</td>\n",
              "      <td>0.035971</td>\n",
              "      <td>0.061694</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>0.212540</td>\n",
              "      <td>0.005393</td>\n",
              "      <td>0.020308</td>\n",
              "      <td>-0.132540</td>\n",
              "      <td>-0.014894</td>\n",
              "      <td>0.376401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>27991</td>\n",
              "      <td>175</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.711947</td>\n",
              "      <td>0.755718</td>\n",
              "      <td>0.717949</td>\n",
              "      <td>0.711929</td>\n",
              "      <td>0.652186</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>0.651960</td>\n",
              "      <td>0.038462</td>\n",
              "      <td>0.061742</td>\n",
              "      <td>0.114286</td>\n",
              "      <td>0.212755</td>\n",
              "      <td>0.006291</td>\n",
              "      <td>0.054081</td>\n",
              "      <td>-0.098470</td>\n",
              "      <td>0.084882</td>\n",
              "      <td>0.537170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>28016</td>\n",
              "      <td>200</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.711145</td>\n",
              "      <td>0.754935</td>\n",
              "      <td>0.578947</td>\n",
              "      <td>0.711531</td>\n",
              "      <td>0.650413</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.650696</td>\n",
              "      <td>0.038889</td>\n",
              "      <td>0.061307</td>\n",
              "      <td>0.090000</td>\n",
              "      <td>0.212108</td>\n",
              "      <td>0.007190</td>\n",
              "      <td>0.061557</td>\n",
              "      <td>-0.122108</td>\n",
              "      <td>-0.100696</td>\n",
              "      <td>0.424312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>28066</td>\n",
              "      <td>250</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.709510</td>\n",
              "      <td>0.754842</td>\n",
              "      <td>0.592593</td>\n",
              "      <td>0.709995</td>\n",
              "      <td>0.648985</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.649290</td>\n",
              "      <td>0.045045</td>\n",
              "      <td>0.061790</td>\n",
              "      <td>0.104000</td>\n",
              "      <td>0.212108</td>\n",
              "      <td>0.008988</td>\n",
              "      <td>0.047304</td>\n",
              "      <td>-0.108108</td>\n",
              "      <td>-0.077862</td>\n",
              "      <td>0.490316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>28116</td>\n",
              "      <td>300</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.709771</td>\n",
              "      <td>0.757064</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.710736</td>\n",
              "      <td>0.649665</td>\n",
              "      <td>0.384615</td>\n",
              "      <td>0.651117</td>\n",
              "      <td>0.022989</td>\n",
              "      <td>0.062274</td>\n",
              "      <td>0.070000</td>\n",
              "      <td>0.212935</td>\n",
              "      <td>0.010785</td>\n",
              "      <td>0.152893</td>\n",
              "      <td>-0.142935</td>\n",
              "      <td>-0.266502</td>\n",
              "      <td>0.328739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>28166</td>\n",
              "      <td>350</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.710894</td>\n",
              "      <td>0.756823</td>\n",
              "      <td>0.634921</td>\n",
              "      <td>0.711261</td>\n",
              "      <td>0.651189</td>\n",
              "      <td>0.606061</td>\n",
              "      <td>0.651398</td>\n",
              "      <td>0.031546</td>\n",
              "      <td>0.061984</td>\n",
              "      <td>0.085714</td>\n",
              "      <td>0.212791</td>\n",
              "      <td>0.012583</td>\n",
              "      <td>0.037888</td>\n",
              "      <td>-0.127077</td>\n",
              "      <td>-0.045337</td>\n",
              "      <td>0.402809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>28216</td>\n",
              "      <td>400</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.713850</td>\n",
              "      <td>0.757444</td>\n",
              "      <td>0.744186</td>\n",
              "      <td>0.713650</td>\n",
              "      <td>0.653964</td>\n",
              "      <td>0.680851</td>\n",
              "      <td>0.653787</td>\n",
              "      <td>0.019830</td>\n",
              "      <td>0.061356</td>\n",
              "      <td>0.097500</td>\n",
              "      <td>0.212935</td>\n",
              "      <td>0.014380</td>\n",
              "      <td>0.034295</td>\n",
              "      <td>-0.115435</td>\n",
              "      <td>0.027064</td>\n",
              "      <td>0.457886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>28266</td>\n",
              "      <td>450</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.714112</td>\n",
              "      <td>0.756619</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.714636</td>\n",
              "      <td>0.655374</td>\n",
              "      <td>0.589286</td>\n",
              "      <td>0.655894</td>\n",
              "      <td>0.032995</td>\n",
              "      <td>0.061790</td>\n",
              "      <td>0.102222</td>\n",
              "      <td>0.213798</td>\n",
              "      <td>0.016178</td>\n",
              "      <td>0.047702</td>\n",
              "      <td>-0.111576</td>\n",
              "      <td>-0.066609</td>\n",
              "      <td>0.478126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>28316</td>\n",
              "      <td>500</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.709829</td>\n",
              "      <td>0.757761</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>0.709593</td>\n",
              "      <td>0.651344</td>\n",
              "      <td>0.677419</td>\n",
              "      <td>0.651117</td>\n",
              "      <td>0.022831</td>\n",
              "      <td>0.063288</td>\n",
              "      <td>0.104000</td>\n",
              "      <td>0.213690</td>\n",
              "      <td>0.017975</td>\n",
              "      <td>0.033380</td>\n",
              "      <td>-0.109690</td>\n",
              "      <td>0.026302</td>\n",
              "      <td>0.486686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>28416</td>\n",
              "      <td>600</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.708037</td>\n",
              "      <td>0.758297</td>\n",
              "      <td>0.597015</td>\n",
              "      <td>0.709174</td>\n",
              "      <td>0.650535</td>\n",
              "      <td>0.540541</td>\n",
              "      <td>0.651679</td>\n",
              "      <td>0.038023</td>\n",
              "      <td>0.064013</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.214373</td>\n",
              "      <td>0.021570</td>\n",
              "      <td>0.068564</td>\n",
              "      <td>-0.114373</td>\n",
              "      <td>-0.111139</td>\n",
              "      <td>0.466477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>28516</td>\n",
              "      <td>700</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.712385</td>\n",
              "      <td>0.756626</td>\n",
              "      <td>0.650307</td>\n",
              "      <td>0.713159</td>\n",
              "      <td>0.653761</td>\n",
              "      <td>0.595506</td>\n",
              "      <td>0.654489</td>\n",
              "      <td>0.034370</td>\n",
              "      <td>0.062225</td>\n",
              "      <td>0.105714</td>\n",
              "      <td>0.213762</td>\n",
              "      <td>0.025165</td>\n",
              "      <td>0.043419</td>\n",
              "      <td>-0.108048</td>\n",
              "      <td>-0.058984</td>\n",
              "      <td>0.494542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>28616</td>\n",
              "      <td>800</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.712838</td>\n",
              "      <td>0.756228</td>\n",
              "      <td>0.662983</td>\n",
              "      <td>0.713530</td>\n",
              "      <td>0.653089</td>\n",
              "      <td>0.594059</td>\n",
              "      <td>0.653927</td>\n",
              "      <td>0.028612</td>\n",
              "      <td>0.061549</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.213115</td>\n",
              "      <td>0.028760</td>\n",
              "      <td>0.046402</td>\n",
              "      <td>-0.113115</td>\n",
              "      <td>-0.059868</td>\n",
              "      <td>0.469231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>28716</td>\n",
              "      <td>900</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.711278</td>\n",
              "      <td>0.757091</td>\n",
              "      <td>0.716981</td>\n",
              "      <td>0.711186</td>\n",
              "      <td>0.654128</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.653927</td>\n",
              "      <td>0.027990</td>\n",
              "      <td>0.063626</td>\n",
              "      <td>0.108889</td>\n",
              "      <td>0.214661</td>\n",
              "      <td>0.032355</td>\n",
              "      <td>0.024188</td>\n",
              "      <td>-0.105772</td>\n",
              "      <td>0.012739</td>\n",
              "      <td>0.507261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>28816</td>\n",
              "      <td>1000</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.710220</td>\n",
              "      <td>0.754031</td>\n",
              "      <td>0.669811</td>\n",
              "      <td>0.710874</td>\n",
              "      <td>0.653448</td>\n",
              "      <td>0.591667</td>\n",
              "      <td>0.654489</td>\n",
              "      <td>0.023864</td>\n",
              "      <td>0.064254</td>\n",
              "      <td>0.092000</td>\n",
              "      <td>0.215272</td>\n",
              "      <td>0.035951</td>\n",
              "      <td>0.051607</td>\n",
              "      <td>-0.123272</td>\n",
              "      <td>-0.062823</td>\n",
              "      <td>0.427367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>29066</td>\n",
              "      <td>1250</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.712904</td>\n",
              "      <td>0.754907</td>\n",
              "      <td>0.678445</td>\n",
              "      <td>0.713651</td>\n",
              "      <td>0.653471</td>\n",
              "      <td>0.607595</td>\n",
              "      <td>0.654489</td>\n",
              "      <td>0.026557</td>\n",
              "      <td>0.061790</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.213438</td>\n",
              "      <td>0.044938</td>\n",
              "      <td>0.041064</td>\n",
              "      <td>-0.113438</td>\n",
              "      <td>-0.046894</td>\n",
              "      <td>0.468519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>29316</td>\n",
              "      <td>1500</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.710813</td>\n",
              "      <td>0.754664</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.712010</td>\n",
              "      <td>0.651984</td>\n",
              "      <td>0.611399</td>\n",
              "      <td>0.653084</td>\n",
              "      <td>0.032900</td>\n",
              "      <td>0.062370</td>\n",
              "      <td>0.107333</td>\n",
              "      <td>0.213510</td>\n",
              "      <td>0.053926</td>\n",
              "      <td>0.035578</td>\n",
              "      <td>-0.106177</td>\n",
              "      <td>-0.041685</td>\n",
              "      <td>0.502708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>29566</td>\n",
              "      <td>1750</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.708734</td>\n",
              "      <td>0.757115</td>\n",
              "      <td>0.673854</td>\n",
              "      <td>0.709727</td>\n",
              "      <td>0.648955</td>\n",
              "      <td>0.612745</td>\n",
              "      <td>0.649993</td>\n",
              "      <td>0.027167</td>\n",
              "      <td>0.062467</td>\n",
              "      <td>0.095429</td>\n",
              "      <td>0.212791</td>\n",
              "      <td>0.062913</td>\n",
              "      <td>0.036274</td>\n",
              "      <td>-0.117363</td>\n",
              "      <td>-0.037248</td>\n",
              "      <td>0.448461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>29816</td>\n",
              "      <td>2000</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.708247</td>\n",
              "      <td>0.754774</td>\n",
              "      <td>0.633803</td>\n",
              "      <td>0.710675</td>\n",
              "      <td>0.649218</td>\n",
              "      <td>0.567227</td>\n",
              "      <td>0.651960</td>\n",
              "      <td>0.030079</td>\n",
              "      <td>0.062853</td>\n",
              "      <td>0.094000</td>\n",
              "      <td>0.213582</td>\n",
              "      <td>0.071901</td>\n",
              "      <td>0.058754</td>\n",
              "      <td>-0.119582</td>\n",
              "      <td>-0.084733</td>\n",
              "      <td>0.440112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>30066</td>\n",
              "      <td>2250</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.709283</td>\n",
              "      <td>0.756406</td>\n",
              "      <td>0.681275</td>\n",
              "      <td>0.710359</td>\n",
              "      <td>0.650264</td>\n",
              "      <td>0.610714</td>\n",
              "      <td>0.651820</td>\n",
              "      <td>0.025888</td>\n",
              "      <td>0.063047</td>\n",
              "      <td>0.098667</td>\n",
              "      <td>0.213690</td>\n",
              "      <td>0.080889</td>\n",
              "      <td>0.039132</td>\n",
              "      <td>-0.115023</td>\n",
              "      <td>-0.041105</td>\n",
              "      <td>0.461728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>30316</td>\n",
              "      <td>2500</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.712960</td>\n",
              "      <td>0.757686</td>\n",
              "      <td>0.668954</td>\n",
              "      <td>0.714918</td>\n",
              "      <td>0.655071</td>\n",
              "      <td>0.594512</td>\n",
              "      <td>0.657861</td>\n",
              "      <td>0.027624</td>\n",
              "      <td>0.062757</td>\n",
              "      <td>0.102000</td>\n",
              "      <td>0.215020</td>\n",
              "      <td>0.089876</td>\n",
              "      <td>0.049241</td>\n",
              "      <td>-0.113020</td>\n",
              "      <td>-0.063349</td>\n",
              "      <td>0.474374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>30566</td>\n",
              "      <td>2750</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.707804</td>\n",
              "      <td>0.755485</td>\n",
              "      <td>0.471655</td>\n",
              "      <td>0.715759</td>\n",
              "      <td>0.642733</td>\n",
              "      <td>0.311377</td>\n",
              "      <td>0.658283</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.062274</td>\n",
              "      <td>0.038909</td>\n",
              "      <td>0.214768</td>\n",
              "      <td>0.098864</td>\n",
              "      <td>0.203969</td>\n",
              "      <td>-0.175859</td>\n",
              "      <td>-0.346906</td>\n",
              "      <td>0.181168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>30816</td>\n",
              "      <td>3000</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.702212</td>\n",
              "      <td>0.755634</td>\n",
              "      <td>0.377919</td>\n",
              "      <td>0.713882</td>\n",
              "      <td>0.635563</td>\n",
              "      <td>0.237968</td>\n",
              "      <td>0.656456</td>\n",
              "      <td>0.003046</td>\n",
              "      <td>0.062805</td>\n",
              "      <td>0.032333</td>\n",
              "      <td>0.214697</td>\n",
              "      <td>0.107852</td>\n",
              "      <td>0.239123</td>\n",
              "      <td>-0.182363</td>\n",
              "      <td>-0.418488</td>\n",
              "      <td>0.150600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    rows_complete  rows_minority  rows_majority  f1_complete  \\\n",
              "0           27821              5          27816     0.714187   \n",
              "1           27826             10          27816     0.711844   \n",
              "2           27836             20          27816     0.713683   \n",
              "3           27846             30          27816     0.712046   \n",
              "4           27856             40          27816     0.712786   \n",
              "5           27866             50          27816     0.710958   \n",
              "6           27891             75          27816     0.707116   \n",
              "7           27916            100          27816     0.708273   \n",
              "8           27941            125          27816     0.709747   \n",
              "9           27966            150          27816     0.711309   \n",
              "10          27991            175          27816     0.711947   \n",
              "11          28016            200          27816     0.711145   \n",
              "12          28066            250          27816     0.709510   \n",
              "13          28116            300          27816     0.709771   \n",
              "14          28166            350          27816     0.710894   \n",
              "15          28216            400          27816     0.713850   \n",
              "16          28266            450          27816     0.714112   \n",
              "17          28316            500          27816     0.709829   \n",
              "18          28416            600          27816     0.708037   \n",
              "19          28516            700          27816     0.712385   \n",
              "20          28616            800          27816     0.712838   \n",
              "21          28716            900          27816     0.711278   \n",
              "22          28816           1000          27816     0.710220   \n",
              "23          29066           1250          27816     0.712904   \n",
              "24          29316           1500          27816     0.710813   \n",
              "25          29566           1750          27816     0.708734   \n",
              "26          29816           2000          27816     0.708247   \n",
              "27          30066           2250          27816     0.709283   \n",
              "28          30316           2500          27816     0.712960   \n",
              "29          30566           2750          27816     0.707804   \n",
              "30          30816           3000          27816     0.702212   \n",
              "\n",
              "    f1_complete_train  f1_minority  f1_majority  tpr_complete  tpr_minority  \\\n",
              "0            0.756365     0.000000     0.714187      0.653225      0.000000   \n",
              "1            0.760895     0.000000     0.711844      0.649431      0.000000   \n",
              "2            0.756079     1.000000     0.713596      0.653884      1.000000   \n",
              "3            0.757311     0.666667     0.712077      0.653609      0.600000   \n",
              "4            0.756766     0.500000     0.712917      0.653096      0.333333   \n",
              "5            0.757652     0.842105     0.710768      0.653150      0.800000   \n",
              "6            0.758356     0.714286     0.707108      0.648470      0.555556   \n",
              "7            0.758556     0.736842     0.708231      0.648008      0.636364   \n",
              "8            0.754835     0.685714     0.709811      0.648164      0.705882   \n",
              "9            0.755391     0.608696     0.711490      0.651235      0.636364   \n",
              "10           0.755718     0.717949     0.711929      0.652186      0.736842   \n",
              "11           0.754935     0.578947     0.711531      0.650413      0.550000   \n",
              "12           0.754842     0.592593     0.709995      0.648985      0.571429   \n",
              "13           0.757064     0.500000     0.710736      0.649665      0.384615   \n",
              "14           0.756823     0.634921     0.711261      0.651189      0.606061   \n",
              "15           0.757444     0.744186     0.713650      0.653964      0.680851   \n",
              "16           0.756619     0.647059     0.714636      0.655374      0.589286   \n",
              "17           0.757761     0.736842     0.709593      0.651344      0.677419   \n",
              "18           0.758297     0.597015     0.709174      0.650535      0.540541   \n",
              "19           0.756626     0.650307     0.713159      0.653761      0.595506   \n",
              "20           0.756228     0.662983     0.713530      0.653089      0.594059   \n",
              "21           0.757091     0.716981     0.711186      0.654128      0.666667   \n",
              "22           0.754031     0.669811     0.710874      0.653448      0.591667   \n",
              "23           0.754907     0.678445     0.713651      0.653471      0.607595   \n",
              "24           0.754664     0.666667     0.712010      0.651984      0.611399   \n",
              "25           0.757115     0.673854     0.709727      0.648955      0.612745   \n",
              "26           0.754774     0.633803     0.710675      0.649218      0.567227   \n",
              "27           0.756406     0.681275     0.710359      0.650264      0.610714   \n",
              "28           0.757686     0.668954     0.714918      0.655071      0.594512   \n",
              "29           0.755485     0.471655     0.715759      0.642733      0.311377   \n",
              "30           0.755634     0.377919     0.713882      0.635563      0.237968   \n",
              "\n",
              "    tpr_majority  fpr_minority  fpr_majority  prob_yhat_1_minority  \\\n",
              "0       0.653225      0.000000      0.060534              0.000000   \n",
              "1       0.649431      0.000000      0.060244              0.000000   \n",
              "2       0.653787      0.000000      0.061404              0.100000   \n",
              "3       0.653646      0.040000      0.062660              0.133333   \n",
              "4       0.653365      0.000000      0.061742              0.050000   \n",
              "5       0.652944      0.025000      0.063385              0.180000   \n",
              "6       0.648588      0.000000      0.063916              0.066667   \n",
              "7       0.648026      0.011236      0.062563              0.080000   \n",
              "8       0.648026      0.055556      0.061162              0.144000   \n",
              "9       0.651258      0.035971      0.061694              0.080000   \n",
              "10      0.651960      0.038462      0.061742              0.114286   \n",
              "11      0.650696      0.038889      0.061307              0.090000   \n",
              "12      0.649290      0.045045      0.061790              0.104000   \n",
              "13      0.651117      0.022989      0.062274              0.070000   \n",
              "14      0.651398      0.031546      0.061984              0.085714   \n",
              "15      0.653787      0.019830      0.061356              0.097500   \n",
              "16      0.655894      0.032995      0.061790              0.102222   \n",
              "17      0.651117      0.022831      0.063288              0.104000   \n",
              "18      0.651679      0.038023      0.064013              0.100000   \n",
              "19      0.654489      0.034370      0.062225              0.105714   \n",
              "20      0.653927      0.028612      0.061549              0.100000   \n",
              "21      0.653927      0.027990      0.063626              0.108889   \n",
              "22      0.654489      0.023864      0.064254              0.092000   \n",
              "23      0.654489      0.026557      0.061790              0.100000   \n",
              "24      0.653084      0.032900      0.062370              0.107333   \n",
              "25      0.649993      0.027167      0.062467              0.095429   \n",
              "26      0.651960      0.030079      0.062853              0.094000   \n",
              "27      0.651820      0.025888      0.063047              0.098667   \n",
              "28      0.657861      0.027624      0.062757              0.102000   \n",
              "29      0.658283      0.001242      0.062274              0.038909   \n",
              "30      0.656456      0.003046      0.062805              0.032333   \n",
              "\n",
              "    prob_yhat_1_majority  rel_share_min_of_maj  aver_abs_odds_diff  \\\n",
              "0               0.212180              0.000180            0.356879   \n",
              "1               0.210994              0.000360            0.354838   \n",
              "2               0.212971              0.000719            0.203809   \n",
              "3               0.213870              0.001079            0.038153   \n",
              "4               0.213115              0.001438            0.190887   \n",
              "5               0.214229              0.001798            0.092721   \n",
              "6               0.213510              0.002696            0.078474   \n",
              "7               0.212360              0.003595            0.031495   \n",
              "8               0.211317              0.004494            0.031732   \n",
              "9               0.212540              0.005393            0.020308   \n",
              "10              0.212755              0.006291            0.054081   \n",
              "11              0.212108              0.007190            0.061557   \n",
              "12              0.212108              0.008988            0.047304   \n",
              "13              0.212935              0.010785            0.152893   \n",
              "14              0.212791              0.012583            0.037888   \n",
              "15              0.212935              0.014380            0.034295   \n",
              "16              0.213798              0.016178            0.047702   \n",
              "17              0.213690              0.017975            0.033380   \n",
              "18              0.214373              0.021570            0.068564   \n",
              "19              0.213762              0.025165            0.043419   \n",
              "20              0.213115              0.028760            0.046402   \n",
              "21              0.214661              0.032355            0.024188   \n",
              "22              0.215272              0.035951            0.051607   \n",
              "23              0.213438              0.044938            0.041064   \n",
              "24              0.213510              0.053926            0.035578   \n",
              "25              0.212791              0.062913            0.036274   \n",
              "26              0.213582              0.071901            0.058754   \n",
              "27              0.213690              0.080889            0.039132   \n",
              "28              0.215020              0.089876            0.049241   \n",
              "29              0.214768              0.098864            0.203969   \n",
              "30              0.214697              0.107852            0.239123   \n",
              "\n",
              "    stat_parity_diff  equal_opport_dist  disparate_impact  \n",
              "0          -0.212180          -0.653225          0.000000  \n",
              "1          -0.210994          -0.649431          0.000000  \n",
              "2          -0.112971           0.346213          0.469548  \n",
              "3          -0.080536          -0.053646          0.623433  \n",
              "4          -0.163115          -0.320032          0.234615  \n",
              "5          -0.034229           0.147056          0.840222  \n",
              "6          -0.146844          -0.093032          0.312241  \n",
              "7          -0.132360          -0.011662          0.376719  \n",
              "8          -0.067317           0.057856          0.681440  \n",
              "9          -0.132540          -0.014894          0.376401  \n",
              "10         -0.098470           0.084882          0.537170  \n",
              "11         -0.122108          -0.100696          0.424312  \n",
              "12         -0.108108          -0.077862          0.490316  \n",
              "13         -0.142935          -0.266502          0.328739  \n",
              "14         -0.127077          -0.045337          0.402809  \n",
              "15         -0.115435           0.027064          0.457886  \n",
              "16         -0.111576          -0.066609          0.478126  \n",
              "17         -0.109690           0.026302          0.486686  \n",
              "18         -0.114373          -0.111139          0.466477  \n",
              "19         -0.108048          -0.058984          0.494542  \n",
              "20         -0.113115          -0.059868          0.469231  \n",
              "21         -0.105772           0.012739          0.507261  \n",
              "22         -0.123272          -0.062823          0.427367  \n",
              "23         -0.113438          -0.046894          0.468519  \n",
              "24         -0.106177          -0.041685          0.502708  \n",
              "25         -0.117363          -0.037248          0.448461  \n",
              "26         -0.119582          -0.084733          0.440112  \n",
              "27         -0.115023          -0.041105          0.461728  \n",
              "28         -0.113020          -0.063349          0.474374  \n",
              "29         -0.175859          -0.346906          0.181168  \n",
              "30         -0.182363          -0.418488          0.150600  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S2lYP02s9aZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save metrics csv\n",
        "results_df_adult.to_csv(\"df_adult_metrics.csv\") \n",
        "from google.colab import files\n",
        "files.download(\"df_adult_metrics.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbXQvAkgi8vN",
        "colab_type": "text"
      },
      "source": [
        "###### 4) Create visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk_dZBxdzJro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Feature Importance\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "from matplotlib import pyplot\n",
        "\n",
        "model = XGBClassifier()\n",
        "model.fit(df_compas_train_input, df_compas_train_label)\n",
        "plot_importance(model)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L8M_z7m_5EL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# B: Learning Curve Function from scikit learn\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "# Define arguments\n",
        "f1 = make_scorer(f1_score) \n",
        "\n",
        "df_3_adult_train_input = pd.get_dummies(df_3_adult_train_input)\n",
        "\n",
        "# Plot actual learning curve\n",
        "plot_learning_curve(estimator = model, \n",
        "                    title = \"Adult Dataset - Random Forest Learning Curve\", \n",
        "                    X = df_3_adult_train_input, y = df_3_adult_train_label, \n",
        "                    cv = 5, \n",
        "                    scoring = f1, \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = np.linspace(.1, 1.0, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u0QSdisyQ9O",
        "colab_type": "text"
      },
      "source": [
        "Minority Line Chart "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b85h5SQhwdv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "f87c7733-3eee-4f0f-8b0f-5c6a416e1a3d"
      },
      "source": [
        "min_metrics_line_chart(metric_df = results_df_adult, title=\"Minority Metrics for the Adult Dataset with XGBoost\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"b699132b-7783-4b9d-ae5e-1a2ca500da84\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"b699132b-7783-4b9d-ae5e-1a2ca500da84\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'b699132b-7783-4b9d-ae5e-1a2ca500da84',\n",
              "                        [{\"mode\": \"lines+markers\", \"name\": \"F1 Minority\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.22222222222222224, 0.4705882352941177, 0.43478260869565216, 0.5161290322580645, 0.4848484848484849, 0.4705882352941176, 0.4878048780487805, 0.48, 0.4745762711864407, 0.5, 0.5063291139240507, 0.5348837209302326, 0.5510204081632654, 0.5932203389830509, 0.6099290780141844, 0.6012269938650308, 0.6187845303867403, 0.6108374384236452, 0.6124031007751938, 0.6163934426229508, 0.6039886039886041, 0.6115288220551378, 0.588235294117647, 0.6067864271457085, 0.6225402504472272, 0.4769539078156313]}, {\"mode\": \"lines+markers\", \"name\": \"TPR/Recall Minority\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.14285714285714285, 0.36363636363636365, 0.3125, 0.38095238095238093, 0.36363636363636365, 0.34782608695652173, 0.37037037037037035, 0.35294117647058826, 0.34146341463414637, 0.375, 0.37735849056603776, 0.39655172413793105, 0.4153846153846154, 0.4605263157894737, 0.4777777777777778, 0.4666666666666667, 0.4827586206896552, 0.4696969696969697, 0.48466257668711654, 0.49473684210526314, 0.4930232558139535, 0.5041322314049587, 0.4744525547445255, 0.5, 0.514792899408284, 0.3233695652173913]}, {\"mode\": \"lines+markers\", \"name\": \"FPR Minority\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.023255813953488372, 0.03125, 0.023809523809523808, 0.019230769230769232, 0.0234375, 0.019736842105263157, 0.023121387283236993, 0.018518518518518517, 0.015444015444015444, 0.019867549668874173, 0.01729106628242075, 0.012755102040816327, 0.013793103448275862, 0.013358778625954198, 0.013114754098360656, 0.012949640287769784, 0.011479591836734694, 0.010368663594470046, 0.014719411223551058, 0.01603053435114504, 0.019543973941368076, 0.019908987485779295, 0.019230769230769232, 0.020491803278688523, 0.019485903814262025, 0.004559270516717325]}, {\"mode\": \"lines+markers\", \"name\": \"Avg. Abs. Odds Difference\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [0.3267813217360542, 0.32792954662262236, 0.3270887077232432, 0.3274289145848995, 0.3267593789280469, 0.24501800275520716, 0.1302053475218777, 0.15948712859036918, 0.1283363906204608, 0.13411602341849663, 0.14317539966921974, 0.13101015160498267, 0.1422699534487097, 0.14915314163327353, 0.13081200956861225, 0.130389579339852, 0.12385789795807225, 0.11310798143624734, 0.09021641483617, 0.0815897043759216, 0.0883760417233408, 0.08019345543673981, 0.08911963508073041, 0.07877864442440707, 0.07203656123854496, 0.0710333727188929, 0.06393517735438485, 0.07961696979813886, 0.06786803924057411, 0.05930816543296586, 0.1641122762417779]}, {\"mode\": \"lines+markers\", \"name\": \"Statistical Parity Difference\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [-0.19359361518550475, -0.1942047742306586, -0.19384526890997986, -0.19366551624964048, -0.19362956571757262, -0.15442047742306586, -0.11406097210238712, -0.12384526890997985, -0.11467213114754098, -0.12087144089732527, -0.13044886807181888, -0.12380931837791198, -0.1302407247627265, -0.1342047742306586, -0.12617260364024818, -0.1289531205061835, -0.13216230466877577, -0.12813287316652286, -0.12388121944204775, -0.12077242286042976, -0.1217407247627265, -0.1214073434953504, -0.12399568593615186, -0.11852832901926948, -0.1174662064998562, -0.11605908213155841, -0.11448245614035087, -0.11892694851883807, -0.1154047742306586, -0.11287047350118964, -0.15082571182053495]}, {\"mode\": \"lines+markers\", \"name\": \"Equal Opportunity Distance\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [-0.5995503723478994, -0.6017985106084024, -0.5999718982717437, -0.6013769846845581, -0.599409863706618, -0.45894136775125954, -0.23760011240691303, -0.28915800196712094, -0.22140816422114729, -0.23731909512435012, -0.2528483545216291, -0.23170915752059495, -0.2489978427790956, -0.25935153548528594, -0.22665800196712094, -0.22429951140108317, -0.2066518728832858, -0.18585186065866127, -0.13958609112355147, -0.1224751377765288, -0.1358343871481429, -0.11833734671234003, -0.1342091705306543, -0.11811949441025593, -0.10565658209032491, -0.1077916943054788, -0.09415356317140777, -0.12411425711440383, -0.10236054517352822, -0.08447645565705253, -0.27828843674972964]}, {\"mode\": \"lines+markers\", \"name\": \"Disparate Impact\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.2057396449704142, 0.4122415709522045, 0.361112759643917, 0.4109473684210527, 0.3776082932247316, 0.3251691065118627, 0.3611797440178075, 0.3294880621876735, 0.308952239911144, 0.35211055143859277, 0.33513253012048194, 0.3200986375685839, 0.33997333333333335, 0.36104580011125537, 0.3762707548671158, 0.37324819544697385, 0.3729917068944176, 0.3641106194690265, 0.3906885973017927, 0.3949185185185185, 0.4010576199310893, 0.4067727272727273, 0.3856876508820799, 0.4057572750833025, 0.41588742494714587, 0.22451608133086876]}],\n",
              "                        {\"font\": {\"size\": 14}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \" Minority Metrics for the Adult Dataset with XGBoost\", \"x\": 0.5, \"y\": 0.9, \"yanchor\": \"top\"}, \"xaxis\": {\"title\": {\"text\": \"Rows Minority\"}}, \"yaxis\": {\"title\": {\"text\": \"Metric Score\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b699132b-7783-4b9d-ae5e-1a2ca500da84');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrPVof-byOTQ",
        "colab_type": "text"
      },
      "source": [
        "Majority <-> Minority Line Chart "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ridl2ll4YxAP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "8d8b8949-7c9d-4617-b82a-a9057b2f828a"
      },
      "source": [
        "maj_min_metrics_line_chart(metric_df=results_df_adult, title=\"Majority and Minority Metrics for the Adult Dataset with XGBoost\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"28fd47ae-efdd-4002-9250-83c9500b1bd5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"28fd47ae-efdd-4002-9250-83c9500b1bd5\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '28fd47ae-efdd-4002-9250-83c9500b1bd5',\n",
              "                        [{\"mode\": \"lines+markers\", \"name\": \"F1 Majority\", \"type\": \"scatter\", \"x\": [27821, 27826, 27836, 27846, 27856, 27866, 27891, 27916, 27941, 27966, 27991, 28016, 28066, 28116, 28166, 28216, 28266, 28316, 28416, 28516, 28616, 28716, 28816, 29066, 29316, 29566, 29816, 30066, 30316, 30566, 30816], \"y\": [0.6826107822748361, 0.6842399552679926, 0.6827084499160604, 0.6845809341010877, 0.6823962249060226, 0.6839121756487027, 0.68381941669996, 0.6846270685106723, 0.6841685285668688, 0.6832814122533748, 0.6843284776692812, 0.685161496642149, 0.6843450479233226, 0.6831216550842718, 0.6832615286420933, 0.684462915601023, 0.6855637176620888, 0.6837101541903012, 0.6828137490007994, 0.6833559945613052, 0.6849840255591054, 0.684315764216588, 0.6854317837493024, 0.6848659003831418, 0.6827514580170968, 0.6837770848324938, 0.6820985182218663, 0.6814909614461686, 0.6848789839444045, 0.6828370156900416, 0.6836433304063224]}, {\"mode\": \"lines+markers\", \"name\": \"F1 Minority\", \"type\": \"scatter\", \"x\": [27821, 27826, 27836, 27846, 27856, 27866, 27891, 27916, 27941, 27966, 27991, 28016, 28066, 28116, 28166, 28216, 28266, 28316, 28416, 28516, 28616, 28716, 28816, 29066, 29316, 29566, 29816, 30066, 30316, 30566, 30816], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.22222222222222224, 0.4705882352941177, 0.43478260869565216, 0.5161290322580645, 0.4848484848484849, 0.4705882352941176, 0.4878048780487805, 0.48, 0.4745762711864407, 0.5, 0.5063291139240507, 0.5348837209302326, 0.5510204081632654, 0.5932203389830509, 0.6099290780141844, 0.6012269938650308, 0.6187845303867403, 0.6108374384236452, 0.6124031007751938, 0.6163934426229508, 0.6039886039886041, 0.6115288220551378, 0.588235294117647, 0.6067864271457085, 0.6225402504472272, 0.4769539078156313]}, {\"mode\": \"lines+markers\", \"name\": \"TPR/Recall Majority\", \"type\": \"scatter\", \"x\": [27821, 27826, 27836, 27846, 27856, 27866, 27891, 27916, 27941, 27966, 27991, 28016, 28066, 28116, 28166, 28216, 28266, 28316, 28416, 28516, 28616, 28716, 28816, 29066, 29316, 29566, 29816, 30066, 30316, 30566, 30816], \"y\": [0.5995503723478994, 0.6017985106084024, 0.5999718982717437, 0.6013769846845581, 0.599409863706618, 0.6017985106084024, 0.6012364760432767, 0.6016580019671209, 0.6023605451735282, 0.6009554587607138, 0.6006744414781509, 0.6020795278909653, 0.6019390192496838, 0.6008149501194323, 0.6016580019671209, 0.6016580019671209, 0.6032035970212168, 0.6012364760432767, 0.6001124069130251, 0.6002529155543066, 0.6025010538148096, 0.6010959674019952, 0.603906140227624, 0.6027820710973725, 0.600393424195588, 0.6008149501194323, 0.5982857945763664, 0.5985668118589293, 0.6023605451735282, 0.5992693550653365, 0.6016580019671209]}, {\"mode\": \"lines+markers\", \"name\": \"TPR/Recall Minority\", \"type\": \"scatter\", \"x\": [27821, 27826, 27836, 27846, 27856, 27866, 27891, 27916, 27941, 27966, 27991, 28016, 28066, 28116, 28166, 28216, 28266, 28316, 28416, 28516, 28616, 28716, 28816, 29066, 29316, 29566, 29816, 30066, 30316, 30566, 30816], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.14285714285714285, 0.36363636363636365, 0.3125, 0.38095238095238093, 0.36363636363636365, 0.34782608695652173, 0.37037037037037035, 0.35294117647058826, 0.34146341463414637, 0.375, 0.37735849056603776, 0.39655172413793105, 0.4153846153846154, 0.4605263157894737, 0.4777777777777778, 0.4666666666666667, 0.4827586206896552, 0.4696969696969697, 0.48466257668711654, 0.49473684210526314, 0.4930232558139535, 0.5041322314049587, 0.4744525547445255, 0.5, 0.514792899408284, 0.3233695652173913]}, {\"mode\": \"lines+markers\", \"name\": \"FPR Majority\", \"type\": \"scatter\", \"x\": [27821, 27826, 27836, 27846, 27856, 27866, 27891, 27916, 27941, 27966, 27991, 28016, 28066, 28116, 28166, 28216, 28266, 28316, 28416, 28516, 28616, 28716, 28816, 29066, 29316, 29566, 29816, 30066, 30316, 30566, 30816], \"y\": [0.0540122711242089, 0.05406058263684236, 0.05420551717474274, 0.05348084448524083, 0.05410889414947582, 0.054350451712643126, 0.05406058263684236, 0.053625779023141217, 0.0544953862505435, 0.054350451712643126, 0.05323928692207353, 0.053432532972607374, 0.05406058263684236, 0.054398763225276585, 0.05483356683897773, 0.05377071356104159, 0.05381902507367506, 0.054157205662109284, 0.05420551717474274, 0.05381902507367506, 0.05386733658630852, 0.05352915599787429, 0.054398763225276585, 0.054157205662109284, 0.054447074737910044, 0.05381902507367506, 0.053625779023141217, 0.054350451712643126, 0.05386733658630852, 0.053625779023141217, 0.0544953862505435]}, {\"mode\": \"lines+markers\", \"name\": \"FPR Minority\", \"type\": \"scatter\", \"x\": [27821, 27826, 27836, 27846, 27856, 27866, 27891, 27916, 27941, 27966, 27991, 28016, 28066, 28116, 28166, 28216, 28266, 28316, 28416, 28516, 28616, 28716, 28816, 29066, 29316, 29566, 29816, 30066, 30316, 30566, 30816], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.023255813953488372, 0.03125, 0.023809523809523808, 0.019230769230769232, 0.0234375, 0.019736842105263157, 0.023121387283236993, 0.018518518518518517, 0.015444015444015444, 0.019867549668874173, 0.01729106628242075, 0.012755102040816327, 0.013793103448275862, 0.013358778625954198, 0.013114754098360656, 0.012949640287769784, 0.011479591836734694, 0.010368663594470046, 0.014719411223551058, 0.01603053435114504, 0.019543973941368076, 0.019908987485779295, 0.019230769230769232, 0.020491803278688523, 0.019485903814262025, 0.004559270516717325]}],\n",
              "                        {\"font\": {\"size\": 14}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Majority and Minority Metrics for the Adult Dataset with XGBoost\", \"x\": 0.5, \"y\": 0.9, \"yanchor\": \"top\"}, \"xaxis\": {\"title\": {\"text\": \"Rows Complete\"}}, \"yaxis\": {\"title\": {\"text\": \"Metric Score\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('28fd47ae-efdd-4002-9250-83c9500b1bd5');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ssH5Y-F0Lsqt"
      },
      "source": [
        "## 2) COMPAS Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK-gROLSu6ag",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ7OVi5Y-MvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_compas = \"/content/drive/My Drive/Master Thesis/Data/compas-scores-two-years_dataset.csv\"\n",
        "compas_column_names = ['id', 'name', 'first', 'last', 'compas screening date', 'sex', 'dob',\n",
        "                       'age', 'age cat', 'race', 'juv fel count', 'decile score',\n",
        "                       'juv misd count', 'juv other count', 'priors count',\n",
        "                       'days b screening arrest', 'c jail in', 'c jail out', 'c case number',\n",
        "                       'c offense date', 'c arrest date', 'c days from compas',\n",
        "                       'c charge degree', 'c charge desc', 'is recid', 'r case number',\n",
        "                       'r charge degree', 'r days from arrest', 'r offense date',\n",
        "                       'r charge desc', 'r jail in', 'r jail out', 'violent recid',\n",
        "                       'is violent recid', 'vr case number', 'vr charge degree',\n",
        "                       'vr offense date', 'vr charge desc', 'type of assessment',\n",
        "                       'decile score.1', 'score text', 'screening date',\n",
        "                       'v type of assessment', 'v decile score', 'v score text',\n",
        "                       'v screening date', 'in custody', 'out custody', 'priors count.1',\n",
        "                       'start', 'end', 'event', 'two year recid']\n",
        "df_compas = pd.read_csv(path_compas, low_memory=False, names = compas_column_names, header = 0, sep = \";\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH6DV7Qcm1yV",
        "colab_type": "text"
      },
      "source": [
        "### Initial Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h69rOkMDjAan",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "9c07b41d-111f-47d6-e3b4-d334717a3692"
      },
      "source": [
        "# Rename columns\n",
        "df_compas = df_compas.rename(index=str, columns={\"decile score.1\":\"decile_score a\",\n",
        "                                                 \"priors count.1\":\"priors_count a\"})\n",
        "\n",
        "# Replace empty strings with NAs\n",
        "df_compas = df_compas.replace(r'^\\s*$', np.nan, regex=True)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_compas = df_compas.drop(['id', 'name', 'age cat', \"is recid\", \"event\", \"start\", \"end\"], axis=1)\n",
        "\n",
        "# Drop NaN from label column \n",
        "df_compas = df_compas[df_compas['two year recid'].notna()]\n",
        "print(df_compas.shape)\n",
        "\n",
        "# Convert columns to datetime format\n",
        "# Columns with yyyy-mm-dd format:\n",
        "# date_columns = [\"dob\", \"c_offense_date\", \"c_arrest_date\", \"r_offense_date\", \"r_jail_out\", \"vr_offense_date\", \"screening_date\", \"v_screening_date\", \"v_screening_date\", \"in_custody\", \"out_custody\"]\n",
        "# for date_time in date_columns:\n",
        "#   df_compas[date_time]= pd.to_datetime(df_compas[date_time])\n",
        "\n",
        "# To Do: Columns with yyyy-mm-dd and hh:mm:ss -> c_jail_in, c_jail_out"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6873, 46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nbu9ppIqXJ0f",
        "colab_type": "text"
      },
      "source": [
        "Shrink the size of the Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gRsvBLJW8IR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_compas.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmDRGtwnX8Vl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "ed505f4f-aa24-4f2d-ef26-cfd4fd00e20b"
      },
      "source": [
        "# Check which columns are suitable for conversion in \"category\" data format\n",
        "## We should stick to using the category type primarily for object columns where less than 50% of the values are unique.\n",
        "df_compas_copy = df_compas.select_dtypes(include=['object']).copy()\n",
        "df_compas_copy.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>first</th>\n",
              "      <th>last</th>\n",
              "      <th>compas screening date</th>\n",
              "      <th>sex</th>\n",
              "      <th>dob</th>\n",
              "      <th>race</th>\n",
              "      <th>c jail in</th>\n",
              "      <th>c jail out</th>\n",
              "      <th>c case number</th>\n",
              "      <th>c offense date</th>\n",
              "      <th>c arrest date</th>\n",
              "      <th>c charge degree</th>\n",
              "      <th>c charge desc</th>\n",
              "      <th>r case number</th>\n",
              "      <th>r charge degree</th>\n",
              "      <th>r offense date</th>\n",
              "      <th>r charge desc</th>\n",
              "      <th>r jail in</th>\n",
              "      <th>r jail out</th>\n",
              "      <th>vr case number</th>\n",
              "      <th>vr charge degree</th>\n",
              "      <th>vr offense date</th>\n",
              "      <th>vr charge desc</th>\n",
              "      <th>type of assessment</th>\n",
              "      <th>score text</th>\n",
              "      <th>screening date</th>\n",
              "      <th>v type of assessment</th>\n",
              "      <th>v score text</th>\n",
              "      <th>v screening date</th>\n",
              "      <th>in custody</th>\n",
              "      <th>out custody</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6873</td>\n",
              "      <td>6873</td>\n",
              "      <td>6873</td>\n",
              "      <td>6873</td>\n",
              "      <td>6873</td>\n",
              "      <td>6873</td>\n",
              "      <td>6574</td>\n",
              "      <td>6574</td>\n",
              "      <td>6852</td>\n",
              "      <td>5768</td>\n",
              "      <td>1084</td>\n",
              "      <td>6873</td>\n",
              "      <td>6847</td>\n",
              "      <td>3295</td>\n",
              "      <td>3295</td>\n",
              "      <td>3295</td>\n",
              "      <td>3240</td>\n",
              "      <td>2194</td>\n",
              "      <td>2194</td>\n",
              "      <td>776</td>\n",
              "      <td>776</td>\n",
              "      <td>776</td>\n",
              "      <td>776</td>\n",
              "      <td>6873</td>\n",
              "      <td>6873</td>\n",
              "      <td>6873</td>\n",
              "      <td>6873</td>\n",
              "      <td>6873</td>\n",
              "      <td>6873</td>\n",
              "      <td>6643</td>\n",
              "      <td>6643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>2698</td>\n",
              "      <td>3812</td>\n",
              "      <td>687</td>\n",
              "      <td>2</td>\n",
              "      <td>5251</td>\n",
              "      <td>6</td>\n",
              "      <td>6521</td>\n",
              "      <td>5928</td>\n",
              "      <td>6852</td>\n",
              "      <td>918</td>\n",
              "      <td>564</td>\n",
              "      <td>2</td>\n",
              "      <td>429</td>\n",
              "      <td>3295</td>\n",
              "      <td>10</td>\n",
              "      <td>1057</td>\n",
              "      <td>329</td>\n",
              "      <td>945</td>\n",
              "      <td>923</td>\n",
              "      <td>776</td>\n",
              "      <td>9</td>\n",
              "      <td>549</td>\n",
              "      <td>79</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>687</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>687</td>\n",
              "      <td>1149</td>\n",
              "      <td>1164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>michael</td>\n",
              "      <td>williams</td>\n",
              "      <td>07.02.2013</td>\n",
              "      <td>Male</td>\n",
              "      <td>02.05.1990</td>\n",
              "      <td>African-American</td>\n",
              "      <td>25.05.2013 04:58</td>\n",
              "      <td>30.04.2013 07:29</td>\n",
              "      <td>10020847CF10A</td>\n",
              "      <td>14.01.2013</td>\n",
              "      <td>06.02.2013</td>\n",
              "      <td>F</td>\n",
              "      <td>Battery</td>\n",
              "      <td>14009256MM10A</td>\n",
              "      <td>M1</td>\n",
              "      <td>08.12.2014</td>\n",
              "      <td>Driving License Suspended</td>\n",
              "      <td>03.03.2015</td>\n",
              "      <td>18.02.2014</td>\n",
              "      <td>15000828CF10A</td>\n",
              "      <td>M1</td>\n",
              "      <td>15.08.2015</td>\n",
              "      <td>Battery</td>\n",
              "      <td>Risk of Recidivism</td>\n",
              "      <td>Low</td>\n",
              "      <td>07.02.2013</td>\n",
              "      <td>Risk of Violence</td>\n",
              "      <td>Low</td>\n",
              "      <td>07.02.2013</td>\n",
              "      <td>07.04.2013</td>\n",
              "      <td>01.01.2020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>136</td>\n",
              "      <td>75</td>\n",
              "      <td>30</td>\n",
              "      <td>5553</td>\n",
              "      <td>5</td>\n",
              "      <td>3526</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>9</td>\n",
              "      <td>4456</td>\n",
              "      <td>1093</td>\n",
              "      <td>1</td>\n",
              "      <td>1139</td>\n",
              "      <td>12</td>\n",
              "      <td>246</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>327</td>\n",
              "      <td>6</td>\n",
              "      <td>312</td>\n",
              "      <td>6873</td>\n",
              "      <td>3722</td>\n",
              "      <td>30</td>\n",
              "      <td>6873</td>\n",
              "      <td>4532</td>\n",
              "      <td>30</td>\n",
              "      <td>19</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          first      last compas screening date   sex         dob  \\\n",
              "count      6873      6873                  6873  6873        6873   \n",
              "unique     2698      3812                   687     2        5251   \n",
              "top     michael  williams            07.02.2013  Male  02.05.1990   \n",
              "freq        136        75                    30  5553           5   \n",
              "\n",
              "                    race         c jail in        c jail out  c case number  \\\n",
              "count               6873              6574              6574           6852   \n",
              "unique                 6              6521              5928           6852   \n",
              "top     African-American  25.05.2013 04:58  30.04.2013 07:29  10020847CF10A   \n",
              "freq                3526                 2                 6              1   \n",
              "\n",
              "       c offense date c arrest date c charge degree c charge desc  \\\n",
              "count            5768          1084            6873          6847   \n",
              "unique            918           564               2           429   \n",
              "top        14.01.2013    06.02.2013               F       Battery   \n",
              "freq               26             9            4456          1093   \n",
              "\n",
              "        r case number r charge degree r offense date  \\\n",
              "count            3295            3295           3295   \n",
              "unique           3295              10           1057   \n",
              "top     14009256MM10A              M1     08.12.2014   \n",
              "freq                1            1139             12   \n",
              "\n",
              "                    r charge desc   r jail in  r jail out vr case number  \\\n",
              "count                        3240        2194        2194            776   \n",
              "unique                        329         945         923            776   \n",
              "top     Driving License Suspended  03.03.2015  18.02.2014  15000828CF10A   \n",
              "freq                          246           8           9              1   \n",
              "\n",
              "       vr charge degree vr offense date vr charge desc  type of assessment  \\\n",
              "count               776             776            776                6873   \n",
              "unique                9             549             79                   1   \n",
              "top                  M1      15.08.2015        Battery  Risk of Recidivism   \n",
              "freq                327               6            312                6873   \n",
              "\n",
              "       score text screening date v type of assessment v score text  \\\n",
              "count        6873           6873                 6873         6873   \n",
              "unique          3            687                    1            3   \n",
              "top           Low     07.02.2013     Risk of Violence          Low   \n",
              "freq         3722             30                 6873         4532   \n",
              "\n",
              "       v screening date  in custody out custody  \n",
              "count              6873        6643        6643  \n",
              "unique              687        1149        1164  \n",
              "top          07.02.2013  07.04.2013  01.01.2020  \n",
              "freq                 30          19          60  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T24jQWn9W6Kg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reduce the size of the numeric columns\n",
        "\n",
        "df_compas, NAlist = reduce_mem_usage(df_compas)\n",
        "print(\"_________________\")\n",
        "print(\"\")\n",
        "print(\"Warning: the following columns have missing values filled with 'df['column_name'].min() -1': \")\n",
        "print(\"_________________\")\n",
        "print(\"\")\n",
        "print(NAlist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLIyfdRtW7QR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for col in [\"race\", \"sex\",\"first\",\"compas screening date\",\"c offense date\",\"c charge degree\",\"c charge desc\",\"r charge degree\",\"r offense date\",\"r charge desc\",\"r jail in\",\"r jail out\",\"vr case number\",\"vr charge degree\",\"vr offense date\",\"vr charge desc\",\"type of assessment\",\"score text\",\"screening date\",\"v type of assessment\",\"v score text\",\"v screening date\",\"in custody\",\"out custody\"]:\n",
        "    df_compas[col] = df_compas[col].astype('category')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_xvYT2xu8L8",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pbt9k0dsxkfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check column names\n",
        "print(df_compas.columns)\n",
        "# Check datatypes of columns\n",
        "print(df_compas.dtypes)\n",
        "df_compas.head(n=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duNufuy52C28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_compas.columns)\n",
        "print(df_compas[\"race\"].unique())\n",
        "print(df_compas.groupby([\"race\"]).agg({\"race\": 'count'}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twN3wQQh2LHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eda_descr_stats(data = df_compas, disc_feature = \"race\", disc_min_value=\"African-American\", \n",
        "                label = \"two_year_recid\", second_disc_feature=\"sex\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YznLf--cDdd",
        "colab_type": "text"
      },
      "source": [
        "### Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgBktRpZcHdP",
        "colab_type": "text"
      },
      "source": [
        "#### 0) Preprocess data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hqczOKFBeeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute preprocessing function\n",
        "df_compas_train_input, df_compas_train_label = fair_preprocess(data = df_compas, \n",
        "                                                               label = \"two year recid\",\n",
        "                                                               neg_class = 0,\n",
        "                                                               pos_class = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SbCWg77cIEG",
        "colab_type": "text"
      },
      "source": [
        "#### 1) Define Hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtKin9sSZaZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_compas_train_input.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AQ5ZvcFm5ZQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "e0674b98-1f24-495a-9984-6bdc92fa8ac0"
      },
      "source": [
        "# Create the hyperparameter grid\n",
        "param_grid = {\"learning_rate\": [0.3, 0.4, 0.5],\n",
        "                \"n_estimators\": [10],\n",
        "                \"max_depth\": [7], \n",
        "                \"min_child_weight\": [1],       \n",
        "                \"reg_lambda\": [1.4]}\n",
        "\n",
        "xgb_grid_search = XGBClassifier(objective= 'binary:logistic', nthread=4)\n",
        "cv=3\n",
        "\n",
        "# Learning Curve for Slice \n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define model\n",
        "f1 = make_scorer(f1_score)\n",
        "\n",
        "# Dummy Coding\n",
        "df_train_input_dummy = pd.get_dummies(df_compas_train_input)\n",
        "\n",
        "grid_rf_class = sklearn.model_selection.GridSearchCV(estimator = xgb_grid_search,\n",
        "                                                      param_grid = param_grid, \n",
        "                                                      scoring= f1, \n",
        "                                                      cv = cv,\n",
        "                                                      refit = True,\n",
        "                                                      return_train_score = False)\n",
        "  \n",
        "# Fit model\n",
        "grid_rf_class.fit(df_train_input_dummy, df_compas_train_label)\n",
        "\n",
        "print(grid_rf_class.best_params_)\n",
        "print(grid_rf_class.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'learning_rate': 0.4, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 10, 'reg_lambda': 1.4}\n",
            "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
            "              learning_rate=0.4, max_delta_step=0, max_depth=7,\n",
            "              min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
            "              nthread=4, objective='binary:logistic', random_state=0,\n",
            "              reg_alpha=0, reg_lambda=1.4, scale_pos_weight=1, seed=None,\n",
            "              silent=None, subsample=1, verbosity=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDFHoRCNcIes",
        "colab_type": "text"
      },
      "source": [
        "#### 2) Create datasets with diff. minority group sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqNYcdQxExno",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "2febb5e5-ddc2-4f5d-e4a4-a2bd88540d1e"
      },
      "source": [
        "training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Creating dfs for minority and majority group\n",
        "is_unpriv = df_compas[\"race\"].isin([\"African-American\"])\n",
        "is_priv = df_compas[\"race\"].isin([\"Caucasian\"])\n",
        "df_compas_unpriv = df_compas[is_unpriv]\n",
        "df_compas_priv = df_compas[is_priv]\n",
        "\n",
        "list_dfs_compas = create_datasets(min_data = df_compas_unpriv, maj_data = df_compas_priv, training_sizes=training_sizes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33\n",
            "[2336, 2341, 2351, 2361, 2371, 2381, 2406, 2431, 2456, 2481, 2506, 2531, 2581, 2631, 2681, 2731, 2781, 2831, 2931, 3031, 3131, 3231, 3331, 3581, 3831, 4081, 4331, 4581, 4831, 5081, 5331, 5581, 5831]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkYaTGG5cI-P",
        "colab_type": "text"
      },
      "source": [
        "#### 3) Create dataframes with diff. metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qTBnMllH4RI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e0d179b8-eef8-4792-a1eb-50ebf973e01a"
      },
      "source": [
        "# Define arguments\n",
        "label = \"two year recid\"\n",
        "compas_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "                             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "                             learning_rate=0.3, max_delta_step=0, max_depth=7,\n",
        "                             min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
        "                             nthread=4, objective='binary:logistic', random_state=0,\n",
        "                             reg_alpha=0, reg_lambda=1.4, scale_pos_weight=1, seed=None,\n",
        "                             silent=None, subsample=1, verbosity=1)\n",
        "cv = 5 \n",
        "discr_feature = \"race\"\n",
        "min_value = \"African-American\"\n",
        "maj_value = \"Caucasian\"\n",
        "\n",
        "# Run function\n",
        "results_df_compas = metrics_to_df(list_dfs=list_dfs_compas, label = label, model = compas_model, \n",
        "                                  cv = cv, discr_feature = discr_feature, min_value = min_value, maj_value = maj_value)\n",
        "\n",
        "results_df_compas\n",
        "\n",
        "# Save metrics csv\n",
        "results_df_compas.to_csv(\"df_compas_metrics.csv\") \n",
        "from google.colab import files\n",
        "files.download(\"df_compas_metrics.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rows_complete</th>\n",
              "      <th>rows_minority</th>\n",
              "      <th>rows_majority</th>\n",
              "      <th>f1_complete</th>\n",
              "      <th>f1_complete_train</th>\n",
              "      <th>f1_minority</th>\n",
              "      <th>f1_majority</th>\n",
              "      <th>tpr_complete</th>\n",
              "      <th>tpr_minority</th>\n",
              "      <th>tpr_majority</th>\n",
              "      <th>fpr_minority</th>\n",
              "      <th>fpr_majority</th>\n",
              "      <th>prob_yhat_1_minority</th>\n",
              "      <th>prob_yhat_1_majority</th>\n",
              "      <th>rel_share_min_of_maj</th>\n",
              "      <th>aver_abs_odds_diff</th>\n",
              "      <th>stat_parity_diff</th>\n",
              "      <th>equal_opport_dist</th>\n",
              "      <th>disparate_impact</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2336</td>\n",
              "      <td>5</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.992432</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.992424</td>\n",
              "      <td>0.997826</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997824</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.398541</td>\n",
              "      <td>0.002145</td>\n",
              "      <td>0.005337</td>\n",
              "      <td>-0.198541</td>\n",
              "      <td>0.002176</td>\n",
              "      <td>0.501830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2341</td>\n",
              "      <td>10</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.992449</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.992424</td>\n",
              "      <td>0.997831</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997824</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.398541</td>\n",
              "      <td>0.004290</td>\n",
              "      <td>0.005337</td>\n",
              "      <td>-0.098541</td>\n",
              "      <td>0.002176</td>\n",
              "      <td>0.752745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2351</td>\n",
              "      <td>20</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.993029</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.992962</td>\n",
              "      <td>0.997845</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997824</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007790</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.398112</td>\n",
              "      <td>0.008580</td>\n",
              "      <td>0.004983</td>\n",
              "      <td>0.051888</td>\n",
              "      <td>0.002176</td>\n",
              "      <td>1.130334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2361</td>\n",
              "      <td>30</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991462</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.965517</td>\n",
              "      <td>0.991870</td>\n",
              "      <td>0.995713</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995647</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.007790</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.397254</td>\n",
              "      <td>0.012870</td>\n",
              "      <td>0.029531</td>\n",
              "      <td>0.102746</td>\n",
              "      <td>0.004353</td>\n",
              "      <td>1.258639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2371</td>\n",
              "      <td>40</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991480</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.941176</td>\n",
              "      <td>0.992408</td>\n",
              "      <td>0.995722</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995647</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.007082</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.396825</td>\n",
              "      <td>0.017160</td>\n",
              "      <td>0.040302</td>\n",
              "      <td>0.053175</td>\n",
              "      <td>0.004353</td>\n",
              "      <td>1.134000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2381</td>\n",
              "      <td>50</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991543</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.956522</td>\n",
              "      <td>0.992416</td>\n",
              "      <td>0.996812</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.007790</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>0.397683</td>\n",
              "      <td>0.021450</td>\n",
              "      <td>0.033451</td>\n",
              "      <td>0.082317</td>\n",
              "      <td>0.003264</td>\n",
              "      <td>1.206990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2406</td>\n",
              "      <td>75</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.992716</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.974359</td>\n",
              "      <td>0.993492</td>\n",
              "      <td>0.996865</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.054054</td>\n",
              "      <td>0.006374</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.396825</td>\n",
              "      <td>0.032175</td>\n",
              "      <td>0.025472</td>\n",
              "      <td>0.136508</td>\n",
              "      <td>0.003264</td>\n",
              "      <td>1.344000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2431</td>\n",
              "      <td>100</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991795</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.980769</td>\n",
              "      <td>0.992416</td>\n",
              "      <td>0.996907</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.040816</td>\n",
              "      <td>0.007790</td>\n",
              "      <td>0.530000</td>\n",
              "      <td>0.397683</td>\n",
              "      <td>0.042900</td>\n",
              "      <td>0.018145</td>\n",
              "      <td>0.132317</td>\n",
              "      <td>0.003264</td>\n",
              "      <td>1.332718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2456</td>\n",
              "      <td>125</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.992397</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.976744</td>\n",
              "      <td>0.993492</td>\n",
              "      <td>0.996945</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.048387</td>\n",
              "      <td>0.006374</td>\n",
              "      <td>0.528000</td>\n",
              "      <td>0.396825</td>\n",
              "      <td>0.053625</td>\n",
              "      <td>0.022639</td>\n",
              "      <td>0.131175</td>\n",
              "      <td>0.003264</td>\n",
              "      <td>1.330560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2481</td>\n",
              "      <td>150</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991555</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.982036</td>\n",
              "      <td>0.992416</td>\n",
              "      <td>0.997003</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.044118</td>\n",
              "      <td>0.007790</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.397683</td>\n",
              "      <td>0.064350</td>\n",
              "      <td>0.019796</td>\n",
              "      <td>0.168983</td>\n",
              "      <td>0.003264</td>\n",
              "      <td>1.424919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2506</td>\n",
              "      <td>175</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.990673</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.978947</td>\n",
              "      <td>0.991879</td>\n",
              "      <td>0.996051</td>\n",
              "      <td>0.989362</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.037037</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.548571</td>\n",
              "      <td>0.398112</td>\n",
              "      <td>0.075075</td>\n",
              "      <td>0.017956</td>\n",
              "      <td>0.150459</td>\n",
              "      <td>-0.007374</td>\n",
              "      <td>1.377931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2531</td>\n",
              "      <td>200</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991304</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.981982</td>\n",
              "      <td>0.992424</td>\n",
              "      <td>0.997085</td>\n",
              "      <td>0.990909</td>\n",
              "      <td>0.997824</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.398541</td>\n",
              "      <td>0.085800</td>\n",
              "      <td>0.015875</td>\n",
              "      <td>0.161459</td>\n",
              "      <td>-0.006915</td>\n",
              "      <td>1.405124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2581</td>\n",
              "      <td>250</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991042</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.985401</td>\n",
              "      <td>0.991879</td>\n",
              "      <td>0.996209</td>\n",
              "      <td>0.992647</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.026316</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.552000</td>\n",
              "      <td>0.398112</td>\n",
              "      <td>0.107250</td>\n",
              "      <td>0.010953</td>\n",
              "      <td>0.153888</td>\n",
              "      <td>-0.004089</td>\n",
              "      <td>1.386543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2631</td>\n",
              "      <td>300</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.990809</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.981707</td>\n",
              "      <td>0.992424</td>\n",
              "      <td>0.995383</td>\n",
              "      <td>0.981707</td>\n",
              "      <td>0.997824</td>\n",
              "      <td>0.022059</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.546667</td>\n",
              "      <td>0.398541</td>\n",
              "      <td>0.128700</td>\n",
              "      <td>0.014838</td>\n",
              "      <td>0.148125</td>\n",
              "      <td>-0.016116</td>\n",
              "      <td>1.371668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2681</td>\n",
              "      <td>350</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.990942</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.980716</td>\n",
              "      <td>0.992954</td>\n",
              "      <td>0.994545</td>\n",
              "      <td>0.983425</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.023669</td>\n",
              "      <td>0.007082</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.397254</td>\n",
              "      <td>0.150150</td>\n",
              "      <td>0.014948</td>\n",
              "      <td>0.122746</td>\n",
              "      <td>-0.013310</td>\n",
              "      <td>1.308985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2731</td>\n",
              "      <td>400</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.990308</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.983452</td>\n",
              "      <td>0.991879</td>\n",
              "      <td>0.994690</td>\n",
              "      <td>0.985782</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.021164</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.530000</td>\n",
              "      <td>0.398112</td>\n",
              "      <td>0.171600</td>\n",
              "      <td>0.011810</td>\n",
              "      <td>0.131888</td>\n",
              "      <td>-0.010954</td>\n",
              "      <td>1.331282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2781</td>\n",
              "      <td>450</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.990501</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.982906</td>\n",
              "      <td>0.992424</td>\n",
              "      <td>0.994796</td>\n",
              "      <td>0.982906</td>\n",
              "      <td>0.997824</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.398541</td>\n",
              "      <td>0.193050</td>\n",
              "      <td>0.012469</td>\n",
              "      <td>0.121459</td>\n",
              "      <td>-0.014918</td>\n",
              "      <td>1.304758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2831</td>\n",
              "      <td>500</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.990733</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.986717</td>\n",
              "      <td>0.991879</td>\n",
              "      <td>0.994924</td>\n",
              "      <td>0.988593</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.016878</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.528000</td>\n",
              "      <td>0.398112</td>\n",
              "      <td>0.214500</td>\n",
              "      <td>0.008261</td>\n",
              "      <td>0.129888</td>\n",
              "      <td>-0.008142</td>\n",
              "      <td>1.326259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2931</td>\n",
              "      <td>600</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.990700</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.987220</td>\n",
              "      <td>0.991879</td>\n",
              "      <td>0.995126</td>\n",
              "      <td>0.990385</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.017361</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.523333</td>\n",
              "      <td>0.398112</td>\n",
              "      <td>0.257400</td>\n",
              "      <td>0.007607</td>\n",
              "      <td>0.125221</td>\n",
              "      <td>-0.006351</td>\n",
              "      <td>1.314537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>3031</td>\n",
              "      <td>700</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991096</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.990502</td>\n",
              "      <td>0.991333</td>\n",
              "      <td>0.996109</td>\n",
              "      <td>0.997268</td>\n",
              "      <td>0.995647</td>\n",
              "      <td>0.017964</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.530000</td>\n",
              "      <td>0.397683</td>\n",
              "      <td>0.300300</td>\n",
              "      <td>0.005543</td>\n",
              "      <td>0.132317</td>\n",
              "      <td>0.001620</td>\n",
              "      <td>1.332718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>3131</td>\n",
              "      <td>800</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991376</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.990268</td>\n",
              "      <td>0.991870</td>\n",
              "      <td>0.995482</td>\n",
              "      <td>0.995110</td>\n",
              "      <td>0.995647</td>\n",
              "      <td>0.015345</td>\n",
              "      <td>0.007790</td>\n",
              "      <td>0.516250</td>\n",
              "      <td>0.397254</td>\n",
              "      <td>0.343200</td>\n",
              "      <td>0.004046</td>\n",
              "      <td>0.118996</td>\n",
              "      <td>-0.000537</td>\n",
              "      <td>1.299545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>3231</td>\n",
              "      <td>900</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991664</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.990164</td>\n",
              "      <td>0.992408</td>\n",
              "      <td>0.995633</td>\n",
              "      <td>0.995604</td>\n",
              "      <td>0.995647</td>\n",
              "      <td>0.015730</td>\n",
              "      <td>0.007082</td>\n",
              "      <td>0.511111</td>\n",
              "      <td>0.396825</td>\n",
              "      <td>0.386100</td>\n",
              "      <td>0.004346</td>\n",
              "      <td>0.114286</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>1.288000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3331</td>\n",
              "      <td>1000</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991573</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.989055</td>\n",
              "      <td>0.992946</td>\n",
              "      <td>0.995769</td>\n",
              "      <td>0.995992</td>\n",
              "      <td>0.995647</td>\n",
              "      <td>0.017964</td>\n",
              "      <td>0.006374</td>\n",
              "      <td>0.506000</td>\n",
              "      <td>0.396396</td>\n",
              "      <td>0.429000</td>\n",
              "      <td>0.005967</td>\n",
              "      <td>0.109604</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>1.276500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>3581</td>\n",
              "      <td>1250</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.990335</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.988906</td>\n",
              "      <td>0.991314</td>\n",
              "      <td>0.994179</td>\n",
              "      <td>0.995215</td>\n",
              "      <td>0.993471</td>\n",
              "      <td>0.017657</td>\n",
              "      <td>0.007082</td>\n",
              "      <td>0.508000</td>\n",
              "      <td>0.395967</td>\n",
              "      <td>0.536251</td>\n",
              "      <td>0.006159</td>\n",
              "      <td>0.112033</td>\n",
              "      <td>0.001744</td>\n",
              "      <td>1.282934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>3831</td>\n",
              "      <td>1500</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991682</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.991464</td>\n",
              "      <td>0.991861</td>\n",
              "      <td>0.995823</td>\n",
              "      <td>0.997358</td>\n",
              "      <td>0.994559</td>\n",
              "      <td>0.014805</td>\n",
              "      <td>0.007082</td>\n",
              "      <td>0.510667</td>\n",
              "      <td>0.396396</td>\n",
              "      <td>0.643501</td>\n",
              "      <td>0.005261</td>\n",
              "      <td>0.114270</td>\n",
              "      <td>0.002799</td>\n",
              "      <td>1.288273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>4081</td>\n",
              "      <td>1750</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.990519</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.989099</td>\n",
              "      <td>0.991861</td>\n",
              "      <td>0.993844</td>\n",
              "      <td>0.993088</td>\n",
              "      <td>0.994559</td>\n",
              "      <td>0.014739</td>\n",
              "      <td>0.007082</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.396396</td>\n",
              "      <td>0.750751</td>\n",
              "      <td>0.004564</td>\n",
              "      <td>0.103604</td>\n",
              "      <td>-0.001472</td>\n",
              "      <td>1.261364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>4331</td>\n",
              "      <td>2000</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991187</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.990571</td>\n",
              "      <td>0.991861</td>\n",
              "      <td>0.995315</td>\n",
              "      <td>0.996008</td>\n",
              "      <td>0.994559</td>\n",
              "      <td>0.015030</td>\n",
              "      <td>0.007082</td>\n",
              "      <td>0.506500</td>\n",
              "      <td>0.396396</td>\n",
              "      <td>0.858001</td>\n",
              "      <td>0.004698</td>\n",
              "      <td>0.110104</td>\n",
              "      <td>0.001449</td>\n",
              "      <td>1.277761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>4581</td>\n",
              "      <td>2250</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.989022</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.986253</td>\n",
              "      <td>0.992408</td>\n",
              "      <td>0.992654</td>\n",
              "      <td>0.990205</td>\n",
              "      <td>0.995647</td>\n",
              "      <td>0.017746</td>\n",
              "      <td>0.007082</td>\n",
              "      <td>0.503111</td>\n",
              "      <td>0.396825</td>\n",
              "      <td>0.965251</td>\n",
              "      <td>0.008053</td>\n",
              "      <td>0.106286</td>\n",
              "      <td>-0.005443</td>\n",
              "      <td>1.267840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>4831</td>\n",
              "      <td>2500</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.990122</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.988451</td>\n",
              "      <td>0.992400</td>\n",
              "      <td>0.994462</td>\n",
              "      <td>0.994391</td>\n",
              "      <td>0.994559</td>\n",
              "      <td>0.017572</td>\n",
              "      <td>0.006374</td>\n",
              "      <td>0.505200</td>\n",
              "      <td>0.395967</td>\n",
              "      <td>1.072501</td>\n",
              "      <td>0.005683</td>\n",
              "      <td>0.109233</td>\n",
              "      <td>-0.000168</td>\n",
              "      <td>1.275863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>5081</td>\n",
              "      <td>2750</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.989660</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.987848</td>\n",
              "      <td>0.992408</td>\n",
              "      <td>0.993512</td>\n",
              "      <td>0.992103</td>\n",
              "      <td>0.995647</td>\n",
              "      <td>0.016949</td>\n",
              "      <td>0.007082</td>\n",
              "      <td>0.510909</td>\n",
              "      <td>0.396825</td>\n",
              "      <td>1.179751</td>\n",
              "      <td>0.006706</td>\n",
              "      <td>0.114084</td>\n",
              "      <td>-0.003544</td>\n",
              "      <td>1.287491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>5331</td>\n",
              "      <td>3000</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.988158</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.986248</td>\n",
              "      <td>0.991323</td>\n",
              "      <td>0.992210</td>\n",
              "      <td>0.990789</td>\n",
              "      <td>0.994559</td>\n",
              "      <td>0.018919</td>\n",
              "      <td>0.007790</td>\n",
              "      <td>0.511333</td>\n",
              "      <td>0.396825</td>\n",
              "      <td>1.287001</td>\n",
              "      <td>0.007449</td>\n",
              "      <td>0.114508</td>\n",
              "      <td>-0.003770</td>\n",
              "      <td>1.288560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>5581</td>\n",
              "      <td>3250</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.987327</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.984784</td>\n",
              "      <td>0.991861</td>\n",
              "      <td>0.992163</td>\n",
              "      <td>0.990814</td>\n",
              "      <td>0.994559</td>\n",
              "      <td>0.021645</td>\n",
              "      <td>0.007082</td>\n",
              "      <td>0.508615</td>\n",
              "      <td>0.396396</td>\n",
              "      <td>1.394251</td>\n",
              "      <td>0.009154</td>\n",
              "      <td>0.112219</td>\n",
              "      <td>-0.003745</td>\n",
              "      <td>1.283098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>5831</td>\n",
              "      <td>3500</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.987854</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.985233</td>\n",
              "      <td>0.992954</td>\n",
              "      <td>0.994074</td>\n",
              "      <td>0.992701</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.023269</td>\n",
              "      <td>0.007082</td>\n",
              "      <td>0.516571</td>\n",
              "      <td>0.397254</td>\n",
              "      <td>1.501502</td>\n",
              "      <td>0.010111</td>\n",
              "      <td>0.119317</td>\n",
              "      <td>-0.004035</td>\n",
              "      <td>1.300354</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    rows_complete  rows_minority  ...  equal_opport_dist  disparate_impact\n",
              "0            2336              5  ...           0.002176          0.501830\n",
              "1            2341             10  ...           0.002176          0.752745\n",
              "2            2351             20  ...           0.002176          1.130334\n",
              "3            2361             30  ...           0.004353          1.258639\n",
              "4            2371             40  ...           0.004353          1.134000\n",
              "5            2381             50  ...           0.003264          1.206990\n",
              "6            2406             75  ...           0.003264          1.344000\n",
              "7            2431            100  ...           0.003264          1.332718\n",
              "8            2456            125  ...           0.003264          1.330560\n",
              "9            2481            150  ...           0.003264          1.424919\n",
              "10           2506            175  ...          -0.007374          1.377931\n",
              "11           2531            200  ...          -0.006915          1.405124\n",
              "12           2581            250  ...          -0.004089          1.386543\n",
              "13           2631            300  ...          -0.016116          1.371668\n",
              "14           2681            350  ...          -0.013310          1.308985\n",
              "15           2731            400  ...          -0.010954          1.331282\n",
              "16           2781            450  ...          -0.014918          1.304758\n",
              "17           2831            500  ...          -0.008142          1.326259\n",
              "18           2931            600  ...          -0.006351          1.314537\n",
              "19           3031            700  ...           0.001620          1.332718\n",
              "20           3131            800  ...          -0.000537          1.299545\n",
              "21           3231            900  ...          -0.000043          1.288000\n",
              "22           3331           1000  ...           0.000345          1.276500\n",
              "23           3581           1250  ...           0.001744          1.282934\n",
              "24           3831           1500  ...           0.002799          1.288273\n",
              "25           4081           1750  ...          -0.001472          1.261364\n",
              "26           4331           2000  ...           0.001449          1.277761\n",
              "27           4581           2250  ...          -0.005443          1.267840\n",
              "28           4831           2500  ...          -0.000168          1.275863\n",
              "29           5081           2750  ...          -0.003544          1.287491\n",
              "30           5331           3000  ...          -0.003770          1.288560\n",
              "31           5581           3250  ...          -0.003745          1.283098\n",
              "32           5831           3500  ...          -0.004035          1.300354\n",
              "\n",
              "[33 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1230qeIkcp-a",
        "colab_type": "text"
      },
      "source": [
        "#### 4) Create visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2DKNaYH3X9j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "aab5ef9f-9d7c-4a32-d5a6-214f3364f964"
      },
      "source": [
        "print(df_compas[\"two year recid\"].unique())\n",
        "print(df_compas.shape)\n",
        "df_compas[\"two year recid\"].isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.  1. nan]\n",
            "(6874, 50)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVvVG_BxzFMF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "3c546c15-0f1e-4640-d300-ff37de2f00ad"
      },
      "source": [
        "# Get Feature Importance\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "from matplotlib import pyplot\n",
        "\n",
        "compas_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=1,\n",
        "              min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1.2, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "compas_model.fit(df_compas_train_input, df_compas_train_label)\n",
        "plot_importance(compas_model)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-748cb64c90d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m               \u001b[0mreg_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_lambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_pos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m               silent=None, subsample=1, verbosity=1)\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mcompas_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_compas_train_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_compas_train_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mplot_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompas_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             train_dmatrix = DMatrix(X, label=training_labels,\n\u001b[0;32m--> 726\u001b[0;31m                                     missing=self.missing, nthread=self.n_jobs)\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         self._Booster = train(xgb_options, train_dmatrix, self.get_num_boosting_rounds(),\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[1;32m    378\u001b[0m         data, feature_names, feature_types = _maybe_pandas_data(data,\n\u001b[1;32m    379\u001b[0m                                                                 \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                                                                 feature_types)\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         data, feature_names, feature_types = _maybe_dt_data(data,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_maybe_pandas_data\u001b[0;34m(data, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    237\u001b[0m         msg = \"\"\"DataFrame.dtypes for data must be int, float or bool.\n\u001b[1;32m    238\u001b[0m                 Did not expect the data types in fields \"\"\"\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbad_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float or bool.\n                Did not expect the data types in fields first, last, compas screening date, sex, dob, race, c jail in, c jail out, c case number, c offense date, c arrest date, c charge degree, c charge desc, r case number, r charge degree, r offense date, r charge desc, r jail in, r jail out, vr case number, vr charge degree, vr offense date, vr charge desc, type of assessment, score text, screening date, v type of assessment, v score text, v screening date, in custody, out custody"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qeup5l7RXeoe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "c05019ad-72ea-4ba7-f184-798d5e6b51fa"
      },
      "source": [
        "# B: Learning Curve Function from scikit learn\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "compas_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=1,\n",
        "              min_child_weight=1, missing=None, n_estimators=150, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "\n",
        "# Define arguments\n",
        "f1 = make_scorer(f1_score) \n",
        "\n",
        "df_compas_train_input = pd.get_dummies(df_compas_train_input)\n",
        "\n",
        "# Plot actual learning curve\n",
        "plot_learning_curve(estimator = compas_model, \n",
        "                    title = \"COMPAS Dataset - XGBoost Learning Curve\", \n",
        "                    X = df_compas_train_input, y = df_compas_train_label, \n",
        "                    cv = 5, \n",
        "                    scoring = f1, \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = np.linspace(.1, 1.0, 10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'matplotlib.pyplot' from '/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Sdh1b0FYzDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_metrics_line_chart(results_df_compas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwnExnGiYzmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maj_min_metrics_line_chart(results_df_compas) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BYLBIMnHMHQy"
      },
      "source": [
        "## 3) Homicide Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inbdFqBmv7QI",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otsY0WTLCVdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_homicide = \"/content/drive/My Drive/Master Thesis/Data/homicide_dataset.csv\"\n",
        "df_homicide_full = pd.read_csv(path_homicide, low_memory=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8zayFMxm3vA",
        "colab_type": "text"
      },
      "source": [
        "### Initial Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx41WDcL4eAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoding Binary \n",
        "df_homicide_full[\"Perpetrator\"] = df_homicide_full[\"Perpetrator Count\"].apply(lambda val: False if val >= 1 else val == 0)\n",
        "\n",
        "# Replace False by 1 and True by 0\n",
        "df_homicide_full[\"Perpetrator\"] = df_homicide_full[\"Perpetrator\"].replace({False: 0, True: 1})\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_homicide_full = df_homicide_full.drop(['Record ID', \"Perpetrator Count\", \"Victim Count\"], axis=1)\n",
        "\n",
        "# Replace values by NaNs\n",
        "df_homicide_full.replace('Unknown', np.nan)\n",
        "\n",
        "# Filter dataset based on year to reduce file size\n",
        "df_homicide = df_homicide_full[df_homicide_full[\"Year\"] > 2012]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNViB1x8W96j",
        "colab_type": "text"
      },
      "source": [
        "**Dataframe Size Reduction Efforts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2s6DoTaZLOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_homicide.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ClezErWV94Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aa2e3cfe-f28b-4ee3-cef4-7884c1034103"
      },
      "source": [
        "# Reduce the size of the numeric columns\n",
        "\n",
        "df_homicide, NAlist = reduce_mem_usage(df_homicide)\n",
        "print(\"_________________\")\n",
        "print(\"\")\n",
        "print(\"Warning: the following columns have missing values filled with 'df['column_name'].min() -1': \")\n",
        "print(\"_________________\")\n",
        "print(\"\")\n",
        "print(NAlist)\n",
        "\n",
        "# Reduce the size of the string columns\n",
        "df_homicide[\"Agency Code\"]=df_homicide[\"Agency Code\"].astype(\"category\")\n",
        "df_homicide[\"Agency Name\"]=df_homicide[\"Agency Name\"].astype(\"category\")\n",
        "df_homicide[\"Agency Type\"]=df_homicide[\"Agency Type\"].astype(\"category\")\n",
        "df_homicide[\"City\"]=df_homicide[\"City\"].astype(\"category\")\n",
        "df_homicide[\"State\"]=df_homicide[\"State\"].astype(\"category\")\n",
        "df_homicide[\"Month\"]=df_homicide[\"Month\"].astype(\"category\")\n",
        "df_homicide[\"Crime Type\"]=df_homicide[\"Crime Type\"].astype(\"category\")\n",
        "df_homicide[\"Crime Solved\"]=df_homicide[\"Crime Solved\"].astype(\"category\")\n",
        "df_homicide[\"Victim Sex\"]=df_homicide[\"Victim Sex\"].astype(\"category\")\n",
        "df_homicide[\"Perpetrator Age\"]=df_homicide[\"Perpetrator Age\"].astype(\"category\")\n",
        "df_homicide[\"Relationship\"]=df_homicide[\"Relationship\"].astype(\"category\")\n",
        "df_homicide[\"Weapon\"]=df_homicide[\"Weapon\"].astype(\"category\")\n",
        "df_homicide[\"Record Source\"]=df_homicide[\"Record Source\"].astype(\"category\")\n",
        "\n",
        "## Columns that a part of further preprocessing methods\n",
        "df_homicide[\"Victim Race\"]=df_homicide[\"Victim Race\"].astype(\"category\")\n",
        "df_homicide[\"Victim Ethnicity\"]=df_homicide[\"Victim Ethnicity\"].astype(\"category\")\n",
        "df_homicide[\"Perpetrator Race\"]=df_homicide[\"Perpetrator Race\"].astype(\"category\")\n",
        "df_homicide[\"Perpetrator Sex\"]=df_homicide[\"Perpetrator Sex\"].astype(\"category\")\n",
        "df_homicide[\"Perpetrator Ethnicity\"]=df_homicide[\"Perpetrator Ethnicity\"].astype(\"category\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of properties dataframe is : 5.04949951171875  MB\n",
            "******************************\n",
            "Column:  Year\n",
            "dtype before:  int64\n",
            "dtype after:  uint16\n",
            "******************************\n",
            "******************************\n",
            "Column:  Incident\n",
            "dtype before:  int64\n",
            "dtype after:  uint16\n",
            "******************************\n",
            "******************************\n",
            "Column:  Victim Age\n",
            "dtype before:  int64\n",
            "dtype after:  uint16\n",
            "******************************\n",
            "******************************\n",
            "Column:  Perpetrator\n",
            "dtype before:  int64\n",
            "dtype after:  uint8\n",
            "******************************\n",
            "___MEMORY USAGE AFTER COMPLETION:___\n",
            "Memory usage is:  4.363426208496094  MB\n",
            "This is  86.41304347826087 % of the initial size\n",
            "_________________\n",
            "\n",
            "Warning: the following columns have missing values filled with 'df['column_name'].min() -1': \n",
            "_________________\n",
            "\n",
            "[]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:517: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:515: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi0QBN_GHPEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check which columns are suitable for conversion in \"category\" data format\n",
        "## We should stick to using the category type primarily for object columns where less than 50% of the values are unique.\n",
        "df_homicide_copy = df_homicide.select_dtypes(include=['object']).copy()\n",
        "df_homicide_copy.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3W2Wqu_hZTPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_72jile7c6Dq",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYfLDcqBU81J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_homicide.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpbuBrUd3aOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_homicide.groupby([\"Perpetrator\"]).agg({\"Perpetrator\": 'count'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzCYLwOCEzZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_homicide.info)\n",
        "print(df_homicide.describe)\n",
        "print(df_homicide.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M963RlbU9eWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_homicide.head(n=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmSHfWmP4k1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eda_descr_stats(data = df_homicide, disc_feature=\"Perpetrator Race\", disc_min_value=\"Black\", label = \"Perpetrator\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C01BFbz3daQn",
        "colab_type": "text"
      },
      "source": [
        "### Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFcHFLoqdh5L",
        "colab_type": "text"
      },
      "source": [
        "#### 0) Preprocess data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSOE-LXCFGOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute preprocessing function\n",
        "df_homicide_train_input, df_homicide_train_label = fair_preprocess(data = df_homicide,\n",
        "                                                                   label = \"Perpetrator\",\n",
        "                                                                   neg_class = 0,\n",
        "                                                                   pos_class = 1)\n",
        "\n",
        "# Check whether binary encoding was successful and seperate datasets were created\n",
        "print(df_homicide.groupby([\"Perpetrator\"]).agg({\"Perpetrator\": 'count'}))\n",
        "print(df_homicide_train_input.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBuKJ_7udqW6",
        "colab_type": "text"
      },
      "source": [
        "#### 1) Define Hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvTp6Q85CMzW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_homicide_train_input.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vknxj1bdnisU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the hyperparameter grid\n",
        "param_grid = {\"learning_rate\": [0.3],\n",
        "                \"n_estimators\": [10, 50, 100],\n",
        "                \"max_depth\": [3, 7], \n",
        "                \"min_child_weight\": [1, 4],       \n",
        "                \"reg_lambda\": [1]}\n",
        "\n",
        "xgb_grid_search = XGBClassifier(objective= 'binary:logistic', nthread=4)\n",
        "cv=3\n",
        "\n",
        "# Learning Curve for Slice \n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define model\n",
        "f1 = make_scorer(f1_score)\n",
        "\n",
        "# Dummy Coding\n",
        "df_train_input_dummy = pd.get_dummies(df_homicide_train_input)\n",
        "\n",
        "grid_rf_class = sklearn.model_selection.GridSearchCV(estimator = xgb_grid_search,\n",
        "                                                      param_grid = param_grid, \n",
        "                                                      scoring= f1,\n",
        "                                                      cv = cv,\n",
        "                                                      refit = True,\n",
        "                                                      return_train_score = False)\n",
        "  \n",
        "# Fit model\n",
        "grid_rf_class.fit(df_train_input_dummy, df_homicide_train_label)\n",
        "\n",
        "print(grid_rf_class.best_params_)\n",
        "print(grid_rf_class.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4WE7mKdF1UU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_classifier_homicide = hyperparameter_tuning(df_homicide_train_input, df_homicide_train_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN65JoIHdxiv",
        "colab_type": "text"
      },
      "source": [
        "#### 2) Create datasets with diff. minority group sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2V-5rfYVdjz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "454d4aba-4d6c-474e-c6f7-530adbc8e4f1"
      },
      "source": [
        "training_sizes_homicide = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                          700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                          4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Creating dfs for minority and majority group\n",
        "is_black = df_homicide[\"Perpetrator Race\"].isin([\"Black\"])\n",
        "is_white = df_homicide[\"Perpetrator Race\"].isin([\"White\"])\n",
        "df_homicide_black = df_homicide[is_black]  # Minority\n",
        "df_homicide_white = df_homicide[is_white]  # Majority\n",
        "\n",
        "list_dfs_homicide = create_datasets(min_data = df_homicide_black, maj_data = df_homicide_white, training_sizes=training_sizes_homicide)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "43\n",
            "[9871, 9876, 9886, 9896, 9906, 9916, 9941, 9966, 9991, 10016, 10041, 10066, 10116, 10166, 10216, 10266, 10316, 10366, 10466, 10566, 10666, 10766, 10866, 11116, 11366, 11616, 11866, 12116, 12366, 12616, 12866, 13116, 13366, 13616, 13866, 14116, 14366, 14616, 14866, 15866, 16866, 17866, 18866]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uIyMWH8d7ma",
        "colab_type": "text"
      },
      "source": [
        "#### 3) Create dataframes with diff. metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbsp9jPVVeHT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7e78bc43-eb5e-4f24-87b2-74f2ecda2d07"
      },
      "source": [
        "# Define arguments\n",
        "label = \"Perpetrator\"\n",
        "homicide_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "                               colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "                               learning_rate=0.3, max_delta_step=0, max_depth=3,\n",
        "                               min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
        "                               nthread=4, objective='binary:logistic', random_state=0,\n",
        "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "                               silent=None, subsample=1, verbosity=1)\n",
        "cv = 5 \n",
        "discr_feature = \"Perpetrator Race\"\n",
        "min_value = \"Black\"\n",
        "maj_value = \"White\"\n",
        "\n",
        "# Run function\n",
        "results_df_homicide = metrics_to_df(list_dfs=list_dfs_homicide, label = label, model = homicide_model, \n",
        "                                  cv = cv, discr_feature = discr_feature, min_value = min_value,\n",
        "                                  maj_value = maj_value)\n",
        "results_df_homicide\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rows_complete</th>\n",
              "      <th>rows_minority</th>\n",
              "      <th>rows_majority</th>\n",
              "      <th>f1_complete</th>\n",
              "      <th>f1_complete_train</th>\n",
              "      <th>f1_minority</th>\n",
              "      <th>f1_majority</th>\n",
              "      <th>tpr_complete</th>\n",
              "      <th>tpr_minority</th>\n",
              "      <th>tpr_majority</th>\n",
              "      <th>fpr_minority</th>\n",
              "      <th>fpr_majority</th>\n",
              "      <th>prob_yhat_1_minority</th>\n",
              "      <th>prob_yhat_1_majority</th>\n",
              "      <th>rel_share_min_of_maj</th>\n",
              "      <th>aver_abs_odds_diff</th>\n",
              "      <th>stat_parity_diff</th>\n",
              "      <th>equal_opport_dist</th>\n",
              "      <th>disparate_impact</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9871</td>\n",
              "      <td>5</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.904307</td>\n",
              "      <td>0.904697</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.904376</td>\n",
              "      <td>0.999755</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999754</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999419</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999696</td>\n",
              "      <td>0.000507</td>\n",
              "      <td>0.000413</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.000246</td>\n",
              "      <td>1.000304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9876</td>\n",
              "      <td>10</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.904046</td>\n",
              "      <td>0.904673</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.904183</td>\n",
              "      <td>0.999264</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999263</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998838</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999189</td>\n",
              "      <td>0.001014</td>\n",
              "      <td>0.000949</td>\n",
              "      <td>0.000811</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>1.000812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9886</td>\n",
              "      <td>20</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.904295</td>\n",
              "      <td>0.904933</td>\n",
              "      <td>0.918919</td>\n",
              "      <td>0.904265</td>\n",
              "      <td>0.999632</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999632</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999696</td>\n",
              "      <td>0.002027</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>1.000304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9896</td>\n",
              "      <td>30</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.904330</td>\n",
              "      <td>0.904763</td>\n",
              "      <td>0.867925</td>\n",
              "      <td>0.904437</td>\n",
              "      <td>0.999878</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999877</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999419</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999797</td>\n",
              "      <td>0.003041</td>\n",
              "      <td>0.000352</td>\n",
              "      <td>0.000203</td>\n",
              "      <td>0.000123</td>\n",
              "      <td>1.000203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9906</td>\n",
              "      <td>40</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.903961</td>\n",
              "      <td>0.904463</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.904265</td>\n",
              "      <td>0.999633</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999632</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999696</td>\n",
              "      <td>0.004054</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>1.000304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>9916</td>\n",
              "      <td>50</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.904428</td>\n",
              "      <td>0.904967</td>\n",
              "      <td>0.936170</td>\n",
              "      <td>0.904262</td>\n",
              "      <td>0.999023</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999018</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996514</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998581</td>\n",
              "      <td>0.005068</td>\n",
              "      <td>0.002234</td>\n",
              "      <td>0.001419</td>\n",
              "      <td>0.000982</td>\n",
              "      <td>1.001421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>9941</td>\n",
              "      <td>75</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.903667</td>\n",
              "      <td>0.904404</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.904082</td>\n",
              "      <td>0.999268</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999263</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999392</td>\n",
              "      <td>0.007602</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>0.000608</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>1.000609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>9966</td>\n",
              "      <td>100</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.903559</td>\n",
              "      <td>0.904239</td>\n",
              "      <td>0.843931</td>\n",
              "      <td>0.904132</td>\n",
              "      <td>0.999270</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999263</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999419</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999290</td>\n",
              "      <td>0.010136</td>\n",
              "      <td>0.000659</td>\n",
              "      <td>0.000710</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>1.000710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9991</td>\n",
              "      <td>125</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.903764</td>\n",
              "      <td>0.904221</td>\n",
              "      <td>0.853211</td>\n",
              "      <td>0.904376</td>\n",
              "      <td>0.999757</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999754</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999419</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999696</td>\n",
              "      <td>0.012670</td>\n",
              "      <td>0.000413</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.000246</td>\n",
              "      <td>1.000304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10016</td>\n",
              "      <td>150</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.903738</td>\n",
              "      <td>0.903968</td>\n",
              "      <td>0.854962</td>\n",
              "      <td>0.904447</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.015204</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10041</td>\n",
              "      <td>175</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.903680</td>\n",
              "      <td>0.904116</td>\n",
              "      <td>0.867314</td>\n",
              "      <td>0.904304</td>\n",
              "      <td>0.999517</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999509</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998838</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999392</td>\n",
              "      <td>0.017738</td>\n",
              "      <td>0.000827</td>\n",
              "      <td>0.000608</td>\n",
              "      <td>0.000491</td>\n",
              "      <td>1.000609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>10066</td>\n",
              "      <td>200</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.903444</td>\n",
              "      <td>0.903869</td>\n",
              "      <td>0.853868</td>\n",
              "      <td>0.904405</td>\n",
              "      <td>0.999518</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999509</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997676</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999189</td>\n",
              "      <td>0.020272</td>\n",
              "      <td>0.001408</td>\n",
              "      <td>0.000811</td>\n",
              "      <td>0.000491</td>\n",
              "      <td>1.000812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>10116</td>\n",
              "      <td>250</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.902977</td>\n",
              "      <td>0.903670</td>\n",
              "      <td>0.855835</td>\n",
              "      <td>0.904122</td>\n",
              "      <td>0.999160</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999141</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998838</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999088</td>\n",
              "      <td>0.025340</td>\n",
              "      <td>0.001011</td>\n",
              "      <td>0.000912</td>\n",
              "      <td>0.000859</td>\n",
              "      <td>1.000913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>10166</td>\n",
              "      <td>300</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.902344</td>\n",
              "      <td>0.902904</td>\n",
              "      <td>0.830409</td>\n",
              "      <td>0.904394</td>\n",
              "      <td>0.999402</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999386</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997095</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998986</td>\n",
              "      <td>0.030407</td>\n",
              "      <td>0.001760</td>\n",
              "      <td>0.001014</td>\n",
              "      <td>0.000614</td>\n",
              "      <td>1.001015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>10216</td>\n",
              "      <td>350</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.903039</td>\n",
              "      <td>0.903315</td>\n",
              "      <td>0.861789</td>\n",
              "      <td>0.904447</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.035475</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>10266</td>\n",
              "      <td>400</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.901766</td>\n",
              "      <td>0.902976</td>\n",
              "      <td>0.852224</td>\n",
              "      <td>0.903685</td>\n",
              "      <td>0.998223</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998158</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998257</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998176</td>\n",
              "      <td>0.040543</td>\n",
              "      <td>0.001792</td>\n",
              "      <td>0.001824</td>\n",
              "      <td>0.001842</td>\n",
              "      <td>1.001828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>10316</td>\n",
              "      <td>450</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.901129</td>\n",
              "      <td>0.903317</td>\n",
              "      <td>0.865069</td>\n",
              "      <td>0.902720</td>\n",
              "      <td>0.996466</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996317</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998838</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996757</td>\n",
              "      <td>0.045611</td>\n",
              "      <td>0.002423</td>\n",
              "      <td>0.003243</td>\n",
              "      <td>0.003683</td>\n",
              "      <td>1.003254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>10366</td>\n",
              "      <td>500</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.902129</td>\n",
              "      <td>0.903262</td>\n",
              "      <td>0.871332</td>\n",
              "      <td>0.903645</td>\n",
              "      <td>0.998359</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998281</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999419</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998480</td>\n",
              "      <td>0.050679</td>\n",
              "      <td>0.001150</td>\n",
              "      <td>0.001520</td>\n",
              "      <td>0.001719</td>\n",
              "      <td>1.001523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>10466</td>\n",
              "      <td>600</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.901244</td>\n",
              "      <td>0.901898</td>\n",
              "      <td>0.853868</td>\n",
              "      <td>0.904000</td>\n",
              "      <td>0.998953</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998895</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998838</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998885</td>\n",
              "      <td>0.060815</td>\n",
              "      <td>0.001134</td>\n",
              "      <td>0.001115</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>1.001116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>10566</td>\n",
              "      <td>700</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.901290</td>\n",
              "      <td>0.901800</td>\n",
              "      <td>0.860862</td>\n",
              "      <td>0.904050</td>\n",
              "      <td>0.998962</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998895</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998257</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998784</td>\n",
              "      <td>0.070951</td>\n",
              "      <td>0.001424</td>\n",
              "      <td>0.001216</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>1.001218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>10666</td>\n",
              "      <td>800</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.901180</td>\n",
              "      <td>0.901806</td>\n",
              "      <td>0.860399</td>\n",
              "      <td>0.904362</td>\n",
              "      <td>0.999086</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999018</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995352</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998378</td>\n",
              "      <td>0.081087</td>\n",
              "      <td>0.002815</td>\n",
              "      <td>0.001622</td>\n",
              "      <td>0.000982</td>\n",
              "      <td>1.001624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>10766</td>\n",
              "      <td>900</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.900434</td>\n",
              "      <td>0.900778</td>\n",
              "      <td>0.854233</td>\n",
              "      <td>0.904466</td>\n",
              "      <td>0.999660</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999632</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997676</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999290</td>\n",
              "      <td>0.091222</td>\n",
              "      <td>0.001346</td>\n",
              "      <td>0.000710</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>1.000710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>10866</td>\n",
              "      <td>1000</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.901340</td>\n",
              "      <td>0.901425</td>\n",
              "      <td>0.868778</td>\n",
              "      <td>0.904537</td>\n",
              "      <td>0.999888</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999877</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998257</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999595</td>\n",
              "      <td>0.101358</td>\n",
              "      <td>0.000933</td>\n",
              "      <td>0.000405</td>\n",
              "      <td>0.000123</td>\n",
              "      <td>1.000406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>11116</td>\n",
              "      <td>1250</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.887138</td>\n",
              "      <td>0.899988</td>\n",
              "      <td>0.849100</td>\n",
              "      <td>0.891779</td>\n",
              "      <td>0.973994</td>\n",
              "      <td>0.989247</td>\n",
              "      <td>0.972253</td>\n",
              "      <td>0.990625</td>\n",
              "      <td>0.985474</td>\n",
              "      <td>0.989600</td>\n",
              "      <td>0.974559</td>\n",
              "      <td>0.126698</td>\n",
              "      <td>0.011073</td>\n",
              "      <td>0.015041</td>\n",
              "      <td>0.016994</td>\n",
              "      <td>1.015434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>11366</td>\n",
              "      <td>1500</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.897993</td>\n",
              "      <td>0.899794</td>\n",
              "      <td>0.863636</td>\n",
              "      <td>0.903036</td>\n",
              "      <td>0.997093</td>\n",
              "      <td>0.997375</td>\n",
              "      <td>0.997053</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999419</td>\n",
              "      <td>0.998000</td>\n",
              "      <td>0.997466</td>\n",
              "      <td>0.152037</td>\n",
              "      <td>0.000451</td>\n",
              "      <td>0.000534</td>\n",
              "      <td>0.000322</td>\n",
              "      <td>1.000535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>11616</td>\n",
              "      <td>1750</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.897736</td>\n",
              "      <td>0.898426</td>\n",
              "      <td>0.860957</td>\n",
              "      <td>0.904011</td>\n",
              "      <td>0.998944</td>\n",
              "      <td>0.998489</td>\n",
              "      <td>0.999018</td>\n",
              "      <td>0.997653</td>\n",
              "      <td>0.999419</td>\n",
              "      <td>0.998286</td>\n",
              "      <td>0.999088</td>\n",
              "      <td>0.177377</td>\n",
              "      <td>0.001147</td>\n",
              "      <td>-0.000802</td>\n",
              "      <td>-0.000528</td>\n",
              "      <td>0.999197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>11866</td>\n",
              "      <td>2000</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.850038</td>\n",
              "      <td>0.898053</td>\n",
              "      <td>0.346570</td>\n",
              "      <td>0.904254</td>\n",
              "      <td>0.877991</td>\n",
              "      <td>0.222517</td>\n",
              "      <td>0.999509</td>\n",
              "      <td>0.189796</td>\n",
              "      <td>0.999419</td>\n",
              "      <td>0.214500</td>\n",
              "      <td>0.999493</td>\n",
              "      <td>0.202716</td>\n",
              "      <td>0.793308</td>\n",
              "      <td>-0.784993</td>\n",
              "      <td>-0.776992</td>\n",
              "      <td>0.214609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>12116</td>\n",
              "      <td>2250</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.825178</td>\n",
              "      <td>0.897468</td>\n",
              "      <td>0.051143</td>\n",
              "      <td>0.904193</td>\n",
              "      <td>0.832181</td>\n",
              "      <td>0.027761</td>\n",
              "      <td>0.999386</td>\n",
              "      <td>0.175943</td>\n",
              "      <td>0.999419</td>\n",
              "      <td>0.064444</td>\n",
              "      <td>0.999392</td>\n",
              "      <td>0.228056</td>\n",
              "      <td>0.897551</td>\n",
              "      <td>-0.934947</td>\n",
              "      <td>-0.971625</td>\n",
              "      <td>0.064484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>12366</td>\n",
              "      <td>2500</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.813914</td>\n",
              "      <td>0.898355</td>\n",
              "      <td>0.041366</td>\n",
              "      <td>0.903208</td>\n",
              "      <td>0.813023</td>\n",
              "      <td>0.022643</td>\n",
              "      <td>0.997299</td>\n",
              "      <td>0.227953</td>\n",
              "      <td>0.998838</td>\n",
              "      <td>0.072000</td>\n",
              "      <td>0.997567</td>\n",
              "      <td>0.253395</td>\n",
              "      <td>0.872770</td>\n",
              "      <td>-0.925567</td>\n",
              "      <td>-0.974655</td>\n",
              "      <td>0.072176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>12616</td>\n",
              "      <td>2750</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.823319</td>\n",
              "      <td>0.897984</td>\n",
              "      <td>0.309963</td>\n",
              "      <td>0.900853</td>\n",
              "      <td>0.829868</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.992265</td>\n",
              "      <td>0.292308</td>\n",
              "      <td>0.997095</td>\n",
              "      <td>0.221818</td>\n",
              "      <td>0.993108</td>\n",
              "      <td>0.278735</td>\n",
              "      <td>0.748526</td>\n",
              "      <td>-0.771289</td>\n",
              "      <td>-0.792265</td>\n",
              "      <td>0.223358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>12866</td>\n",
              "      <td>3000</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.879599</td>\n",
              "      <td>0.896586</td>\n",
              "      <td>0.791539</td>\n",
              "      <td>0.902070</td>\n",
              "      <td>0.951290</td>\n",
              "      <td>0.794658</td>\n",
              "      <td>0.995212</td>\n",
              "      <td>0.680168</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.767333</td>\n",
              "      <td>0.996047</td>\n",
              "      <td>0.304075</td>\n",
              "      <td>0.260193</td>\n",
              "      <td>-0.228714</td>\n",
              "      <td>-0.200553</td>\n",
              "      <td>0.770379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>13116</td>\n",
              "      <td>3250</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.882630</td>\n",
              "      <td>0.895208</td>\n",
              "      <td>0.805643</td>\n",
              "      <td>0.904447</td>\n",
              "      <td>0.961180</td>\n",
              "      <td>0.833063</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.741688</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.811077</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.329414</td>\n",
              "      <td>0.212624</td>\n",
              "      <td>-0.188923</td>\n",
              "      <td>-0.166937</td>\n",
              "      <td>0.811077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>13366</td>\n",
              "      <td>3500</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.892451</td>\n",
              "      <td>0.894387</td>\n",
              "      <td>0.856287</td>\n",
              "      <td>0.904527</td>\n",
              "      <td>0.992131</td>\n",
              "      <td>0.968762</td>\n",
              "      <td>0.999754</td>\n",
              "      <td>0.926453</td>\n",
              "      <td>0.997676</td>\n",
              "      <td>0.958571</td>\n",
              "      <td>0.999392</td>\n",
              "      <td>0.354754</td>\n",
              "      <td>0.051108</td>\n",
              "      <td>-0.040820</td>\n",
              "      <td>-0.030993</td>\n",
              "      <td>0.959155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>13616</td>\n",
              "      <td>3750</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.891451</td>\n",
              "      <td>0.893922</td>\n",
              "      <td>0.855151</td>\n",
              "      <td>0.904466</td>\n",
              "      <td>0.991902</td>\n",
              "      <td>0.969782</td>\n",
              "      <td>0.999632</td>\n",
              "      <td>0.939159</td>\n",
              "      <td>0.997676</td>\n",
              "      <td>0.962400</td>\n",
              "      <td>0.999290</td>\n",
              "      <td>0.380093</td>\n",
              "      <td>0.044183</td>\n",
              "      <td>-0.036890</td>\n",
              "      <td>-0.029850</td>\n",
              "      <td>0.963083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>13866</td>\n",
              "      <td>4000</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.892474</td>\n",
              "      <td>0.893814</td>\n",
              "      <td>0.863526</td>\n",
              "      <td>0.903706</td>\n",
              "      <td>0.995801</td>\n",
              "      <td>0.988849</td>\n",
              "      <td>0.998404</td>\n",
              "      <td>0.966351</td>\n",
              "      <td>0.999419</td>\n",
              "      <td>0.983500</td>\n",
              "      <td>0.998581</td>\n",
              "      <td>0.405433</td>\n",
              "      <td>0.021311</td>\n",
              "      <td>-0.015081</td>\n",
              "      <td>-0.009555</td>\n",
              "      <td>0.984898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>14116</td>\n",
              "      <td>4250</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.891592</td>\n",
              "      <td>0.893328</td>\n",
              "      <td>0.863991</td>\n",
              "      <td>0.902993</td>\n",
              "      <td>0.994817</td>\n",
              "      <td>0.990429</td>\n",
              "      <td>0.996562</td>\n",
              "      <td>0.968348</td>\n",
              "      <td>0.997095</td>\n",
              "      <td>0.985176</td>\n",
              "      <td>0.996655</td>\n",
              "      <td>0.430772</td>\n",
              "      <td>0.017440</td>\n",
              "      <td>-0.011479</td>\n",
              "      <td>-0.006133</td>\n",
              "      <td>0.988483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>14366</td>\n",
              "      <td>4500</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.881773</td>\n",
              "      <td>0.893977</td>\n",
              "      <td>0.866112</td>\n",
              "      <td>0.888776</td>\n",
              "      <td>0.976869</td>\n",
              "      <td>0.998256</td>\n",
              "      <td>0.967833</td>\n",
              "      <td>0.997167</td>\n",
              "      <td>0.994189</td>\n",
              "      <td>0.998000</td>\n",
              "      <td>0.972431</td>\n",
              "      <td>0.456112</td>\n",
              "      <td>0.016701</td>\n",
              "      <td>0.025569</td>\n",
              "      <td>0.030423</td>\n",
              "      <td>1.026294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>14616</td>\n",
              "      <td>4750</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.878406</td>\n",
              "      <td>0.892246</td>\n",
              "      <td>0.859135</td>\n",
              "      <td>0.887448</td>\n",
              "      <td>0.974868</td>\n",
              "      <td>0.994712</td>\n",
              "      <td>0.966114</td>\n",
              "      <td>0.996543</td>\n",
              "      <td>0.999419</td>\n",
              "      <td>0.995158</td>\n",
              "      <td>0.971924</td>\n",
              "      <td>0.481451</td>\n",
              "      <td>0.015737</td>\n",
              "      <td>0.023234</td>\n",
              "      <td>0.028598</td>\n",
              "      <td>1.023905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>14866</td>\n",
              "      <td>5000</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.879423</td>\n",
              "      <td>0.893017</td>\n",
              "      <td>0.865586</td>\n",
              "      <td>0.886267</td>\n",
              "      <td>0.973660</td>\n",
              "      <td>0.994494</td>\n",
              "      <td>0.963904</td>\n",
              "      <td>0.975548</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.970201</td>\n",
              "      <td>0.506791</td>\n",
              "      <td>0.027521</td>\n",
              "      <td>0.019799</td>\n",
              "      <td>0.030590</td>\n",
              "      <td>1.020407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>15866</td>\n",
              "      <td>6000</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.889076</td>\n",
              "      <td>0.889292</td>\n",
              "      <td>0.862831</td>\n",
              "      <td>0.904447</td>\n",
              "      <td>0.999842</td>\n",
              "      <td>0.999561</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998618</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.608149</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>-0.000667</td>\n",
              "      <td>-0.000439</td>\n",
              "      <td>0.999333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>16866</td>\n",
              "      <td>7000</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.887599</td>\n",
              "      <td>0.887906</td>\n",
              "      <td>0.862971</td>\n",
              "      <td>0.904426</td>\n",
              "      <td>0.999480</td>\n",
              "      <td>0.999059</td>\n",
              "      <td>0.999754</td>\n",
              "      <td>0.997034</td>\n",
              "      <td>0.998838</td>\n",
              "      <td>0.998571</td>\n",
              "      <td>0.999595</td>\n",
              "      <td>0.709507</td>\n",
              "      <td>0.001249</td>\n",
              "      <td>-0.001023</td>\n",
              "      <td>-0.000695</td>\n",
              "      <td>0.998976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>17866</td>\n",
              "      <td>8000</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.885412</td>\n",
              "      <td>0.886074</td>\n",
              "      <td>0.861262</td>\n",
              "      <td>0.904204</td>\n",
              "      <td>0.998310</td>\n",
              "      <td>0.996696</td>\n",
              "      <td>0.999509</td>\n",
              "      <td>0.988695</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.994750</td>\n",
              "      <td>0.999595</td>\n",
              "      <td>0.810866</td>\n",
              "      <td>0.007059</td>\n",
              "      <td>-0.004845</td>\n",
              "      <td>-0.002813</td>\n",
              "      <td>0.995153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>18866</td>\n",
              "      <td>9000</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.885243</td>\n",
              "      <td>0.885709</td>\n",
              "      <td>0.863447</td>\n",
              "      <td>0.904386</td>\n",
              "      <td>0.999066</td>\n",
              "      <td>0.998100</td>\n",
              "      <td>0.999877</td>\n",
              "      <td>0.994903</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997333</td>\n",
              "      <td>0.999899</td>\n",
              "      <td>0.912224</td>\n",
              "      <td>0.003437</td>\n",
              "      <td>-0.002565</td>\n",
              "      <td>-0.001777</td>\n",
              "      <td>0.997434</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    rows_complete  rows_minority  rows_majority  f1_complete  \\\n",
              "0            9871              5           9866     0.904307   \n",
              "1            9876             10           9866     0.904046   \n",
              "2            9886             20           9866     0.904295   \n",
              "3            9896             30           9866     0.904330   \n",
              "4            9906             40           9866     0.903961   \n",
              "5            9916             50           9866     0.904428   \n",
              "6            9941             75           9866     0.903667   \n",
              "7            9966            100           9866     0.903559   \n",
              "8            9991            125           9866     0.903764   \n",
              "9           10016            150           9866     0.903738   \n",
              "10          10041            175           9866     0.903680   \n",
              "11          10066            200           9866     0.903444   \n",
              "12          10116            250           9866     0.902977   \n",
              "13          10166            300           9866     0.902344   \n",
              "14          10216            350           9866     0.903039   \n",
              "15          10266            400           9866     0.901766   \n",
              "16          10316            450           9866     0.901129   \n",
              "17          10366            500           9866     0.902129   \n",
              "18          10466            600           9866     0.901244   \n",
              "19          10566            700           9866     0.901290   \n",
              "20          10666            800           9866     0.901180   \n",
              "21          10766            900           9866     0.900434   \n",
              "22          10866           1000           9866     0.901340   \n",
              "23          11116           1250           9866     0.887138   \n",
              "24          11366           1500           9866     0.897993   \n",
              "25          11616           1750           9866     0.897736   \n",
              "26          11866           2000           9866     0.850038   \n",
              "27          12116           2250           9866     0.825178   \n",
              "28          12366           2500           9866     0.813914   \n",
              "29          12616           2750           9866     0.823319   \n",
              "30          12866           3000           9866     0.879599   \n",
              "31          13116           3250           9866     0.882630   \n",
              "32          13366           3500           9866     0.892451   \n",
              "33          13616           3750           9866     0.891451   \n",
              "34          13866           4000           9866     0.892474   \n",
              "35          14116           4250           9866     0.891592   \n",
              "36          14366           4500           9866     0.881773   \n",
              "37          14616           4750           9866     0.878406   \n",
              "38          14866           5000           9866     0.879423   \n",
              "39          15866           6000           9866     0.889076   \n",
              "40          16866           7000           9866     0.887599   \n",
              "41          17866           8000           9866     0.885412   \n",
              "42          18866           9000           9866     0.885243   \n",
              "\n",
              "    f1_complete_train  f1_minority  f1_majority  tpr_complete  tpr_minority  \\\n",
              "0            0.904697     0.750000     0.904376      0.999755      1.000000   \n",
              "1            0.904673     0.750000     0.904183      0.999264      1.000000   \n",
              "2            0.904933     0.918919     0.904265      0.999632      1.000000   \n",
              "3            0.904763     0.867925     0.904437      0.999878      1.000000   \n",
              "4            0.904463     0.823529     0.904265      0.999633      1.000000   \n",
              "5            0.904967     0.936170     0.904262      0.999023      1.000000   \n",
              "6            0.904404     0.846154     0.904082      0.999268      1.000000   \n",
              "7            0.904239     0.843931     0.904132      0.999270      1.000000   \n",
              "8            0.904221     0.853211     0.904376      0.999757      1.000000   \n",
              "9            0.903968     0.854962     0.904447      1.000000      1.000000   \n",
              "10           0.904116     0.867314     0.904304      0.999517      1.000000   \n",
              "11           0.903869     0.853868     0.904405      0.999518      1.000000   \n",
              "12           0.903670     0.855835     0.904122      0.999160      1.000000   \n",
              "13           0.902904     0.830409     0.904394      0.999402      1.000000   \n",
              "14           0.903315     0.861789     0.904447      1.000000      1.000000   \n",
              "15           0.902976     0.852224     0.903685      0.998223      1.000000   \n",
              "16           0.903317     0.865069     0.902720      0.996466      1.000000   \n",
              "17           0.903262     0.871332     0.903645      0.998359      1.000000   \n",
              "18           0.901898     0.853868     0.904000      0.998953      1.000000   \n",
              "19           0.901800     0.860862     0.904050      0.998962      1.000000   \n",
              "20           0.901806     0.860399     0.904362      0.999086      1.000000   \n",
              "21           0.900778     0.854233     0.904466      0.999660      1.000000   \n",
              "22           0.901425     0.868778     0.904537      0.999888      1.000000   \n",
              "23           0.899988     0.849100     0.891779      0.973994      0.989247   \n",
              "24           0.899794     0.863636     0.903036      0.997093      0.997375   \n",
              "25           0.898426     0.860957     0.904011      0.998944      0.998489   \n",
              "26           0.898053     0.346570     0.904254      0.877991      0.222517   \n",
              "27           0.897468     0.051143     0.904193      0.832181      0.027761   \n",
              "28           0.898355     0.041366     0.903208      0.813023      0.022643   \n",
              "29           0.897984     0.309963     0.900853      0.829868      0.200000   \n",
              "30           0.896586     0.791539     0.902070      0.951290      0.794658   \n",
              "31           0.895208     0.805643     0.904447      0.961180      0.833063   \n",
              "32           0.894387     0.856287     0.904527      0.992131      0.968762   \n",
              "33           0.893922     0.855151     0.904466      0.991902      0.969782   \n",
              "34           0.893814     0.863526     0.903706      0.995801      0.988849   \n",
              "35           0.893328     0.863991     0.902993      0.994817      0.990429   \n",
              "36           0.893977     0.866112     0.888776      0.976869      0.998256   \n",
              "37           0.892246     0.859135     0.887448      0.974868      0.994712   \n",
              "38           0.893017     0.865586     0.886267      0.973660      0.994494   \n",
              "39           0.889292     0.862831     0.904447      0.999842      0.999561   \n",
              "40           0.887906     0.862971     0.904426      0.999480      0.999059   \n",
              "41           0.886074     0.861262     0.904204      0.998310      0.996696   \n",
              "42           0.885709     0.863447     0.904386      0.999066      0.998100   \n",
              "\n",
              "    tpr_majority  fpr_minority  fpr_majority  prob_yhat_1_minority  \\\n",
              "0       0.999754      1.000000      0.999419              1.000000   \n",
              "1       0.999263      1.000000      0.998838              1.000000   \n",
              "2       0.999632      1.000000      1.000000              1.000000   \n",
              "3       0.999877      1.000000      0.999419              1.000000   \n",
              "4       0.999632      1.000000      1.000000              1.000000   \n",
              "5       0.999018      1.000000      0.996514              1.000000   \n",
              "6       0.999263      1.000000      1.000000              1.000000   \n",
              "7       0.999263      1.000000      0.999419              1.000000   \n",
              "8       0.999754      1.000000      0.999419              1.000000   \n",
              "9       1.000000      1.000000      1.000000              1.000000   \n",
              "10      0.999509      1.000000      0.998838              1.000000   \n",
              "11      0.999509      1.000000      0.997676              1.000000   \n",
              "12      0.999141      1.000000      0.998838              1.000000   \n",
              "13      0.999386      1.000000      0.997095              1.000000   \n",
              "14      1.000000      1.000000      1.000000              1.000000   \n",
              "15      0.998158      1.000000      0.998257              1.000000   \n",
              "16      0.996317      1.000000      0.998838              1.000000   \n",
              "17      0.998281      1.000000      0.999419              1.000000   \n",
              "18      0.998895      1.000000      0.998838              1.000000   \n",
              "19      0.998895      1.000000      0.998257              1.000000   \n",
              "20      0.999018      1.000000      0.995352              1.000000   \n",
              "21      0.999632      1.000000      0.997676              1.000000   \n",
              "22      0.999877      1.000000      0.998257              1.000000   \n",
              "23      0.972253      0.990625      0.985474              0.989600   \n",
              "24      0.997053      1.000000      0.999419              0.998000   \n",
              "25      0.999018      0.997653      0.999419              0.998286   \n",
              "26      0.999509      0.189796      0.999419              0.214500   \n",
              "27      0.999386      0.175943      0.999419              0.064444   \n",
              "28      0.997299      0.227953      0.998838              0.072000   \n",
              "29      0.992265      0.292308      0.997095              0.221818   \n",
              "30      0.995212      0.680168      1.000000              0.767333   \n",
              "31      1.000000      0.741688      1.000000              0.811077   \n",
              "32      0.999754      0.926453      0.997676              0.958571   \n",
              "33      0.999632      0.939159      0.997676              0.962400   \n",
              "34      0.998404      0.966351      0.999419              0.983500   \n",
              "35      0.996562      0.968348      0.997095              0.985176   \n",
              "36      0.967833      0.997167      0.994189              0.998000   \n",
              "37      0.966114      0.996543      0.999419              0.995158   \n",
              "38      0.963904      0.975548      1.000000              0.990000   \n",
              "39      1.000000      0.998618      1.000000              0.999333   \n",
              "40      0.999754      0.997034      0.998838              0.998571   \n",
              "41      0.999509      0.988695      1.000000              0.994750   \n",
              "42      0.999877      0.994903      1.000000              0.997333   \n",
              "\n",
              "    prob_yhat_1_majority  rel_share_min_of_maj  aver_abs_odds_diff  \\\n",
              "0               0.999696              0.000507            0.000413   \n",
              "1               0.999189              0.001014            0.000949   \n",
              "2               0.999696              0.002027            0.000184   \n",
              "3               0.999797              0.003041            0.000352   \n",
              "4               0.999696              0.004054            0.000184   \n",
              "5               0.998581              0.005068            0.002234   \n",
              "6               0.999392              0.007602            0.000368   \n",
              "7               0.999290              0.010136            0.000659   \n",
              "8               0.999696              0.012670            0.000413   \n",
              "9               1.000000              0.015204            0.000000   \n",
              "10              0.999392              0.017738            0.000827   \n",
              "11              0.999189              0.020272            0.001408   \n",
              "12              0.999088              0.025340            0.001011   \n",
              "13              0.998986              0.030407            0.001760   \n",
              "14              1.000000              0.035475            0.000000   \n",
              "15              0.998176              0.040543            0.001792   \n",
              "16              0.996757              0.045611            0.002423   \n",
              "17              0.998480              0.050679            0.001150   \n",
              "18              0.998885              0.060815            0.001134   \n",
              "19              0.998784              0.070951            0.001424   \n",
              "20              0.998378              0.081087            0.002815   \n",
              "21              0.999290              0.091222            0.001346   \n",
              "22              0.999595              0.101358            0.000933   \n",
              "23              0.974559              0.126698            0.011073   \n",
              "24              0.997466              0.152037            0.000451   \n",
              "25              0.999088              0.177377            0.001147   \n",
              "26              0.999493              0.202716            0.793308   \n",
              "27              0.999392              0.228056            0.897551   \n",
              "28              0.997567              0.253395            0.872770   \n",
              "29              0.993108              0.278735            0.748526   \n",
              "30              0.996047              0.304075            0.260193   \n",
              "31              1.000000              0.329414            0.212624   \n",
              "32              0.999392              0.354754            0.051108   \n",
              "33              0.999290              0.380093            0.044183   \n",
              "34              0.998581              0.405433            0.021311   \n",
              "35              0.996655              0.430772            0.017440   \n",
              "36              0.972431              0.456112            0.016701   \n",
              "37              0.971924              0.481451            0.015737   \n",
              "38              0.970201              0.506791            0.027521   \n",
              "39              1.000000              0.608149            0.000911   \n",
              "40              0.999595              0.709507            0.001249   \n",
              "41              0.999595              0.810866            0.007059   \n",
              "42              0.999899              0.912224            0.003437   \n",
              "\n",
              "    stat_parity_diff  equal_opport_dist  disparate_impact  \n",
              "0           0.000304           0.000246          1.000304  \n",
              "1           0.000811           0.000737          1.000812  \n",
              "2           0.000304           0.000368          1.000304  \n",
              "3           0.000203           0.000123          1.000203  \n",
              "4           0.000304           0.000368          1.000304  \n",
              "5           0.001419           0.000982          1.001421  \n",
              "6           0.000608           0.000737          1.000609  \n",
              "7           0.000710           0.000737          1.000710  \n",
              "8           0.000304           0.000246          1.000304  \n",
              "9           0.000000           0.000000          1.000000  \n",
              "10          0.000608           0.000491          1.000609  \n",
              "11          0.000811           0.000491          1.000812  \n",
              "12          0.000912           0.000859          1.000913  \n",
              "13          0.001014           0.000614          1.001015  \n",
              "14          0.000000           0.000000          1.000000  \n",
              "15          0.001824           0.001842          1.001828  \n",
              "16          0.003243           0.003683          1.003254  \n",
              "17          0.001520           0.001719          1.001523  \n",
              "18          0.001115           0.001105          1.001116  \n",
              "19          0.001216           0.001105          1.001218  \n",
              "20          0.001622           0.000982          1.001624  \n",
              "21          0.000710           0.000368          1.000710  \n",
              "22          0.000405           0.000123          1.000406  \n",
              "23          0.015041           0.016994          1.015434  \n",
              "24          0.000534           0.000322          1.000535  \n",
              "25         -0.000802          -0.000528          0.999197  \n",
              "26         -0.784993          -0.776992          0.214609  \n",
              "27         -0.934947          -0.971625          0.064484  \n",
              "28         -0.925567          -0.974655          0.072176  \n",
              "29         -0.771289          -0.792265          0.223358  \n",
              "30         -0.228714          -0.200553          0.770379  \n",
              "31         -0.188923          -0.166937          0.811077  \n",
              "32         -0.040820          -0.030993          0.959155  \n",
              "33         -0.036890          -0.029850          0.963083  \n",
              "34         -0.015081          -0.009555          0.984898  \n",
              "35         -0.011479          -0.006133          0.988483  \n",
              "36          0.025569           0.030423          1.026294  \n",
              "37          0.023234           0.028598          1.023905  \n",
              "38          0.019799           0.030590          1.020407  \n",
              "39         -0.000667          -0.000439          0.999333  \n",
              "40         -0.001023          -0.000695          0.998976  \n",
              "41         -0.004845          -0.002813          0.995153  \n",
              "42         -0.002565          -0.001777          0.997434  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9e1lYjrvSBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save metrics csv\n",
        "results_df_homicide.to_csv(\"df_homicide_metrics.csv\") \n",
        "from google.colab import files\n",
        "files.download(\"df_homicide_metrics.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFknrG64eQwJ",
        "colab_type": "text"
      },
      "source": [
        "#### 4) Create visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5k8nM-fzHxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Feature Importance\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "from matplotlib import pyplot\n",
        "\n",
        "homicide_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "                               colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "                               learning_rate=0.3, max_delta_step=0, max_depth=3,\n",
        "                               min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
        "                               nthread=4, objective='binary:logistic', random_state=0,\n",
        "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "                               silent=None, subsample=1, verbosity=1)\n",
        "\n",
        "homicide_model.fit(df_homicide_train_input, df_homicide_train_label)\n",
        "plot_importance(homicide_model)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2nN17MLRMGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# B: Learning Curve Function from scikit learn\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "# Define arguments\n",
        "f1 = make_scorer(f1_score) \n",
        "\n",
        "df_homicide_train_input = pd.get_dummies(df_homicide_train_input)\n",
        "\n",
        "# Plot actual learning curve\n",
        "plot_learning_curve(estimator = model, \n",
        "                    title = \"Adult Dataset - Random Forest Learning Curve\", \n",
        "                    X = df_homicide_train_input, y = df_homicide_train_label, \n",
        "                    cv = 5, \n",
        "                    scoring = f1, \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = np.linspace(.1, 1.0, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmkVU1LaVe4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_metrics_line_chart(results_df_homicide)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jG0KRhjWmM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maj_min_metrics_line_chart(results_df_homicide) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm5PQKjKwpdT",
        "colab_type": "text"
      },
      "source": [
        "## 4) German Credit Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzwsnRs9XPxz",
        "colab_type": "text"
      },
      "source": [
        "The German Credit dataset contains 1000 credit records containing attributes such as personal status and sex, credit score, credit amount, housing status etc. It can be used in studies about gender inequalities on credit-related issues [42].\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv9tpy66--lw",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQsfLldE3OLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATA IMPORT - Long dataset version\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# path_credit_uci = \"/content/drive/My Drive/Master Thesis/Data/german_credit_dataset.csv\"\n",
        "\n",
        "# Set the path to the CSV containing the dataset to train on.\n",
        "path_credit_uci = 'https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data'\n",
        "column_names_credit = [\"chk_acct\", \"duration\", \"credit_his\", \"purpose\", \"amount\", \"saving_acct\", \n",
        "                       \"present_emp\", \"installment_rate\", \"sex\", \"other_debtor\", \"present_resid\", \n",
        "                       \"property\", \"age\", \"other_install\", \"housing\", \"n_credits\", \"job\", \"n_people\", \n",
        "                       \"telephone\", \"foreign\", \"response\"]\n",
        "\n",
        "# Read the dataset from the provided CSV and print out information about it.\n",
        "df_credit_long = pd.read_csv(path_credit_uci, delim_whitespace=True, names = column_names_credit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pGCylXv_vSz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATA IMPORT - Short dataset version\n",
        "path_credit = \"/content/drive/My Drive/Master Thesis/Data/german_credit_dataset.csv\"\n",
        "df_credit_short = pd.read_csv(path_credit, header = 0, sep = \";\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmMJQzTeRjCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Join Risk and Sex column of the short dataset to rest of columns of the long dataset\n",
        "df_credit_long['id'] = range(0, len(df_credit_long))\n",
        "\n",
        "# Join based on \"id\" column - add risk and Sex column from short dataset to long dataset\n",
        "df_credit_short_reduced_cols = df_credit_short.loc[:, [\"id\", \"Sex\", \"Risk\"]]\n",
        "df_credit = pd.merge(df_credit_long, df_credit_short_reduced_cols, how = \"left\", on = \"id\")\n",
        "df_credit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf9tFALHm5sc",
        "colab_type": "text"
      },
      "source": [
        "### Initial Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv9rHRxsM-ns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop unnecessary columns\n",
        "df_credit = df_credit.drop(['id', \"response\"], axis=1)\n",
        "\n",
        "df_credit[\"Risk\"] = df_credit[\"Risk\"].replace({'good': 1, 'bad': 0})\n",
        "\n",
        "df_credit.rename(columns={'sex':'sex_specific'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXqrA4Pbc_as",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiICbX-HE02N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_credit.info)\n",
        "print(df_credit.describe())\n",
        "print(df_credit.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x9J2edEZ1Ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_credit)\n",
        "print(df_credit.groupby([\"Risk\"]).agg({\"Risk\": 'count'}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enom-Sm6gUsV",
        "colab_type": "text"
      },
      "source": [
        "Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njYG0xUTEouo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eda_descr_stats(df_credit, disc_feature=\"Sex\", disc_min_value=\"female\", label =\"Risk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDEIZJoydbve",
        "colab_type": "text"
      },
      "source": [
        "### Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhxL3Q3hdjgM",
        "colab_type": "text"
      },
      "source": [
        "#### 0) Preprocess data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvAybUs5FHo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute preprocessing function\n",
        "df_credit_train_input, df_credit_train_label = fair_preprocess(data = df_credit, \n",
        "                                                               label = \"Risk\",\n",
        "                                                               neg_class = 0,\n",
        "                                                               pos_class = 1)\n",
        "\n",
        "# Check whether binary encoding was successful and seperate datasets were created\n",
        "print(df_credit.groupby([\"Risk\"]).agg({\"Risk\": 'count'}))\n",
        "print(df_credit_train_input.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buM8oHaXdrdm",
        "colab_type": "text"
      },
      "source": [
        "#### 1) Define Hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7mw8mSvF2lB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "edae66a0-5c9a-4f63-cfa9-b058d527838a"
      },
      "source": [
        "# Create the hyperparameter grid\n",
        "param_grid = {\"learning_rate\": [0.2],\n",
        "              \"n_estimators\": [10, 20, 30, 40, 50],\n",
        "              \"max_depth\": [6, 9, 12], \n",
        "              \"min_child_weight\": [0.5, 1, 3, 5],   \n",
        "              \"reg_lambda\": [1, 1.2]}\n",
        "xgb_grid_search = XGBClassifier(objective= 'binary:logistic', nthread=4)\n",
        "cv=5\n",
        "# Learning Curve for Slice \n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "# Define model\n",
        "f1 = make_scorer(f1_score)\n",
        "\n",
        "# Dummy Coding\n",
        "df_train_input_dummy = pd.get_dummies(df_credit_train_input)\n",
        "\n",
        "grid_rf_class = sklearn.model_selection.GridSearchCV(estimator = xgb_grid_search,\n",
        "                                                     param_grid = param_grid, \n",
        "                                                     scoring= f1,\n",
        "                                                     cv = cv,\n",
        "                                                     refit = True,\n",
        "                                                     return_train_score = False)\n",
        "  \n",
        "# Fit model\n",
        "grid_rf_class.fit(df_train_input_dummy, df_credit_train_label)\n",
        "\n",
        "print(grid_rf_class.best_params_)\n",
        "print(grid_rf_class.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'learning_rate': 0.2, 'max_depth': 9, 'min_child_weight': 1, 'n_estimators': 30, 'reg_lambda': 1}\n",
            "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
            "              learning_rate=0.2, max_delta_step=0, max_depth=9,\n",
            "              min_child_weight=1, missing=None, n_estimators=30, n_jobs=1,\n",
            "              nthread=4, objective='binary:logistic', random_state=0,\n",
            "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
            "              silent=None, subsample=1, verbosity=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ0lfdI0dzit",
        "colab_type": "text"
      },
      "source": [
        "#### 2) Create datasets with diff. minority group sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOicTRmoQ8qK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "9b84a1ef-a5f2-4038-dd8a-4459628cdb48"
      },
      "source": [
        "training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Creating dfs for minority and majority group\n",
        "is_unpriv = df_credit[\"Sex\"].isin([\"female\"])\n",
        "is_priv = df_credit[\"Sex\"].isin([\"male\"])\n",
        "df_credit_unpriv = df_credit[is_unpriv]  # Minority\n",
        "df_credit_priv = df_credit[is_priv]  # Majority\n",
        "\n",
        "list_dfs_credit = create_datasets(min_data = df_credit_unpriv, maj_data = df_credit_priv, training_sizes=training_sizes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n",
            "[695, 700, 710, 720, 730, 740, 765, 790, 815, 840, 865, 890, 940, 990]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArInhVkxd9N0",
        "colab_type": "text"
      },
      "source": [
        "#### 3) Create dataframes with diff. metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4-M2nsFd9FA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "3dd3b925-3559-4de8-b84c-c8262fb7d3f1"
      },
      "source": [
        "list_dfs = list_dfs_credit\n",
        "label = \"Risk\"\n",
        "credit_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "                             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "                             learning_rate=0.2, max_delta_step=0, max_depth=9,\n",
        "                             min_child_weight=1, missing=None, n_estimators=30, n_jobs=1,\n",
        "                             nthread=4, objective='binary:logistic', random_state=0,\n",
        "                             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "                             silent=None, subsample=1, verbosity=1)\n",
        "cv = 5\n",
        "discr_feature = \"Sex\"\n",
        "min_value = \"female\"\n",
        "maj_value = \"male\"\n",
        "\n",
        "results_df_credit = metrics_to_df(list_dfs=list_dfs_credit, label = label, model = credit_model, \n",
        "                                  cv = cv, discr_feature = discr_feature, min_value = min_value,\n",
        "                                  maj_value = maj_value)\n",
        "\n",
        "results_df_credit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rows_complete</th>\n",
              "      <th>rows_minority</th>\n",
              "      <th>rows_majority</th>\n",
              "      <th>f1_complete</th>\n",
              "      <th>f1_complete_train</th>\n",
              "      <th>f1_minority</th>\n",
              "      <th>f1_majority</th>\n",
              "      <th>tpr_complete</th>\n",
              "      <th>tpr_minority</th>\n",
              "      <th>tpr_majority</th>\n",
              "      <th>fpr_minority</th>\n",
              "      <th>fpr_majority</th>\n",
              "      <th>prob_yhat_1_minority</th>\n",
              "      <th>prob_yhat_1_majority</th>\n",
              "      <th>rel_share_min_of_maj</th>\n",
              "      <th>aver_abs_odds_diff</th>\n",
              "      <th>stat_parity_diff</th>\n",
              "      <th>equal_opport_dist</th>\n",
              "      <th>disparate_impact</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>695</td>\n",
              "      <td>5</td>\n",
              "      <td>690</td>\n",
              "      <td>0.832861</td>\n",
              "      <td>0.999255</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.832381</td>\n",
              "      <td>0.876740</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.875752</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.596859</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.798551</td>\n",
              "      <td>0.007246</td>\n",
              "      <td>0.263695</td>\n",
              "      <td>0.201449</td>\n",
              "      <td>0.124248</td>\n",
              "      <td>1.252269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>700</td>\n",
              "      <td>10</td>\n",
              "      <td>690</td>\n",
              "      <td>0.848256</td>\n",
              "      <td>0.999013</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.848138</td>\n",
              "      <td>0.889328</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.889780</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.544503</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.794203</td>\n",
              "      <td>0.014493</td>\n",
              "      <td>0.121903</td>\n",
              "      <td>-0.094203</td>\n",
              "      <td>-0.032637</td>\n",
              "      <td>0.881387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>710</td>\n",
              "      <td>20</td>\n",
              "      <td>690</td>\n",
              "      <td>0.842686</td>\n",
              "      <td>0.999270</td>\n",
              "      <td>0.827586</td>\n",
              "      <td>0.843100</td>\n",
              "      <td>0.892788</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.893788</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.591623</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.810145</td>\n",
              "      <td>0.028986</td>\n",
              "      <td>0.064134</td>\n",
              "      <td>-0.060145</td>\n",
              "      <td>-0.036645</td>\n",
              "      <td>0.925760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>720</td>\n",
              "      <td>30</td>\n",
              "      <td>690</td>\n",
              "      <td>0.831025</td>\n",
              "      <td>0.999270</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>0.836364</td>\n",
              "      <td>0.877193</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.875752</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>0.570681</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.791304</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>0.084820</td>\n",
              "      <td>0.008696</td>\n",
              "      <td>0.052820</td>\n",
              "      <td>1.010989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>730</td>\n",
              "      <td>40</td>\n",
              "      <td>690</td>\n",
              "      <td>0.843155</td>\n",
              "      <td>0.998812</td>\n",
              "      <td>0.754717</td>\n",
              "      <td>0.847619</td>\n",
              "      <td>0.885714</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.891784</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.554974</td>\n",
              "      <td>0.675000</td>\n",
              "      <td>0.798551</td>\n",
              "      <td>0.057971</td>\n",
              "      <td>0.088763</td>\n",
              "      <td>-0.123551</td>\n",
              "      <td>-0.122553</td>\n",
              "      <td>0.845281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>740</td>\n",
              "      <td>50</td>\n",
              "      <td>690</td>\n",
              "      <td>0.848057</td>\n",
              "      <td>0.999534</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.848197</td>\n",
              "      <td>0.895522</td>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.895792</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.565445</td>\n",
              "      <td>0.820000</td>\n",
              "      <td>0.804348</td>\n",
              "      <td>0.072464</td>\n",
              "      <td>0.026920</td>\n",
              "      <td>0.015652</td>\n",
              "      <td>-0.003900</td>\n",
              "      <td>1.019459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>765</td>\n",
              "      <td>75</td>\n",
              "      <td>690</td>\n",
              "      <td>0.842105</td>\n",
              "      <td>0.999091</td>\n",
              "      <td>0.810811</td>\n",
              "      <td>0.845420</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.887776</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.554974</td>\n",
              "      <td>0.813333</td>\n",
              "      <td>0.795652</td>\n",
              "      <td>0.108696</td>\n",
              "      <td>0.048625</td>\n",
              "      <td>0.017681</td>\n",
              "      <td>0.012224</td>\n",
              "      <td>1.022222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>790</td>\n",
              "      <td>100</td>\n",
              "      <td>690</td>\n",
              "      <td>0.843252</td>\n",
              "      <td>0.999128</td>\n",
              "      <td>0.813793</td>\n",
              "      <td>0.847328</td>\n",
              "      <td>0.879371</td>\n",
              "      <td>0.808219</td>\n",
              "      <td>0.889780</td>\n",
              "      <td>0.481481</td>\n",
              "      <td>0.549738</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.795652</td>\n",
              "      <td>0.144928</td>\n",
              "      <td>0.074909</td>\n",
              "      <td>-0.075652</td>\n",
              "      <td>-0.081560</td>\n",
              "      <td>0.904918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>815</td>\n",
              "      <td>125</td>\n",
              "      <td>690</td>\n",
              "      <td>0.828383</td>\n",
              "      <td>0.999566</td>\n",
              "      <td>0.759036</td>\n",
              "      <td>0.839388</td>\n",
              "      <td>0.871528</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.879760</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.565445</td>\n",
              "      <td>0.712000</td>\n",
              "      <td>0.792754</td>\n",
              "      <td>0.181159</td>\n",
              "      <td>0.042678</td>\n",
              "      <td>-0.080754</td>\n",
              "      <td>-0.061578</td>\n",
              "      <td>0.898135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>840</td>\n",
              "      <td>150</td>\n",
              "      <td>690</td>\n",
              "      <td>0.831210</td>\n",
              "      <td>0.998960</td>\n",
              "      <td>0.757282</td>\n",
              "      <td>0.845714</td>\n",
              "      <td>0.870000</td>\n",
              "      <td>0.772277</td>\n",
              "      <td>0.889780</td>\n",
              "      <td>0.551020</td>\n",
              "      <td>0.560209</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.798551</td>\n",
              "      <td>0.217391</td>\n",
              "      <td>0.063346</td>\n",
              "      <td>-0.098551</td>\n",
              "      <td>-0.117502</td>\n",
              "      <td>0.876588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>865</td>\n",
              "      <td>175</td>\n",
              "      <td>690</td>\n",
              "      <td>0.805259</td>\n",
              "      <td>0.999387</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.830739</td>\n",
              "      <td>0.801964</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.855711</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.534031</td>\n",
              "      <td>0.440000</td>\n",
              "      <td>0.766667</td>\n",
              "      <td>0.253623</td>\n",
              "      <td>0.302510</td>\n",
              "      <td>-0.326667</td>\n",
              "      <td>-0.293211</td>\n",
              "      <td>0.573913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>890</td>\n",
              "      <td>200</td>\n",
              "      <td>690</td>\n",
              "      <td>0.818824</td>\n",
              "      <td>0.999206</td>\n",
              "      <td>0.715517</td>\n",
              "      <td>0.841802</td>\n",
              "      <td>0.829889</td>\n",
              "      <td>0.638462</td>\n",
              "      <td>0.879760</td>\n",
              "      <td>0.271429</td>\n",
              "      <td>0.549738</td>\n",
              "      <td>0.510000</td>\n",
              "      <td>0.788406</td>\n",
              "      <td>0.289855</td>\n",
              "      <td>0.259804</td>\n",
              "      <td>-0.278406</td>\n",
              "      <td>-0.241298</td>\n",
              "      <td>0.646875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>940</td>\n",
              "      <td>250</td>\n",
              "      <td>690</td>\n",
              "      <td>0.823964</td>\n",
              "      <td>0.999049</td>\n",
              "      <td>0.719472</td>\n",
              "      <td>0.854147</td>\n",
              "      <td>0.847793</td>\n",
              "      <td>0.689873</td>\n",
              "      <td>0.897796</td>\n",
              "      <td>0.391304</td>\n",
              "      <td>0.534031</td>\n",
              "      <td>0.580000</td>\n",
              "      <td>0.797101</td>\n",
              "      <td>0.362319</td>\n",
              "      <td>0.175325</td>\n",
              "      <td>-0.217101</td>\n",
              "      <td>-0.207922</td>\n",
              "      <td>0.727636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>990</td>\n",
              "      <td>300</td>\n",
              "      <td>690</td>\n",
              "      <td>0.829368</td>\n",
              "      <td>0.999280</td>\n",
              "      <td>0.796117</td>\n",
              "      <td>0.842304</td>\n",
              "      <td>0.878963</td>\n",
              "      <td>0.841026</td>\n",
              "      <td>0.893788</td>\n",
              "      <td>0.504762</td>\n",
              "      <td>0.596859</td>\n",
              "      <td>0.723333</td>\n",
              "      <td>0.811594</td>\n",
              "      <td>0.434783</td>\n",
              "      <td>0.072429</td>\n",
              "      <td>-0.088261</td>\n",
              "      <td>-0.052762</td>\n",
              "      <td>0.891250</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    rows_complete  rows_minority  rows_majority  f1_complete  \\\n",
              "0             695              5            690     0.832861   \n",
              "1             700             10            690     0.848256   \n",
              "2             710             20            690     0.842686   \n",
              "3             720             30            690     0.831025   \n",
              "4             730             40            690     0.843155   \n",
              "5             740             50            690     0.848057   \n",
              "6             765             75            690     0.842105   \n",
              "7             790            100            690     0.843252   \n",
              "8             815            125            690     0.828383   \n",
              "9             840            150            690     0.831210   \n",
              "10            865            175            690     0.805259   \n",
              "11            890            200            690     0.818824   \n",
              "12            940            250            690     0.823964   \n",
              "13            990            300            690     0.829368   \n",
              "\n",
              "    f1_complete_train  f1_minority  f1_majority  tpr_complete  tpr_minority  \\\n",
              "0            0.999255     0.888889     0.832381      0.876740      1.000000   \n",
              "1            0.999013     0.857143     0.848138      0.889328      0.857143   \n",
              "2            0.999270     0.827586     0.843100      0.892788      0.857143   \n",
              "3            0.999270     0.684211     0.836364      0.877193      0.928571   \n",
              "4            0.998812     0.754717     0.847619      0.885714      0.769231   \n",
              "5            0.999534     0.846154     0.848197      0.895522      0.891892   \n",
              "6            0.999091     0.810811     0.845420      0.888889      0.900000   \n",
              "7            0.999128     0.813793     0.847328      0.879371      0.808219   \n",
              "8            0.999566     0.759036     0.839388      0.871528      0.818182   \n",
              "9            0.998960     0.757282     0.845714      0.870000      0.772277   \n",
              "10           0.999387     0.666667     0.830739      0.801964      0.562500   \n",
              "11           0.999206     0.715517     0.841802      0.829889      0.638462   \n",
              "12           0.999049     0.719472     0.854147      0.847793      0.689873   \n",
              "13           0.999280     0.796117     0.842304      0.878963      0.841026   \n",
              "\n",
              "    tpr_majority  fpr_minority  fpr_majority  prob_yhat_1_minority  \\\n",
              "0       0.875752      1.000000      0.596859              1.000000   \n",
              "1       0.889780      0.333333      0.544503              0.700000   \n",
              "2       0.893788      0.500000      0.591623              0.750000   \n",
              "3       0.875752      0.687500      0.570681              0.800000   \n",
              "4       0.891784      0.500000      0.554974              0.675000   \n",
              "5       0.895792      0.615385      0.565445              0.820000   \n",
              "6       0.887776      0.640000      0.554974              0.813333   \n",
              "7       0.889780      0.481481      0.549738              0.720000   \n",
              "8       0.879760      0.541667      0.565445              0.712000   \n",
              "9       0.889780      0.551020      0.560209              0.700000   \n",
              "10      0.855711      0.222222      0.534031              0.440000   \n",
              "11      0.879760      0.271429      0.549738              0.510000   \n",
              "12      0.897796      0.391304      0.534031              0.580000   \n",
              "13      0.893788      0.504762      0.596859              0.723333   \n",
              "\n",
              "    prob_yhat_1_majority  rel_share_min_of_maj  aver_abs_odds_diff  \\\n",
              "0               0.798551              0.007246            0.263695   \n",
              "1               0.794203              0.014493            0.121903   \n",
              "2               0.810145              0.028986            0.064134   \n",
              "3               0.791304              0.043478            0.084820   \n",
              "4               0.798551              0.057971            0.088763   \n",
              "5               0.804348              0.072464            0.026920   \n",
              "6               0.795652              0.108696            0.048625   \n",
              "7               0.795652              0.144928            0.074909   \n",
              "8               0.792754              0.181159            0.042678   \n",
              "9               0.798551              0.217391            0.063346   \n",
              "10              0.766667              0.253623            0.302510   \n",
              "11              0.788406              0.289855            0.259804   \n",
              "12              0.797101              0.362319            0.175325   \n",
              "13              0.811594              0.434783            0.072429   \n",
              "\n",
              "    stat_parity_diff  equal_opport_dist  disparate_impact  \n",
              "0           0.201449           0.124248          1.252269  \n",
              "1          -0.094203          -0.032637          0.881387  \n",
              "2          -0.060145          -0.036645          0.925760  \n",
              "3           0.008696           0.052820          1.010989  \n",
              "4          -0.123551          -0.122553          0.845281  \n",
              "5           0.015652          -0.003900          1.019459  \n",
              "6           0.017681           0.012224          1.022222  \n",
              "7          -0.075652          -0.081560          0.904918  \n",
              "8          -0.080754          -0.061578          0.898135  \n",
              "9          -0.098551          -0.117502          0.876588  \n",
              "10         -0.326667          -0.293211          0.573913  \n",
              "11         -0.278406          -0.241298          0.646875  \n",
              "12         -0.217101          -0.207922          0.727636  \n",
              "13         -0.088261          -0.052762          0.891250  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-PhSga0SMUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save metrics csv\n",
        "results_df_credit.to_csv(\"df_credit_metrics.csv\") \n",
        "from google.colab import files\n",
        "files.download(\"df_credit_metrics.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nLhop7beSsL",
        "colab_type": "text"
      },
      "source": [
        "#### 4) Create visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMDUGL3Nrq0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Feature Importance\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "from matplotlib import pyplot\n",
        "\n",
        "credit_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=1,\n",
        "              min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1.2, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "credit_model.fit(df_credit_train_input, df_credit_train_label)\n",
        "plot_importance(credit_model)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0krkR9GRNrK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "8e8abdf1-6faa-42ba-b06e-b12387bd0f04"
      },
      "source": [
        "# B: Learning Curve Function from scikit learn\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "# Define arguments\n",
        "f1 = make_scorer(f1_score) \n",
        "\n",
        "df_credit_train_input = pd.get_dummies(df_credit_train_input)\n",
        "\n",
        "\n",
        "# Plot actual learning curve\n",
        "plot_learning_curve(estimator = credit_model, \n",
        "                    title = \"Credit Dataset - XGBoost Learning Curve\", \n",
        "                    X = df_credit_train_input, y = df_credit_train_label, \n",
        "                    cv = 5, \n",
        "                    scoring = f1, \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = np.linspace(.1, 1.0, 20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'matplotlib.pyplot' from '/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxRUHBMJRTiX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "0470f150-b236-4785-ef44-c5c2a6068366"
      },
      "source": [
        "min_metrics_line_chart(results_df_credit, title = \"Minority Metrics for the Adult Dataset with XGBoost\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"f4690177-5cac-47a8-9f14-f6e1f1631ad3\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"f4690177-5cac-47a8-9f14-f6e1f1631ad3\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'f4690177-5cac-47a8-9f14-f6e1f1631ad3',\n",
              "                        [{\"mode\": \"lines+markers\", \"name\": \"F1 Minority\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300], \"y\": [0.7499999999999999, 0.7499999999999999, 0.787878787878788, 0.8, 0.8059701492537313, 0.8095238095238095, 0.8095238095238095, 0.7804878048780487, 0.8, 0.7883817427385892, 0.6822429906542056, 0.5217391304347826, 0.7911547911547911, 0.795131845841785]}, {\"mode\": \"lines+markers\", \"name\": \"TPR/Recall Minority\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300], \"y\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.979381443298969, 0.6636363636363637, 0.42857142857142855, 0.9757575757575757, 0.98989898989899]}, {\"mode\": \"lines+markers\", \"name\": \"FPR Minority\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300], \"y\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9534883720930233, 0.9245283018867925, 0.47692307692307695, 0.36486486486486486, 0.9529411764705882, 0.9705882352941176]}, {\"mode\": \"lines+markers\", \"name\": \"Avg. Abs. Odds Difference\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300], \"y\": [0.03157624148821203, 0.03157624148821203, 0.029572233472179965, 0.028570229464163932, 0.03319203852731645, 0.03319203852731645, 0.04705746571677388, 0.041433652645605334, 0.025417448802389886, 0.003245936395879445, 0.39914604224008365, 0.5509887854708386, 0.030733453168714786, 0.03777228250180936]}, {\"mode\": \"lines+markers\", \"name\": \"Statistical Parity Difference\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300], \"y\": [0.024637681159420333, 0.024637681159420333, 0.021739130434782594, 0.020289855072463725, 0.024637681159420333, 0.024637681159420333, 0.03768115942028982, 0.03188405797101446, 0.02168115942028981, -0.0008695652173913437, -0.38252587991718423, -0.5544202898550724, 0.017275362318840526, 0.02681159420289847]}, {\"mode\": \"lines+markers\", \"name\": \"Equal Opportunity Distance\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300], \"y\": [0.01603206412825653, 0.01603206412825653, 0.012024048096192397, 0.01002004008016033, 0.014028056112224463, 0.014028056112224463, 0.02605210420841686, 0.02004008016032066, 0.024048096192384794, 0.003429539491353828, -0.32233558025141185, -0.5453764672201546, 0.0038136879820246383, 0.015951094107406805]}, {\"mode\": \"lines+markers\", \"name\": \"Disparate Impact\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300], \"y\": [1.0252600297176822, 1.0252600297176822, 1.0222222222222221, 1.0207100591715976, 1.0252600297176822, 1.0252600297176822, 1.0391566265060241, 1.0329341317365268, 1.0225301204819277, 0.9990950226244344, 0.6083933870284018, 0.42212990936555894, 1.018170731707317, 1.0280303030303028]}],\n",
              "                        {\"font\": {\"size\": 14}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Minority Metrics for the Adult Dataset with XGBoost\", \"x\": 0.5, \"y\": 0.9, \"yanchor\": \"top\"}, \"xaxis\": {\"title\": {\"text\": \"Rows Minority\"}}, \"yaxis\": {\"title\": {\"text\": \"Metric Score\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f4690177-5cac-47a8-9f14-f6e1f1631ad3');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxycTP8pRYzV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "a81e47e1-fab2-48a4-9642-167f27a78b65"
      },
      "source": [
        "maj_min_metrics_line_chart(results_df_credit, title = \"Majority and Minority Metrics for the Adult Dataset with XGBoost\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"d5e52a13-47e6-4c9f-8dee-1a5e6363abb8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"d5e52a13-47e6-4c9f-8dee-1a5e6363abb8\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'd5e52a13-47e6-4c9f-8dee-1a5e6363abb8',\n",
              "                        [{\"mode\": \"lines+markers\", \"name\": \"F1 Majority\", \"type\": \"scatter\", \"x\": [695, 700, 710, 720, 730, 740, 765, 790, 815, 840, 865, 890, 940, 990], \"y\": [0.8378839590443685, 0.8378839590443685, 0.8398637137989777, 0.8408510638297872, 0.8395904436860069, 0.8395904436860069, 0.8357695614789338, 0.8380462724935732, 0.8374892519346518, 0.8382099827882961, 0.8388746803069054, 0.8372093023255814, 0.8398268398268398, 0.8386540120793787]}, {\"mode\": \"lines+markers\", \"name\": \"F1 Minority\", \"type\": \"scatter\", \"x\": [695, 700, 710, 720, 730, 740, 765, 790, 815, 840, 865, 890, 940, 990], \"y\": [0.7499999999999999, 0.7499999999999999, 0.787878787878788, 0.8, 0.8059701492537313, 0.8095238095238095, 0.8095238095238095, 0.7804878048780487, 0.8, 0.7883817427385892, 0.6822429906542056, 0.5217391304347826, 0.7911547911547911, 0.795131845841785]}, {\"mode\": \"lines+markers\", \"name\": \"TPR/Recall Majority\", \"type\": \"scatter\", \"x\": [695, 700, 710, 720, 730, 740, 765, 790, 815, 840, 865, 890, 940, 990], \"y\": [0.9839679358717435, 0.9839679358717435, 0.9879759519038076, 0.9899799599198397, 0.9859719438877755, 0.9859719438877755, 0.9739478957915831, 0.9799599198396793, 0.9759519038076152, 0.9759519038076152, 0.9859719438877755, 0.9739478957915831, 0.9719438877755511, 0.9739478957915831]}, {\"mode\": \"lines+markers\", \"name\": \"TPR/Recall Minority\", \"type\": \"scatter\", \"x\": [695, 700, 710, 720, 730, 740, 765, 790, 815, 840, 865, 890, 940, 990], \"y\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.979381443298969, 0.6636363636363637, 0.42857142857142855, 0.9757575757575757, 0.98989898989899]}, {\"mode\": \"lines+markers\", \"name\": \"FPR Majority\", \"type\": \"scatter\", \"x\": [695, 700, 710, 720, 730, 740, 765, 790, 815, 840, 865, 890, 940, 990], \"y\": [0.9528795811518325, 0.9528795811518325, 0.9528795811518325, 0.9528795811518325, 0.9476439790575916, 0.9476439790575916, 0.9319371727748691, 0.93717277486911, 0.9267015706806283, 0.9214659685863874, 0.9528795811518325, 0.9214659685863874, 0.8952879581151832, 0.9109947643979057]}, {\"mode\": \"lines+markers\", \"name\": \"FPR Minority\", \"type\": \"scatter\", \"x\": [695, 700, 710, 720, 730, 740, 765, 790, 815, 840, 865, 890, 940, 990], \"y\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9534883720930233, 0.9245283018867925, 0.47692307692307695, 0.36486486486486486, 0.9529411764705882, 0.9705882352941176]}],\n",
              "                        {\"font\": {\"size\": 14}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Majority and Minority Metrics for the Adult Dataset with XGBoost\", \"x\": 0.5, \"y\": 0.9, \"yanchor\": \"top\"}, \"xaxis\": {\"title\": {\"text\": \"Rows Complete\"}}, \"yaxis\": {\"title\": {\"text\": \"Metric Score\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d5e52a13-47e6-4c9f-8dee-1a5e6363abb8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EgjHGsyBv5q",
        "colab_type": "text"
      },
      "source": [
        "## 5) Home Credit Default Risk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JokphHiwjPhH",
        "colab_type": "text"
      },
      "source": [
        "https://www.kaggle.com/c/home-credit-default-risk/overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q14pe6cBdFMR",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpIKmP03oKYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/content/drive/My Drive/Master Thesis/Data/application_train.csv\"\n",
        "df_application = pd.read_csv(path)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsRsw_liHVYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_application"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMAaNMSWo6Z3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_application.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yL5zLpHm977",
        "colab_type": "text"
      },
      "source": [
        "### Initial Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QrkTKrr-0Fi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "15ebbbb4-6878-4185-ad25-e851ceb93cf5"
      },
      "source": [
        "df_application = df_application.drop([\"SK_ID_CURR\"], axis=1)\n",
        "\n",
        "# 1 should be positive (= good consequence, predict to repay load), 0 should be negative (= bad consequence, predict not to repay the loan)\n",
        "df_application[\"TARGET\"] = df_application[\"TARGET\"].replace({1: \"no_repay\", 0: \"repay\"})\n",
        "df_application[\"TARGET\"] = df_application[\"TARGET\"].replace({\"no_repay\": 0, \"repay\": 1})\n",
        "\n",
        "# Replace XNA for CODE_GENDER by NaN\n",
        "df_application[\"CODE_GENDER\"].replace('XNA', np.nan)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         M\n",
              "1         F\n",
              "2         M\n",
              "3         F\n",
              "4         M\n",
              "         ..\n",
              "307506    M\n",
              "307507    F\n",
              "307508    F\n",
              "307509    F\n",
              "307510    F\n",
              "Name: CODE_GENDER, Length: 307511, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBMUx46IHssC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check whether binary encoding was successful and seperate datasets were created\n",
        "print(df_application.groupby([\"TARGET\"]).agg({\"TARGET\": 'count'}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9wvVDK-HiBd",
        "colab_type": "text"
      },
      "source": [
        "**Dataframe Size Reduction Efforts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFQisM73Hul8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_application.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phrZc26CHcRC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "7ed36f23-5dca-4dc6-a353-07e66e1bd0e5"
      },
      "source": [
        "# Check which columns are suitable for conversion in \"category\" data format\n",
        "## We should stick to using the category type primarily for object columns where less than 50% of the values are unique.\n",
        "df_application_copy = df_application.select_dtypes(include=['object']).copy()\n",
        "df_application_copy.describe()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NAME_CONTRACT_TYPE</th>\n",
              "      <th>CODE_GENDER</th>\n",
              "      <th>FLAG_OWN_CAR</th>\n",
              "      <th>FLAG_OWN_REALTY</th>\n",
              "      <th>NAME_TYPE_SUITE</th>\n",
              "      <th>NAME_INCOME_TYPE</th>\n",
              "      <th>NAME_EDUCATION_TYPE</th>\n",
              "      <th>NAME_FAMILY_STATUS</th>\n",
              "      <th>NAME_HOUSING_TYPE</th>\n",
              "      <th>OCCUPATION_TYPE</th>\n",
              "      <th>WEEKDAY_APPR_PROCESS_START</th>\n",
              "      <th>ORGANIZATION_TYPE</th>\n",
              "      <th>FONDKAPREMONT_MODE</th>\n",
              "      <th>HOUSETYPE_MODE</th>\n",
              "      <th>WALLSMATERIAL_MODE</th>\n",
              "      <th>EMERGENCYSTATE_MODE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>307511</td>\n",
              "      <td>307511</td>\n",
              "      <td>307511</td>\n",
              "      <td>307511</td>\n",
              "      <td>306219</td>\n",
              "      <td>307511</td>\n",
              "      <td>307511</td>\n",
              "      <td>307511</td>\n",
              "      <td>307511</td>\n",
              "      <td>211120</td>\n",
              "      <td>307511</td>\n",
              "      <td>307511</td>\n",
              "      <td>97216</td>\n",
              "      <td>153214</td>\n",
              "      <td>151170</td>\n",
              "      <td>161756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>18</td>\n",
              "      <td>7</td>\n",
              "      <td>58</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Cash loans</td>\n",
              "      <td>F</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Unaccompanied</td>\n",
              "      <td>Working</td>\n",
              "      <td>Secondary / secondary special</td>\n",
              "      <td>Married</td>\n",
              "      <td>House / apartment</td>\n",
              "      <td>Laborers</td>\n",
              "      <td>TUESDAY</td>\n",
              "      <td>Business Entity Type 3</td>\n",
              "      <td>reg oper account</td>\n",
              "      <td>block of flats</td>\n",
              "      <td>Panel</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>278232</td>\n",
              "      <td>202448</td>\n",
              "      <td>202924</td>\n",
              "      <td>213312</td>\n",
              "      <td>248526</td>\n",
              "      <td>158774</td>\n",
              "      <td>218391</td>\n",
              "      <td>196432</td>\n",
              "      <td>272868</td>\n",
              "      <td>55186</td>\n",
              "      <td>53901</td>\n",
              "      <td>67992</td>\n",
              "      <td>73830</td>\n",
              "      <td>150503</td>\n",
              "      <td>66040</td>\n",
              "      <td>159428</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR FLAG_OWN_REALTY  \\\n",
              "count              307511      307511       307511          307511   \n",
              "unique                  2           3            2               2   \n",
              "top            Cash loans           F            N               Y   \n",
              "freq               278232      202448       202924          213312   \n",
              "\n",
              "       NAME_TYPE_SUITE NAME_INCOME_TYPE            NAME_EDUCATION_TYPE  \\\n",
              "count           306219           307511                         307511   \n",
              "unique               7                8                              5   \n",
              "top      Unaccompanied          Working  Secondary / secondary special   \n",
              "freq            248526           158774                         218391   \n",
              "\n",
              "       NAME_FAMILY_STATUS  NAME_HOUSING_TYPE OCCUPATION_TYPE  \\\n",
              "count              307511             307511          211120   \n",
              "unique                  6                  6              18   \n",
              "top               Married  House / apartment        Laborers   \n",
              "freq               196432             272868           55186   \n",
              "\n",
              "       WEEKDAY_APPR_PROCESS_START       ORGANIZATION_TYPE FONDKAPREMONT_MODE  \\\n",
              "count                      307511                  307511              97216   \n",
              "unique                          7                      58                  4   \n",
              "top                       TUESDAY  Business Entity Type 3   reg oper account   \n",
              "freq                        53901                   67992              73830   \n",
              "\n",
              "        HOUSETYPE_MODE WALLSMATERIAL_MODE EMERGENCYSTATE_MODE  \n",
              "count           153214             151170              161756  \n",
              "unique               3                  7                   2  \n",
              "top     block of flats              Panel                  No  \n",
              "freq            150503              66040              159428  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Lm-m7IeH59S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reduce the size of the numeric columns\n",
        "\n",
        "df_application, NAlist = reduce_mem_usage(df_application)\n",
        "print(\"_________________\")\n",
        "print(\"\")\n",
        "print(\"Warning: the following columns have missing values filled with 'df['column_name'].min() -1': \")\n",
        "print(\"_________________\")\n",
        "print(\"\")\n",
        "print(NAlist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3m20uYXIyiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_application.select_dtypes(include=['object']).columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qkLqZNcH60e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for col in ['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY',\n",
        "            'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE',\n",
        "            'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE',\n",
        "            'WEEKDAY_APPR_PROCESS_START', 'ORGANIZATION_TYPE', 'FONDKAPREMONT_MODE',\n",
        "            'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE']:\n",
        "    df_application[col] = df_application[col].astype('category')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXEVy0agdEPj",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Wh0GXBzEsM0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3b24fbe7-6995-43a2-fa9b-59b926150122"
      },
      "source": [
        "eda_descr_stats(df_application, disc_feature= \"CODE_GENDER\", disc_min_value=\"F\", label=\"TARGET\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. Sensitive Attribute: One or more of the following features are sensitive ones: Index(['TARGET', 'NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR',\n",
            "       'FLAG_OWN_REALTY', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT',\n",
            "       'AMT_ANNUITY', 'AMT_GOODS_PRICE',\n",
            "       ...\n",
            "       'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20',\n",
            "       'FLAG_DOCUMENT_21', 'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
            "       'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
            "       'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT',\n",
            "       'AMT_REQ_CREDIT_BUREAU_YEAR'],\n",
            "      dtype='object', length=121).\n",
            "1. Sensitive Attribute: These are the individual values for the sensitive attribute: [M, F, XNA]\n",
            "Categories (3, object): [M, F, XNA].\n",
            "2. Binary Target Variable: The Binary Target Feature has the following values and counts:\n",
            "        TARGET\n",
            "TARGET        \n",
            "0        24825\n",
            "1       282686\n",
            "3. The Total Number of Predictor Features is: 121.\n",
            "4. The Total Number of Training Examples is: 307511.\n",
            "5. The Total Number of Training Examples in the Minority Group is: 202448.\n",
            "6. Sample Size Disparity: The Absolute numbers of members of different races are as follows:  F      202448\n",
            "M      105059\n",
            "XNA         4\n",
            "Name: CODE_GENDER, dtype: int64.\n",
            "6. Sample Size Disparity: The Percentages of the number of members of different races are as follows: F      0.658344\n",
            "M      0.341643\n",
            "XNA    0.000013\n",
            "Name: CODE_GENDER, dtype: float64.\n",
            "7. Class Balance: The Class Balance looks as follows:\n",
            "1    282686\n",
            "0     24825\n",
            "Name: TARGET, dtype: int64\n",
            "1    0.919271\n",
            "0    0.080729\n",
            "Name: TARGET, dtype: float64\n",
            "8. Coarseness of Features: Details on missing values of features in the dataset:\n",
            "TARGET                        0\n",
            "NAME_CONTRACT_TYPE            0\n",
            "CODE_GENDER                   0\n",
            "FLAG_OWN_CAR                  0\n",
            "FLAG_OWN_REALTY               0\n",
            "                             ..\n",
            "AMT_REQ_CREDIT_BUREAU_DAY     0\n",
            "AMT_REQ_CREDIT_BUREAU_WEEK    0\n",
            "AMT_REQ_CREDIT_BUREAU_MON     0\n",
            "AMT_REQ_CREDIT_BUREAU_QRT     0\n",
            "AMT_REQ_CREDIT_BUREAU_YEAR    0\n",
            "Length: 121, dtype: int64\n",
            "             TARGET  NAME_CONTRACT_TYPE  CODE_GENDER  FLAG_OWN_CAR  \\\n",
            "CODE_GENDER                                                          \n",
            "F                 0                   0            0             0   \n",
            "M                 0                   0            0             0   \n",
            "XNA               0                   0            0             0   \n",
            "\n",
            "             FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  \\\n",
            "CODE_GENDER                                                                \n",
            "F                          0             0                 0           0   \n",
            "M                          0             0                 0           0   \n",
            "XNA                        0             0                 0           0   \n",
            "\n",
            "             AMT_ANNUITY  AMT_GOODS_PRICE  NAME_TYPE_SUITE  NAME_INCOME_TYPE  \\\n",
            "CODE_GENDER                                                                    \n",
            "F                      0                0              805                 0   \n",
            "M                      0                0              487                 0   \n",
            "XNA                    0                0                0                 0   \n",
            "\n",
            "             NAME_EDUCATION_TYPE  NAME_FAMILY_STATUS  NAME_HOUSING_TYPE  \\\n",
            "CODE_GENDER                                                               \n",
            "F                              0                   0                  0   \n",
            "M                              0                   0                  0   \n",
            "XNA                            0                   0                  0   \n",
            "\n",
            "             REGION_POPULATION_RELATIVE  DAYS_BIRTH  DAYS_EMPLOYED  \\\n",
            "CODE_GENDER                                                          \n",
            "F                                     0           0              0   \n",
            "M                                     0           0              0   \n",
            "XNA                                   0           0              0   \n",
            "\n",
            "             DAYS_REGISTRATION  DAYS_ID_PUBLISH  OWN_CAR_AGE  FLAG_MOBIL  \\\n",
            "CODE_GENDER                                                                \n",
            "F                            0                0            0           0   \n",
            "M                            0                0            0           0   \n",
            "XNA                          0                0            0           0   \n",
            "\n",
            "             FLAG_EMP_PHONE  FLAG_WORK_PHONE  FLAG_CONT_MOBILE  FLAG_PHONE  \\\n",
            "CODE_GENDER                                                                  \n",
            "F                         0                0                 0           0   \n",
            "M                         0                0                 0           0   \n",
            "XNA                       0                0                 0           0   \n",
            "\n",
            "             FLAG_EMAIL  OCCUPATION_TYPE  CNT_FAM_MEMBERS  \\\n",
            "CODE_GENDER                                                 \n",
            "F                     0            73166                0   \n",
            "M                     0            23223                0   \n",
            "XNA                   0                2                0   \n",
            "\n",
            "             REGION_RATING_CLIENT  REGION_RATING_CLIENT_W_CITY  \\\n",
            "CODE_GENDER                                                      \n",
            "F                               0                            0   \n",
            "M                               0                            0   \n",
            "XNA                             0                            0   \n",
            "\n",
            "             WEEKDAY_APPR_PROCESS_START  HOUR_APPR_PROCESS_START  \\\n",
            "CODE_GENDER                                                        \n",
            "F                                     0                        0   \n",
            "M                                     0                        0   \n",
            "XNA                                   0                        0   \n",
            "\n",
            "             REG_REGION_NOT_LIVE_REGION  REG_REGION_NOT_WORK_REGION  \\\n",
            "CODE_GENDER                                                           \n",
            "F                                     0                           0   \n",
            "M                                     0                           0   \n",
            "XNA                                   0                           0   \n",
            "\n",
            "             LIVE_REGION_NOT_WORK_REGION  REG_CITY_NOT_LIVE_CITY  \\\n",
            "CODE_GENDER                                                        \n",
            "F                                      0                       0   \n",
            "M                                      0                       0   \n",
            "XNA                                    0                       0   \n",
            "\n",
            "             REG_CITY_NOT_WORK_CITY  LIVE_CITY_NOT_WORK_CITY  \\\n",
            "CODE_GENDER                                                    \n",
            "F                                 0                        0   \n",
            "M                                 0                        0   \n",
            "XNA                               0                        0   \n",
            "\n",
            "             ORGANIZATION_TYPE  EXT_SOURCE_1  EXT_SOURCE_2  EXT_SOURCE_3  \\\n",
            "CODE_GENDER                                                                \n",
            "F                            0             0             0             0   \n",
            "M                            0             0             0             0   \n",
            "XNA                          0             0             0             0   \n",
            "\n",
            "             APARTMENTS_AVG  BASEMENTAREA_AVG  YEARS_BEGINEXPLUATATION_AVG  \\\n",
            "CODE_GENDER                                                                  \n",
            "F                         0                 0                            0   \n",
            "M                         0                 0                            0   \n",
            "XNA                       0                 0                            0   \n",
            "\n",
            "             YEARS_BUILD_AVG  COMMONAREA_AVG  ELEVATORS_AVG  ENTRANCES_AVG  \\\n",
            "CODE_GENDER                                                                  \n",
            "F                          0               0              0              0   \n",
            "M                          0               0              0              0   \n",
            "XNA                        0               0              0              0   \n",
            "\n",
            "             FLOORSMAX_AVG  FLOORSMIN_AVG  LANDAREA_AVG  LIVINGAPARTMENTS_AVG  \\\n",
            "CODE_GENDER                                                                     \n",
            "F                        0              0             0                     0   \n",
            "M                        0              0             0                     0   \n",
            "XNA                      0              0             0                     0   \n",
            "\n",
            "             LIVINGAREA_AVG  NONLIVINGAPARTMENTS_AVG  NONLIVINGAREA_AVG  \\\n",
            "CODE_GENDER                                                               \n",
            "F                         0                        0                  0   \n",
            "M                         0                        0                  0   \n",
            "XNA                       0                        0                  0   \n",
            "\n",
            "             APARTMENTS_MODE  BASEMENTAREA_MODE  YEARS_BEGINEXPLUATATION_MODE  \\\n",
            "CODE_GENDER                                                                     \n",
            "F                          0                  0                             0   \n",
            "M                          0                  0                             0   \n",
            "XNA                        0                  0                             0   \n",
            "\n",
            "             YEARS_BUILD_MODE  COMMONAREA_MODE  ELEVATORS_MODE  \\\n",
            "CODE_GENDER                                                      \n",
            "F                           0                0               0   \n",
            "M                           0                0               0   \n",
            "XNA                         0                0               0   \n",
            "\n",
            "             ENTRANCES_MODE  FLOORSMAX_MODE  FLOORSMIN_MODE  LANDAREA_MODE  \\\n",
            "CODE_GENDER                                                                  \n",
            "F                         0               0               0              0   \n",
            "M                         0               0               0              0   \n",
            "XNA                       0               0               0              0   \n",
            "\n",
            "             LIVINGAPARTMENTS_MODE  LIVINGAREA_MODE  NONLIVINGAPARTMENTS_MODE  \\\n",
            "CODE_GENDER                                                                     \n",
            "F                                0                0                         0   \n",
            "M                                0                0                         0   \n",
            "XNA                              0                0                         0   \n",
            "\n",
            "             NONLIVINGAREA_MODE  APARTMENTS_MEDI  BASEMENTAREA_MEDI  \\\n",
            "CODE_GENDER                                                           \n",
            "F                             0                0                  0   \n",
            "M                             0                0                  0   \n",
            "XNA                           0                0                  0   \n",
            "\n",
            "             YEARS_BEGINEXPLUATATION_MEDI  YEARS_BUILD_MEDI  COMMONAREA_MEDI  \\\n",
            "CODE_GENDER                                                                    \n",
            "F                                       0                 0                0   \n",
            "M                                       0                 0                0   \n",
            "XNA                                     0                 0                0   \n",
            "\n",
            "             ELEVATORS_MEDI  ENTRANCES_MEDI  FLOORSMAX_MEDI  FLOORSMIN_MEDI  \\\n",
            "CODE_GENDER                                                                   \n",
            "F                         0               0               0               0   \n",
            "M                         0               0               0               0   \n",
            "XNA                       0               0               0               0   \n",
            "\n",
            "             LANDAREA_MEDI  LIVINGAPARTMENTS_MEDI  LIVINGAREA_MEDI  \\\n",
            "CODE_GENDER                                                          \n",
            "F                        0                      0                0   \n",
            "M                        0                      0                0   \n",
            "XNA                      0                      0                0   \n",
            "\n",
            "             NONLIVINGAPARTMENTS_MEDI  NONLIVINGAREA_MEDI  FONDKAPREMONT_MODE  \\\n",
            "CODE_GENDER                                                                     \n",
            "F                                   0                   0              137320   \n",
            "M                                   0                   0               72973   \n",
            "XNA                                 0                   0                   2   \n",
            "\n",
            "             HOUSETYPE_MODE  TOTALAREA_MODE  WALLSMATERIAL_MODE  \\\n",
            "CODE_GENDER                                                       \n",
            "F                     99549               0              100951   \n",
            "M                     54747               0               55389   \n",
            "XNA                       1               0                   1   \n",
            "\n",
            "             EMERGENCYSTATE_MODE  OBS_30_CNT_SOCIAL_CIRCLE  \\\n",
            "CODE_GENDER                                                  \n",
            "F                          93753                         0   \n",
            "M                          52001                         0   \n",
            "XNA                            1                         0   \n",
            "\n",
            "             DEF_30_CNT_SOCIAL_CIRCLE  OBS_60_CNT_SOCIAL_CIRCLE  \\\n",
            "CODE_GENDER                                                       \n",
            "F                                   0                         0   \n",
            "M                                   0                         0   \n",
            "XNA                                 0                         0   \n",
            "\n",
            "             DEF_60_CNT_SOCIAL_CIRCLE  DAYS_LAST_PHONE_CHANGE  \\\n",
            "CODE_GENDER                                                     \n",
            "F                                   0                       0   \n",
            "M                                   0                       0   \n",
            "XNA                                 0                       0   \n",
            "\n",
            "             FLAG_DOCUMENT_2  FLAG_DOCUMENT_3  FLAG_DOCUMENT_4  \\\n",
            "CODE_GENDER                                                      \n",
            "F                          0                0                0   \n",
            "M                          0                0                0   \n",
            "XNA                        0                0                0   \n",
            "\n",
            "             FLAG_DOCUMENT_5  FLAG_DOCUMENT_6  FLAG_DOCUMENT_7  \\\n",
            "CODE_GENDER                                                      \n",
            "F                          0                0                0   \n",
            "M                          0                0                0   \n",
            "XNA                        0                0                0   \n",
            "\n",
            "             FLAG_DOCUMENT_8  FLAG_DOCUMENT_9  FLAG_DOCUMENT_10  \\\n",
            "CODE_GENDER                                                       \n",
            "F                          0                0                 0   \n",
            "M                          0                0                 0   \n",
            "XNA                        0                0                 0   \n",
            "\n",
            "             FLAG_DOCUMENT_11  FLAG_DOCUMENT_12  FLAG_DOCUMENT_13  \\\n",
            "CODE_GENDER                                                         \n",
            "F                           0                 0                 0   \n",
            "M                           0                 0                 0   \n",
            "XNA                         0                 0                 0   \n",
            "\n",
            "             FLAG_DOCUMENT_14  FLAG_DOCUMENT_15  FLAG_DOCUMENT_16  \\\n",
            "CODE_GENDER                                                         \n",
            "F                           0                 0                 0   \n",
            "M                           0                 0                 0   \n",
            "XNA                         0                 0                 0   \n",
            "\n",
            "             FLAG_DOCUMENT_17  FLAG_DOCUMENT_18  FLAG_DOCUMENT_19  \\\n",
            "CODE_GENDER                                                         \n",
            "F                           0                 0                 0   \n",
            "M                           0                 0                 0   \n",
            "XNA                         0                 0                 0   \n",
            "\n",
            "             FLAG_DOCUMENT_20  FLAG_DOCUMENT_21  AMT_REQ_CREDIT_BUREAU_HOUR  \\\n",
            "CODE_GENDER                                                                   \n",
            "F                           0                 0                           0   \n",
            "M                           0                 0                           0   \n",
            "XNA                         0                 0                           0   \n",
            "\n",
            "             AMT_REQ_CREDIT_BUREAU_DAY  AMT_REQ_CREDIT_BUREAU_WEEK  \\\n",
            "CODE_GENDER                                                          \n",
            "F                                    0                           0   \n",
            "M                                    0                           0   \n",
            "XNA                                  0                           0   \n",
            "\n",
            "             AMT_REQ_CREDIT_BUREAU_MON  AMT_REQ_CREDIT_BUREAU_QRT  \\\n",
            "CODE_GENDER                                                         \n",
            "F                                    0                          0   \n",
            "M                                    0                          0   \n",
            "XNA                                  0                          0   \n",
            "\n",
            "             AMT_REQ_CREDIT_BUREAU_YEAR  \n",
            "CODE_GENDER                              \n",
            "F                                     0  \n",
            "M                                     0  \n",
            "XNA                                   0  \n",
            "             TARGET  NAME_CONTRACT_TYPE  CODE_GENDER  FLAG_OWN_CAR  \\\n",
            "CODE_GENDER                                                          \n",
            "F               0.0                 0.0          0.0           0.0   \n",
            "M               0.0                 0.0          0.0           0.0   \n",
            "XNA             0.0                 0.0          0.0           0.0   \n",
            "\n",
            "             FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  \\\n",
            "CODE_GENDER                                                                \n",
            "F                        0.0           0.0               0.0         0.0   \n",
            "M                        0.0           0.0               0.0         0.0   \n",
            "XNA                      0.0           0.0               0.0         0.0   \n",
            "\n",
            "             AMT_ANNUITY  AMT_GOODS_PRICE  NAME_TYPE_SUITE  NAME_INCOME_TYPE  \\\n",
            "CODE_GENDER                                                                    \n",
            "F                    0.0              0.0         0.003976               0.0   \n",
            "M                    0.0              0.0         0.004635               0.0   \n",
            "XNA                  0.0              0.0         0.000000               0.0   \n",
            "\n",
            "             NAME_EDUCATION_TYPE  NAME_FAMILY_STATUS  NAME_HOUSING_TYPE  \\\n",
            "CODE_GENDER                                                               \n",
            "F                            0.0                 0.0                0.0   \n",
            "M                            0.0                 0.0                0.0   \n",
            "XNA                          0.0                 0.0                0.0   \n",
            "\n",
            "             REGION_POPULATION_RELATIVE  DAYS_BIRTH  DAYS_EMPLOYED  \\\n",
            "CODE_GENDER                                                          \n",
            "F                                   0.0         0.0            0.0   \n",
            "M                                   0.0         0.0            0.0   \n",
            "XNA                                 0.0         0.0            0.0   \n",
            "\n",
            "             DAYS_REGISTRATION  DAYS_ID_PUBLISH  OWN_CAR_AGE  FLAG_MOBIL  \\\n",
            "CODE_GENDER                                                                \n",
            "F                          0.0              0.0          0.0         0.0   \n",
            "M                          0.0              0.0          0.0         0.0   \n",
            "XNA                        0.0              0.0          0.0         0.0   \n",
            "\n",
            "             FLAG_EMP_PHONE  FLAG_WORK_PHONE  FLAG_CONT_MOBILE  FLAG_PHONE  \\\n",
            "CODE_GENDER                                                                  \n",
            "F                       0.0              0.0               0.0         0.0   \n",
            "M                       0.0              0.0               0.0         0.0   \n",
            "XNA                     0.0              0.0               0.0         0.0   \n",
            "\n",
            "             FLAG_EMAIL  OCCUPATION_TYPE  CNT_FAM_MEMBERS  \\\n",
            "CODE_GENDER                                                 \n",
            "F                   0.0         0.361406              0.0   \n",
            "M                   0.0         0.221047              0.0   \n",
            "XNA                 0.0         0.500000              0.0   \n",
            "\n",
            "             REGION_RATING_CLIENT  REGION_RATING_CLIENT_W_CITY  \\\n",
            "CODE_GENDER                                                      \n",
            "F                             0.0                          0.0   \n",
            "M                             0.0                          0.0   \n",
            "XNA                           0.0                          0.0   \n",
            "\n",
            "             WEEKDAY_APPR_PROCESS_START  HOUR_APPR_PROCESS_START  \\\n",
            "CODE_GENDER                                                        \n",
            "F                                   0.0                      0.0   \n",
            "M                                   0.0                      0.0   \n",
            "XNA                                 0.0                      0.0   \n",
            "\n",
            "             REG_REGION_NOT_LIVE_REGION  REG_REGION_NOT_WORK_REGION  \\\n",
            "CODE_GENDER                                                           \n",
            "F                                   0.0                         0.0   \n",
            "M                                   0.0                         0.0   \n",
            "XNA                                 0.0                         0.0   \n",
            "\n",
            "             LIVE_REGION_NOT_WORK_REGION  REG_CITY_NOT_LIVE_CITY  \\\n",
            "CODE_GENDER                                                        \n",
            "F                                    0.0                     0.0   \n",
            "M                                    0.0                     0.0   \n",
            "XNA                                  0.0                     0.0   \n",
            "\n",
            "             REG_CITY_NOT_WORK_CITY  LIVE_CITY_NOT_WORK_CITY  \\\n",
            "CODE_GENDER                                                    \n",
            "F                               0.0                      0.0   \n",
            "M                               0.0                      0.0   \n",
            "XNA                             0.0                      0.0   \n",
            "\n",
            "             ORGANIZATION_TYPE  EXT_SOURCE_1  EXT_SOURCE_2  EXT_SOURCE_3  \\\n",
            "CODE_GENDER                                                                \n",
            "F                          0.0           0.0           0.0           0.0   \n",
            "M                          0.0           0.0           0.0           0.0   \n",
            "XNA                        0.0           0.0           0.0           0.0   \n",
            "\n",
            "             APARTMENTS_AVG  BASEMENTAREA_AVG  YEARS_BEGINEXPLUATATION_AVG  \\\n",
            "CODE_GENDER                                                                  \n",
            "F                       0.0               0.0                          0.0   \n",
            "M                       0.0               0.0                          0.0   \n",
            "XNA                     0.0               0.0                          0.0   \n",
            "\n",
            "             YEARS_BUILD_AVG  COMMONAREA_AVG  ELEVATORS_AVG  ENTRANCES_AVG  \\\n",
            "CODE_GENDER                                                                  \n",
            "F                        0.0             0.0            0.0            0.0   \n",
            "M                        0.0             0.0            0.0            0.0   \n",
            "XNA                      0.0             0.0            0.0            0.0   \n",
            "\n",
            "             FLOORSMAX_AVG  FLOORSMIN_AVG  LANDAREA_AVG  LIVINGAPARTMENTS_AVG  \\\n",
            "CODE_GENDER                                                                     \n",
            "F                      0.0            0.0           0.0                   0.0   \n",
            "M                      0.0            0.0           0.0                   0.0   \n",
            "XNA                    0.0            0.0           0.0                   0.0   \n",
            "\n",
            "             LIVINGAREA_AVG  NONLIVINGAPARTMENTS_AVG  NONLIVINGAREA_AVG  \\\n",
            "CODE_GENDER                                                               \n",
            "F                       0.0                      0.0                0.0   \n",
            "M                       0.0                      0.0                0.0   \n",
            "XNA                     0.0                      0.0                0.0   \n",
            "\n",
            "             APARTMENTS_MODE  BASEMENTAREA_MODE  YEARS_BEGINEXPLUATATION_MODE  \\\n",
            "CODE_GENDER                                                                     \n",
            "F                        0.0                0.0                           0.0   \n",
            "M                        0.0                0.0                           0.0   \n",
            "XNA                      0.0                0.0                           0.0   \n",
            "\n",
            "             YEARS_BUILD_MODE  COMMONAREA_MODE  ELEVATORS_MODE  \\\n",
            "CODE_GENDER                                                      \n",
            "F                         0.0              0.0             0.0   \n",
            "M                         0.0              0.0             0.0   \n",
            "XNA                       0.0              0.0             0.0   \n",
            "\n",
            "             ENTRANCES_MODE  FLOORSMAX_MODE  FLOORSMIN_MODE  LANDAREA_MODE  \\\n",
            "CODE_GENDER                                                                  \n",
            "F                       0.0             0.0             0.0            0.0   \n",
            "M                       0.0             0.0             0.0            0.0   \n",
            "XNA                     0.0             0.0             0.0            0.0   \n",
            "\n",
            "             LIVINGAPARTMENTS_MODE  LIVINGAREA_MODE  NONLIVINGAPARTMENTS_MODE  \\\n",
            "CODE_GENDER                                                                     \n",
            "F                              0.0              0.0                       0.0   \n",
            "M                              0.0              0.0                       0.0   \n",
            "XNA                            0.0              0.0                       0.0   \n",
            "\n",
            "             NONLIVINGAREA_MODE  APARTMENTS_MEDI  BASEMENTAREA_MEDI  \\\n",
            "CODE_GENDER                                                           \n",
            "F                           0.0              0.0                0.0   \n",
            "M                           0.0              0.0                0.0   \n",
            "XNA                         0.0              0.0                0.0   \n",
            "\n",
            "             YEARS_BEGINEXPLUATATION_MEDI  YEARS_BUILD_MEDI  COMMONAREA_MEDI  \\\n",
            "CODE_GENDER                                                                    \n",
            "F                                     0.0               0.0              0.0   \n",
            "M                                     0.0               0.0              0.0   \n",
            "XNA                                   0.0               0.0              0.0   \n",
            "\n",
            "             ELEVATORS_MEDI  ENTRANCES_MEDI  FLOORSMAX_MEDI  FLOORSMIN_MEDI  \\\n",
            "CODE_GENDER                                                                   \n",
            "F                       0.0             0.0             0.0             0.0   \n",
            "M                       0.0             0.0             0.0             0.0   \n",
            "XNA                     0.0             0.0             0.0             0.0   \n",
            "\n",
            "             LANDAREA_MEDI  LIVINGAPARTMENTS_MEDI  LIVINGAREA_MEDI  \\\n",
            "CODE_GENDER                                                          \n",
            "F                      0.0                    0.0              0.0   \n",
            "M                      0.0                    0.0              0.0   \n",
            "XNA                    0.0                    0.0              0.0   \n",
            "\n",
            "             NONLIVINGAPARTMENTS_MEDI  NONLIVINGAREA_MEDI  FONDKAPREMONT_MODE  \\\n",
            "CODE_GENDER                                                                     \n",
            "F                                 0.0                 0.0            0.678298   \n",
            "M                                 0.0                 0.0            0.694591   \n",
            "XNA                               0.0                 0.0            0.500000   \n",
            "\n",
            "             HOUSETYPE_MODE  TOTALAREA_MODE  WALLSMATERIAL_MODE  \\\n",
            "CODE_GENDER                                                       \n",
            "F                  0.491726             0.0            0.498652   \n",
            "M                  0.521107             0.0            0.527218   \n",
            "XNA                0.250000             0.0            0.250000   \n",
            "\n",
            "             EMERGENCYSTATE_MODE  OBS_30_CNT_SOCIAL_CIRCLE  \\\n",
            "CODE_GENDER                                                  \n",
            "F                       0.463097                       0.0   \n",
            "M                       0.494969                       0.0   \n",
            "XNA                     0.250000                       0.0   \n",
            "\n",
            "             DEF_30_CNT_SOCIAL_CIRCLE  OBS_60_CNT_SOCIAL_CIRCLE  \\\n",
            "CODE_GENDER                                                       \n",
            "F                                 0.0                       0.0   \n",
            "M                                 0.0                       0.0   \n",
            "XNA                               0.0                       0.0   \n",
            "\n",
            "             DEF_60_CNT_SOCIAL_CIRCLE  DAYS_LAST_PHONE_CHANGE  \\\n",
            "CODE_GENDER                                                     \n",
            "F                                 0.0                     0.0   \n",
            "M                                 0.0                     0.0   \n",
            "XNA                               0.0                     0.0   \n",
            "\n",
            "             FLAG_DOCUMENT_2  FLAG_DOCUMENT_3  FLAG_DOCUMENT_4  \\\n",
            "CODE_GENDER                                                      \n",
            "F                        0.0              0.0              0.0   \n",
            "M                        0.0              0.0              0.0   \n",
            "XNA                      0.0              0.0              0.0   \n",
            "\n",
            "             FLAG_DOCUMENT_5  FLAG_DOCUMENT_6  FLAG_DOCUMENT_7  \\\n",
            "CODE_GENDER                                                      \n",
            "F                        0.0              0.0              0.0   \n",
            "M                        0.0              0.0              0.0   \n",
            "XNA                      0.0              0.0              0.0   \n",
            "\n",
            "             FLAG_DOCUMENT_8  FLAG_DOCUMENT_9  FLAG_DOCUMENT_10  \\\n",
            "CODE_GENDER                                                       \n",
            "F                        0.0              0.0               0.0   \n",
            "M                        0.0              0.0               0.0   \n",
            "XNA                      0.0              0.0               0.0   \n",
            "\n",
            "             FLAG_DOCUMENT_11  FLAG_DOCUMENT_12  FLAG_DOCUMENT_13  \\\n",
            "CODE_GENDER                                                         \n",
            "F                         0.0               0.0               0.0   \n",
            "M                         0.0               0.0               0.0   \n",
            "XNA                       0.0               0.0               0.0   \n",
            "\n",
            "             FLAG_DOCUMENT_14  FLAG_DOCUMENT_15  FLAG_DOCUMENT_16  \\\n",
            "CODE_GENDER                                                         \n",
            "F                         0.0               0.0               0.0   \n",
            "M                         0.0               0.0               0.0   \n",
            "XNA                       0.0               0.0               0.0   \n",
            "\n",
            "             FLAG_DOCUMENT_17  FLAG_DOCUMENT_18  FLAG_DOCUMENT_19  \\\n",
            "CODE_GENDER                                                         \n",
            "F                         0.0               0.0               0.0   \n",
            "M                         0.0               0.0               0.0   \n",
            "XNA                       0.0               0.0               0.0   \n",
            "\n",
            "             FLAG_DOCUMENT_20  FLAG_DOCUMENT_21  AMT_REQ_CREDIT_BUREAU_HOUR  \\\n",
            "CODE_GENDER                                                                   \n",
            "F                         0.0               0.0                         0.0   \n",
            "M                         0.0               0.0                         0.0   \n",
            "XNA                       0.0               0.0                         0.0   \n",
            "\n",
            "             AMT_REQ_CREDIT_BUREAU_DAY  AMT_REQ_CREDIT_BUREAU_WEEK  \\\n",
            "CODE_GENDER                                                          \n",
            "F                                  0.0                         0.0   \n",
            "M                                  0.0                         0.0   \n",
            "XNA                                0.0                         0.0   \n",
            "\n",
            "             AMT_REQ_CREDIT_BUREAU_MON  AMT_REQ_CREDIT_BUREAU_QRT  \\\n",
            "CODE_GENDER                                                         \n",
            "F                                  0.0                        0.0   \n",
            "M                                  0.0                        0.0   \n",
            "XNA                                0.0                        0.0   \n",
            "\n",
            "             AMT_REQ_CREDIT_BUREAU_YEAR  \n",
            "CODE_GENDER                              \n",
            "F                                   0.0  \n",
            "M                                   0.0  \n",
            "XNA                                 0.0  \n",
            "9. Severity of Outliers for Numeric Features\n",
            "AxesSubplot(0.125,0.125;0.775x0.755)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frEeeyOcdeia",
        "colab_type": "text"
      },
      "source": [
        "### Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbfAc6dAdmp5",
        "colab_type": "text"
      },
      "source": [
        "#### 0) Preprocess data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjSg39N2FKik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute preprocessing function\n",
        "df_application_train_input, df_application_train_label = fair_preprocess(data = df_application, \n",
        "                                                                        label = \"TARGET\", \n",
        "                                                                        neg_class = 0, \n",
        "                                                                        pos_class = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIaa-Iwodmg9",
        "colab_type": "text"
      },
      "source": [
        "#### 1) Define Hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcsCtMQYF5LE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "7f4feb5c-26a7-4810-9408-f232902d36a3"
      },
      "source": [
        "# Create the hyperparameter grid\n",
        "param_grid = {\"learning_rate\": [0.3],\n",
        "                \"n_estimators\": [60],\n",
        "                \"max_depth\": [3], \n",
        "                \"min_child_weight\": [4],       \n",
        "                \"reg_lambda\": [1, 1.2, 1.4]}\n",
        "xgb_grid_search = XGBClassifier(objective= 'binary:logistic', nthread=4)\n",
        "cv=3\n",
        "# Learning Curve for Slice \n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "# Define model\n",
        "f1 = make_scorer(f1_score)\n",
        "\n",
        "# Dummy Coding\n",
        "df_train_input_dummy = pd.get_dummies(df_application_train_input)\n",
        "\n",
        "grid_rf_class = sklearn.model_selection.GridSearchCV(estimator = xgb_grid_search,\n",
        "                                                     param_grid = param_grid, \n",
        "                                                     scoring= f1,\n",
        "                                                     cv = cv,\n",
        "                                                     refit = True,\n",
        "                                                     return_train_score = False)\n",
        "  \n",
        "# Fit model\n",
        "grid_rf_class.fit(df_train_input_dummy, df_application_train_label)\n",
        "\n",
        "print(grid_rf_class.best_params_)\n",
        "print(grid_rf_class.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'learning_rate': 0.3, 'max_depth': 3, 'min_child_weight': 4, 'n_estimators': 60, 'reg_lambda': 1}\n",
            "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
            "              learning_rate=0.3, max_delta_step=0, max_depth=3,\n",
            "              min_child_weight=4, missing=None, n_estimators=60, n_jobs=1,\n",
            "              nthread=4, objective='binary:logistic', random_state=0,\n",
            "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
            "              silent=None, subsample=1, verbosity=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxo5ErlDd1sE",
        "colab_type": "text"
      },
      "source": [
        "#### 2) Create datasets with diff. minority group sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFW3Y7yiQ_1X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "8ff187f8-0025-47d9-c04c-b80005b96bdf"
      },
      "source": [
        "training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Creating dfs for minority and majority group\n",
        "is_unpriv = df_application[\"CODE_GENDER\"].isin([\"F\"])\n",
        "is_priv = df_application[\"CODE_GENDER\"].isin([\"M\"])\n",
        "df_application_unpriv = df_application[is_unpriv]  # Minority\n",
        "df_application_priv = df_application[is_priv]  # Majority\n",
        "\n",
        "list_dfs_application = create_datasets(min_data = df_application_unpriv, maj_data = df_application_priv, training_sizes=training_sizes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45\n",
            "[105064, 105069, 105079, 105089, 105099, 105109, 105134, 105159, 105184, 105209, 105234, 105259, 105309, 105359, 105409, 105459, 105509, 105559, 105659, 105759, 105859, 105959, 106059, 106309, 106559, 106809, 107059, 107309, 107559, 107809, 108059, 108309, 108559, 108809, 109059, 109309, 109559, 109809, 110059, 111059, 112059, 113059, 114059, 115059, 120059]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogKyufJNeArd",
        "colab_type": "text"
      },
      "source": [
        "#### 3) Create dataframes with diff. metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77EkDXXQRG2S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "7dcb7132-bc3c-42a5-8b2a-22d960048142"
      },
      "source": [
        "list_dfs = list_dfs_application\n",
        "label = \"TARGET\"\n",
        "cv = 5 \n",
        "discr_feature = \"CODE_GENDER\"\n",
        "min_value = \"F\"\n",
        "maj_value = \"M\"\n",
        "application_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "                                  colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "                                  learning_rate=0.3, max_delta_step=0, max_depth=3,\n",
        "                                  min_child_weight=4, missing=None, n_estimators=60, n_jobs=1,\n",
        "                                  nthread=4, objective='binary:logistic', random_state=0,\n",
        "                                  reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "                                  silent=None, subsample=1, verbosity=1)\n",
        "\n",
        "\n",
        "results_df_application = metrics_to_df(list_dfs=list_dfs, label = label, model = application_model, \n",
        "                                  cv = cv, discr_feature = discr_feature, min_value = min_value,\n",
        "                                  maj_value = maj_value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:219: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in long_scalars\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epIrb_-JsWz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_df_application"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vpO_uoIW7Q1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save metrics csv\n",
        "results_df_application.to_csv(\"df_application_metrics.csv\") \n",
        "from google.colab import files\n",
        "files.download(\"df_application_metrics.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lsugkMieWV6",
        "colab_type": "text"
      },
      "source": [
        "#### 4) Create visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV6BW1hQRQXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# B: Learning Curve Function from scikit learn\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "# Define arguments\n",
        "f1 = make_scorer(f1_score) \n",
        "\n",
        "df_application_train_input = pd.get_dummies(df_application_train_input)\n",
        "\n",
        "# Plot actual learning curve\n",
        "plot_learning_curve(estimator = model, \n",
        "                    title = \"Adult Dataset - Random Forest Learning Curve\", \n",
        "                    X = df_application_train_input, y = df_application_train_label, \n",
        "                    cv = 5, \n",
        "                    scoring = f1, \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = np.linspace(.1, 1.0, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C2H-qopRWLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_metrics_line_chart(results_df_application)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp3ZnNGmRbK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maj_min_metrics_line_chart(results_df_application) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i07ZtFVpOwcv"
      },
      "source": [
        "# Generalizable Rule "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wse3LOyiH_2",
        "colab_type": "text"
      },
      "source": [
        "**Approach**:\n",
        "1. Rename columns of each metrics_df to a format in which the name of the original dataset is stored. \n",
        "2. Merge data frames with metrics (column bind) on minority row index via left join. Start with the dataframe with the most rows_minority and then merge step by step with the df with second most, etc.\n",
        "3. Determine slop change per row in comparison to previous row. \n",
        "4. Then plot only relevant fairness metrics. \n",
        "5. For each inflection point (slope change < threshold) for each dataset and metric of the chosen subset, plot a line that vertically touches then the x-axis (-> https://plotly.com/python/shapes/).\n",
        "6. Then, create corridor shape (dotted, medium alpha) that covers inflection point with the minimum number of training examples and the maximum and everything in between (-> https://plotly.com/python/shapes/ - *Highlighting Time Series Regions with Rectangle Shapes*).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSgVqfuZiHRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Rename columns of each results_df\n",
        "\n",
        "# results_df_adult\n",
        "results_df_adult.columns = [str(col) + '_adult' for col in results_df_adult.columns]\n",
        "# Rename rows_minority_adult column into standard format\n",
        "results_df_adult = results_df_adult.rename(index=str, \n",
        "                                           columns={\"rows_minority_adult\":\"rows_minority\"})\n",
        "\n",
        "# results_df_compas\n",
        "results_df_compas.columns = [str(col) + '_compas' for col in results_df_compas.columns]\n",
        "# Rename rows_minority_compas column into standard format\n",
        "results_df_adult = results_df_adult.rename(index=str, \n",
        "                                           columns={\"rows_minority_compas\":\"rows_minority\"})\n",
        "\n",
        "# results_df_homicide\n",
        "results_df_homicide.columns = [str(col) + '_homicide' for col in results_df_homicide.columns]\n",
        "# Rename rows_minority_homicide column into standard format\n",
        "results_df_adult = results_df_adult.rename(index=str, \n",
        "                                           columns={\"rows_minority_homicide\":\"rows_minority\"})\n",
        "\n",
        "# results_df_credit\n",
        "results_df_credit.columns = [str(col) + '_credit' for col in results_df_credit.columns]\n",
        "# Rename rows_minority_credit column into standard format\n",
        "results_df_adult = results_df_adult.rename(index=str, \n",
        "                                           columns={\"rows_minority_credit\":\"rows_minority\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpUm1gEXwMjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. Left join all result dfs into one\n",
        "merge_1 = pd.merge(data1, data2, \n",
        "                   how='left', on='X1')\n",
        "\n",
        "merge_2 = pd.merge(data1, data2, \n",
        "                   how='left', on='X1')\n",
        "\n",
        "complete_results_df = pd.merge(data1, data2, \n",
        "                               how='left', on='X1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz6vRFnW3ebk",
        "colab_type": "text"
      },
      "source": [
        "Determine slop change per row in comparison to previous row: \n",
        "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.diff.html\n",
        "\n",
        "\n",
        "**Ideas:**\n",
        "- Get diff for rows_minority and the metric of interest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sLcz2OqzLcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3. Determine slope change \n",
        "\n",
        "# Calculate slope (rise(y-axis change)/run(x-axis change))\n",
        "\n",
        "# Run\n",
        "complete_results_df[\"rows_minority_diff\"] = complete_results_df[\"rows_minority\"].diff()\n",
        "\n",
        "# Rise\n",
        "# Calculate slopes for aver_abs_odds_diff\n",
        "complete_results_df[\"aver_abs_odds_diff_adult_diff\"] = complete_results_df[\"aver_abs_odds_diff_adult\"].diff()\n",
        "complete_results_df[\"aver_abs_odds_diff_adult_slope\"] = complete_results_df[\"aver_abs_odds_diff_adult_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"aver_abs_odds_diff_compas_diff\"] = complete_results_df[\"aver_abs_odds_diff_compas\"].diff()\n",
        "complete_results_df[\"aver_abs_odds_diff_compas_slope\"] = complete_results_df[\"aver_abs_odds_diff_compas_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"aver_abs_odds_diff_homicide_diff\"] = complete_results_df[\"aver_abs_odds_diff_homicide\"].diff()\n",
        "complete_results_df[\"aver_abs_odds_diff_homicide_slope\"] = complete_results_df[\"aver_abs_odds_diff_homicide_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"aver_abs_odds_diff_credit_diff\"] = complete_results_df[\"aver_abs_odds_diff_credit\"].diff()\n",
        "complete_results_df[\"aver_abs_odds_diff_credit_slope\"] = complete_results_df[\"aver_abs_odds_diff_credit_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "\n",
        "# Calculate slopes for stat_parity_diff\n",
        "complete_results_df[\"stat_parity_diff_adult_diff\"] = complete_results_df[\"stat_parity_diff_adult\"].diff()\n",
        "complete_results_df[\"stat_parity_diff_adult_slope\"] = complete_results_df[\"stat_parity_diff_adult_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"stat_parity_diff_compas_diff\"] = complete_results_df[\"stat_parity_diff_compas\"].diff()\n",
        "complete_results_df[\"stat_parity_diff_compas_slope\"] = complete_results_df[\"stat_parity_diff_compas_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"stat_parity_diff_homicide_diff\"] = complete_results_df[\"stat_parity_diff_homicide\"].diff()\n",
        "complete_results_df[\"stat_parity_diff_homicide_slope\"] = complete_results_df[\"stat_parity_diff_homicide_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"stat_parity_diff_credit_diff\"] = complete_results_df[\"stat_parity_diff_credit\"].diff()\n",
        "complete_results_df[\"stat_parity_diff_credit_slope\"] = complete_results_df[\"stat_parity_diff_credit_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "\n",
        "# Calculate slopes for equal_opport_dist\n",
        "complete_results_df[\"equal_opport_dist_adult_diff\"] = complete_results_df[\"equal_opport_dist_adult\"].diff()\n",
        "complete_results_df[\"equal_opport_dist_adult_slope\"] = complete_results_df[\"equal_opport_dist_adult_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"equal_opport_dist_compas_diff\"] = complete_results_df[\"equal_opport_dist_compas\"].diff()\n",
        "complete_results_df[\"equal_opport_dist_compas_slope\"] = complete_results_df[\"equal_opport_dist_compas_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"equal_opport_dist_homicide_diff\"] = complete_results_df[\"equal_opport_dist_homicide\"].diff()\n",
        "complete_results_df[\"equal_opport_dist_homicide_slope\"] = complete_results_df[\"equal_opport_dist_homicide_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"equal_opport_dist_credit_diff\"] = complete_results_df[\"equal_opport_dist_credit\"].diff()\n",
        "complete_results_df[\"equal_opport_dist_credit_slope\"] = complete_results_df[\"equal_opport_dist_credit_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "\n",
        "# Calculate slopes for disparate_impact\n",
        "complete_results_df[\"disparate_impact_adult_diff\"] = complete_results_df[\"disparate_impact_adult\"].diff()\n",
        "complete_results_df[\"disparate_impact_adult_slope\"] = complete_results_df[\"disparate_impact_adult_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"disparate_impact_compas_diff\"] = complete_results_df[\"disparate_impact_compas\"].diff()\n",
        "complete_results_df[\"disparate_impact_compas_slope\"] = complete_results_df[\"disparate_impact_compas_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"disparate_impact_homicide_diff\"] = complete_results_df[\"disparate_impact_homicide\"].diff()\n",
        "complete_results_df[\"disparate_impact_homicide_slope\"] = complete_results_df[\"disparate_impact_homicide_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"disparate_impact_credit_diff\"] = complete_results_df[\"disparate_impact_credit\"].diff()\n",
        "complete_results_df[\"disparate_impact_credit_slope\"] = complete_results_df[\"disparate_impact_credit_diff\"] / complete_results_df[\"rows_minority_diff\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CB0JtaoKaJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "complete_results_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZDFjNB0FJTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4. Plot all fairness metrics \n",
        "\n",
        "  import plotly.graph_objects as go\n",
        "\n",
        "  # Create traces\n",
        "\n",
        "  # Performance Metrics\n",
        "  fig = go.Figure()\n",
        "  fig.add_trace(go.Scatter(x=complete_results_df[\"rows_complete\"], y=complete_results_df[\"f1_majority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='F1 Majority'))\n",
        "  fig.add_trace(go.Scatter(x=complete_results_df[\"rows_complete\"], y=complete_results_df[\"f1_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='F1 Minority'))\n",
        "  # TPR\n",
        "  fig.add_trace(go.Scatter(x=complete_results_df[\"rows_complete\"], y=complete_results_df[\"tpr_majority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='TPR/Recall Majority'))\n",
        "  fig.add_trace(go.Scatter(x=complete_results_df[\"rows_complete\"], y=complete_results_df[\"tpr_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='TPR/Recall Minority'))\n",
        "  # FPR\n",
        "  fig.add_trace(go.Scatter(x=complete_results_df[\"rows_complete\"], y=complete_results_df[\"fpr_majority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='FPR Majority'))\n",
        "  fig.add_trace(go.Scatter(x=complete_results_df[\"rows_complete\"], y=complete_results_df[\"fpr_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='FPR Minority'))\n",
        "\n",
        "  # Edit the layout\n",
        "  fig.update_layout(title={'text': title,\n",
        "                           'y':0.9,\n",
        "                           'x':0.5,\n",
        "                           # 'xanchor': 'center',\n",
        "                           'yanchor': 'top'},\n",
        "                    xaxis_title='Rows Complete',\n",
        "                    yaxis_title='Metric Score', \n",
        "                    font=dict(size=14))\n",
        "  \n",
        "  fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJOmyc6VFswT",
        "colab_type": "text"
      },
      "source": [
        "5. For each inflection point (slope change < threshold) for each dataset and metric of the chosen subset, plot a line that vertically touches then the x-axis (-> https://plotly.com/python/shapes/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0jr3FtCFfRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add shapes\n",
        "fig.add_shape(\n",
        "        # Line Vertical\n",
        "        dict(\n",
        "            type=\"line\",\n",
        "            x0=1,\n",
        "            y0=0,\n",
        "            x1=1,\n",
        "            y1=2,\n",
        "            line=dict(\n",
        "                color=\"RoyalBlue\",\n",
        "                width=3, \n",
        "                dash=\"dot\",\n",
        "                opacity=0.7\n",
        "            )\n",
        "\n",
        "fig.add_shape(\n",
        "        # Line reference to the axes\n",
        "            type=\"line\",\n",
        "            xref=\"x\",\n",
        "            yref=\"y\",\n",
        "            x0=4,\n",
        "            y0=0,\n",
        "            x1=8,\n",
        "            y1=1,\n",
        "            line=dict(\n",
        "                color=\"LightSeaGreen\",\n",
        "                width=3,\n",
        "            ),\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxH0zA8eGNB8",
        "colab_type": "text"
      },
      "source": [
        "6. Then, create corridor shape (dotted, medium alpha) that covers inflection point with the minimum number of training examples and the maximum and everything in between (-> https://plotly.com/python/shapes/ - *Highlighting Time Series Regions with Rectangle Shapes*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM-X6rYBGQZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "# Add shape regions\n",
        "fig.update_layout(\n",
        "    shapes=[\n",
        "        # 1st highlight during Feb 4 - Feb 6\n",
        "        dict(\n",
        "            type=\"rect\",\n",
        "            # x-reference is assigned to the x-values\n",
        "            xref=\"x\",\n",
        "            # y-reference is assigned to the plot paper [0,1]\n",
        "            yref=\"paper\",\n",
        "            x0=\"2015-02-04\", # First value of corridor\n",
        "            y0=0,\n",
        "            x1=\"2015-02-06\", # Second value of corridor \n",
        "            y1=1,\n",
        "            fillcolor=\"LightSalmon\",\n",
        "            opacity=0.5,\n",
        "            layer=\"below\",\n",
        "            line_width=0,\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC806v-lnXrg",
        "colab_type": "text"
      },
      "source": [
        "# Other Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8SAZPLhwuUD",
        "colab_type": "text"
      },
      "source": [
        "## Communities and Crime Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRiwaHW2XE43",
        "colab_type": "text"
      },
      "source": [
        "The Communities and Crime dataset gathers information from different communities in the United States related to several factors that can highly influence some common crimes such as robberies, murders or rapes. The data includes crime data obtained from the 1990 US LEMAS survey and the 1995 FBI Unified Crime Report. It also contains socio-economic data from the 1990 US Census."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v2ws3jO_65w",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9RypNWRj_D8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "37e0d43d-2e6d-4d41-95ec-458884c994c8"
      },
      "source": [
        "path_communities = \"/content/drive/My Drive/Master Thesis/Data/communities_and_crime_dataset.csv\"\n",
        "df_communities = pd.read_csv(path_communities, encoding = \"utf-8\")\n",
        "df_communities"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>communityname</th>\n",
              "      <th>state</th>\n",
              "      <th>countyCode</th>\n",
              "      <th>communityCode</th>\n",
              "      <th>fold</th>\n",
              "      <th>population</th>\n",
              "      <th>householdsize</th>\n",
              "      <th>racepctblack</th>\n",
              "      <th>racePctWhite</th>\n",
              "      <th>racePctAsian</th>\n",
              "      <th>racePctHisp</th>\n",
              "      <th>agePct12t21</th>\n",
              "      <th>agePct12t29</th>\n",
              "      <th>agePct16t24</th>\n",
              "      <th>agePct65up</th>\n",
              "      <th>numbUrban</th>\n",
              "      <th>pctUrban</th>\n",
              "      <th>medIncome</th>\n",
              "      <th>pctWWage</th>\n",
              "      <th>pctWFarmSelf</th>\n",
              "      <th>pctWInvInc</th>\n",
              "      <th>pctWSocSec</th>\n",
              "      <th>pctWPubAsst</th>\n",
              "      <th>pctWRetire</th>\n",
              "      <th>medFamInc</th>\n",
              "      <th>perCapInc</th>\n",
              "      <th>whitePerCap</th>\n",
              "      <th>blackPerCap</th>\n",
              "      <th>indianPerCap</th>\n",
              "      <th>AsianPerCap</th>\n",
              "      <th>OtherPerCap</th>\n",
              "      <th>HispPerCap</th>\n",
              "      <th>NumUnderPov</th>\n",
              "      <th>PctPopUnderPov</th>\n",
              "      <th>PctLess9thGrade</th>\n",
              "      <th>PctNotHSGrad</th>\n",
              "      <th>PctBSorMore</th>\n",
              "      <th>PctUnemployed</th>\n",
              "      <th>PctEmploy</th>\n",
              "      <th>PctEmplManu</th>\n",
              "      <th>PctEmplProfServ</th>\n",
              "      <th>PctOccupManu</th>\n",
              "      <th>PctOccupMgmtProf</th>\n",
              "      <th>MalePctDivorce</th>\n",
              "      <th>MalePctNevMarr</th>\n",
              "      <th>FemalePctDiv</th>\n",
              "      <th>TotalPctDiv</th>\n",
              "      <th>PersPerFam</th>\n",
              "      <th>PctFam2Par</th>\n",
              "      <th>PctKids2Par</th>\n",
              "      <th>PctYoungKids2Par</th>\n",
              "      <th>PctTeen2Par</th>\n",
              "      <th>PctWorkMomYoungKids</th>\n",
              "      <th>PctWorkMom</th>\n",
              "      <th>NumKidsBornNeverMar</th>\n",
              "      <th>PctKidsBornNeverMar</th>\n",
              "      <th>NumImmig</th>\n",
              "      <th>PctImmigRecent</th>\n",
              "      <th>PctImmigRec5</th>\n",
              "      <th>PctImmigRec8</th>\n",
              "      <th>PctImmigRec10</th>\n",
              "      <th>PctRecentImmig</th>\n",
              "      <th>PctRecImmig5</th>\n",
              "      <th>PctRecImmig8</th>\n",
              "      <th>PctRecImmig10</th>\n",
              "      <th>PctSpeakEnglOnly</th>\n",
              "      <th>PctNotSpeakEnglWell</th>\n",
              "      <th>PctLargHouseFam</th>\n",
              "      <th>PctLargHouseOccup</th>\n",
              "      <th>PersPerOccupHous</th>\n",
              "      <th>PersPerOwnOccHous</th>\n",
              "      <th>PersPerRentOccHous</th>\n",
              "      <th>PctPersOwnOccup</th>\n",
              "      <th>PctPersDenseHous</th>\n",
              "      <th>PctHousLess3BR</th>\n",
              "      <th>MedNumBR</th>\n",
              "      <th>HousVacant</th>\n",
              "      <th>PctHousOccup</th>\n",
              "      <th>PctHousOwnOcc</th>\n",
              "      <th>PctVacantBoarded</th>\n",
              "      <th>PctVacMore6Mos</th>\n",
              "      <th>MedYrHousBuilt</th>\n",
              "      <th>PctHousNoPhone</th>\n",
              "      <th>PctWOFullPlumb</th>\n",
              "      <th>OwnOccLowQuart</th>\n",
              "      <th>OwnOccMedVal</th>\n",
              "      <th>OwnOccHiQuart</th>\n",
              "      <th>OwnOccQrange</th>\n",
              "      <th>RentLowQ</th>\n",
              "      <th>RentMedian</th>\n",
              "      <th>RentHighQ</th>\n",
              "      <th>RentQrange</th>\n",
              "      <th>MedRent</th>\n",
              "      <th>MedRentPctHousInc</th>\n",
              "      <th>MedOwnCostPctInc</th>\n",
              "      <th>MedOwnCostPctIncNoMtg</th>\n",
              "      <th>NumInShelters</th>\n",
              "      <th>NumStreet</th>\n",
              "      <th>PctForeignBorn</th>\n",
              "      <th>PctBornSameState</th>\n",
              "      <th>PctSameHouse85</th>\n",
              "      <th>PctSameCity85</th>\n",
              "      <th>PctSameState85</th>\n",
              "      <th>LemasSwornFT</th>\n",
              "      <th>LemasSwFTPerPop</th>\n",
              "      <th>LemasSwFTFieldOps</th>\n",
              "      <th>LemasSwFTFieldPerPop</th>\n",
              "      <th>LemasTotalReq</th>\n",
              "      <th>LemasTotReqPerPop</th>\n",
              "      <th>PolicReqPerOffic</th>\n",
              "      <th>PolicPerPop</th>\n",
              "      <th>RacialMatchCommPol</th>\n",
              "      <th>PctPolicWhite</th>\n",
              "      <th>PctPolicBlack</th>\n",
              "      <th>PctPolicHisp</th>\n",
              "      <th>PctPolicAsian</th>\n",
              "      <th>PctPolicMinor</th>\n",
              "      <th>OfficAssgnDrugUnits</th>\n",
              "      <th>NumKindsDrugsSeiz</th>\n",
              "      <th>PolicAveOTWorked</th>\n",
              "      <th>LandArea</th>\n",
              "      <th>PopDens</th>\n",
              "      <th>PctUsePubTrans</th>\n",
              "      <th>PolicCars</th>\n",
              "      <th>PolicOperBudg</th>\n",
              "      <th>LemasPctPolicOnPatr</th>\n",
              "      <th>LemasGangUnitDeploy</th>\n",
              "      <th>LemasPctOfficDrugUn</th>\n",
              "      <th>PolicBudgPerPop</th>\n",
              "      <th>murders</th>\n",
              "      <th>murdPerPop</th>\n",
              "      <th>rapes</th>\n",
              "      <th>rapesPerPop</th>\n",
              "      <th>robberies</th>\n",
              "      <th>robbbPerPop</th>\n",
              "      <th>assaults</th>\n",
              "      <th>assaultPerPop</th>\n",
              "      <th>burglaries</th>\n",
              "      <th>burglPerPop</th>\n",
              "      <th>larcenies</th>\n",
              "      <th>larcPerPop</th>\n",
              "      <th>autoTheft</th>\n",
              "      <th>autoTheftPerPop</th>\n",
              "      <th>arsons</th>\n",
              "      <th>arsonsPerPop</th>\n",
              "      <th>ViolentCrimesPerPop</th>\n",
              "      <th>nonViolPerPop</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BerkeleyHeightstownship</td>\n",
              "      <td>NJ</td>\n",
              "      <td>39</td>\n",
              "      <td>5320</td>\n",
              "      <td>1</td>\n",
              "      <td>11980</td>\n",
              "      <td>3.10</td>\n",
              "      <td>1.37</td>\n",
              "      <td>91.78</td>\n",
              "      <td>6.50</td>\n",
              "      <td>1.88</td>\n",
              "      <td>12.47</td>\n",
              "      <td>21.44</td>\n",
              "      <td>10.93</td>\n",
              "      <td>11.33</td>\n",
              "      <td>11980</td>\n",
              "      <td>100.00</td>\n",
              "      <td>75122</td>\n",
              "      <td>89.24</td>\n",
              "      <td>1.55</td>\n",
              "      <td>70.20</td>\n",
              "      <td>23.62</td>\n",
              "      <td>1.03</td>\n",
              "      <td>18.39</td>\n",
              "      <td>79584</td>\n",
              "      <td>29711</td>\n",
              "      <td>30233</td>\n",
              "      <td>13600</td>\n",
              "      <td>5725</td>\n",
              "      <td>27101</td>\n",
              "      <td>5115</td>\n",
              "      <td>22838</td>\n",
              "      <td>227</td>\n",
              "      <td>1.96</td>\n",
              "      <td>5.81</td>\n",
              "      <td>9.90</td>\n",
              "      <td>48.18</td>\n",
              "      <td>2.70</td>\n",
              "      <td>64.55</td>\n",
              "      <td>14.65</td>\n",
              "      <td>28.82</td>\n",
              "      <td>5.49</td>\n",
              "      <td>50.73</td>\n",
              "      <td>3.67</td>\n",
              "      <td>26.38</td>\n",
              "      <td>5.22</td>\n",
              "      <td>4.47</td>\n",
              "      <td>3.22</td>\n",
              "      <td>91.43</td>\n",
              "      <td>90.17</td>\n",
              "      <td>95.78</td>\n",
              "      <td>95.81</td>\n",
              "      <td>44.56</td>\n",
              "      <td>58.88</td>\n",
              "      <td>31</td>\n",
              "      <td>0.36</td>\n",
              "      <td>1277</td>\n",
              "      <td>8.69</td>\n",
              "      <td>13.00</td>\n",
              "      <td>20.99</td>\n",
              "      <td>30.93</td>\n",
              "      <td>0.93</td>\n",
              "      <td>1.39</td>\n",
              "      <td>2.24</td>\n",
              "      <td>3.30</td>\n",
              "      <td>85.68</td>\n",
              "      <td>1.37</td>\n",
              "      <td>4.81</td>\n",
              "      <td>4.17</td>\n",
              "      <td>2.99</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.84</td>\n",
              "      <td>91.46</td>\n",
              "      <td>0.39</td>\n",
              "      <td>11.06</td>\n",
              "      <td>3</td>\n",
              "      <td>64</td>\n",
              "      <td>98.37</td>\n",
              "      <td>91.01</td>\n",
              "      <td>3.12</td>\n",
              "      <td>37.50</td>\n",
              "      <td>1959</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.28</td>\n",
              "      <td>215900</td>\n",
              "      <td>262600</td>\n",
              "      <td>326900</td>\n",
              "      <td>111000</td>\n",
              "      <td>685</td>\n",
              "      <td>1001</td>\n",
              "      <td>1001</td>\n",
              "      <td>316</td>\n",
              "      <td>1001</td>\n",
              "      <td>23.8</td>\n",
              "      <td>21.1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>10.66</td>\n",
              "      <td>53.72</td>\n",
              "      <td>65.29</td>\n",
              "      <td>78.09</td>\n",
              "      <td>89.14</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>6.5</td>\n",
              "      <td>1845.9</td>\n",
              "      <td>9.63</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8.2</td>\n",
              "      <td>4</td>\n",
              "      <td>32.81</td>\n",
              "      <td>14</td>\n",
              "      <td>114.85</td>\n",
              "      <td>138</td>\n",
              "      <td>1132.08</td>\n",
              "      <td>16</td>\n",
              "      <td>131.26</td>\n",
              "      <td>2</td>\n",
              "      <td>16.41</td>\n",
              "      <td>41.02</td>\n",
              "      <td>1394.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Marpletownship</td>\n",
              "      <td>PA</td>\n",
              "      <td>45</td>\n",
              "      <td>47616</td>\n",
              "      <td>1</td>\n",
              "      <td>23123</td>\n",
              "      <td>2.82</td>\n",
              "      <td>0.80</td>\n",
              "      <td>95.57</td>\n",
              "      <td>3.44</td>\n",
              "      <td>0.85</td>\n",
              "      <td>11.01</td>\n",
              "      <td>21.30</td>\n",
              "      <td>10.48</td>\n",
              "      <td>17.18</td>\n",
              "      <td>23123</td>\n",
              "      <td>100.00</td>\n",
              "      <td>47917</td>\n",
              "      <td>78.99</td>\n",
              "      <td>1.11</td>\n",
              "      <td>64.11</td>\n",
              "      <td>35.50</td>\n",
              "      <td>2.75</td>\n",
              "      <td>22.85</td>\n",
              "      <td>55323</td>\n",
              "      <td>20148</td>\n",
              "      <td>20191</td>\n",
              "      <td>18137</td>\n",
              "      <td>0</td>\n",
              "      <td>20074</td>\n",
              "      <td>5250</td>\n",
              "      <td>12222</td>\n",
              "      <td>885</td>\n",
              "      <td>3.98</td>\n",
              "      <td>5.61</td>\n",
              "      <td>13.72</td>\n",
              "      <td>29.89</td>\n",
              "      <td>2.43</td>\n",
              "      <td>61.96</td>\n",
              "      <td>12.26</td>\n",
              "      <td>29.28</td>\n",
              "      <td>6.39</td>\n",
              "      <td>37.64</td>\n",
              "      <td>4.23</td>\n",
              "      <td>27.99</td>\n",
              "      <td>6.45</td>\n",
              "      <td>5.42</td>\n",
              "      <td>3.11</td>\n",
              "      <td>86.91</td>\n",
              "      <td>85.33</td>\n",
              "      <td>96.82</td>\n",
              "      <td>86.46</td>\n",
              "      <td>51.14</td>\n",
              "      <td>62.43</td>\n",
              "      <td>43</td>\n",
              "      <td>0.24</td>\n",
              "      <td>1920</td>\n",
              "      <td>5.21</td>\n",
              "      <td>8.65</td>\n",
              "      <td>13.33</td>\n",
              "      <td>22.50</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.72</td>\n",
              "      <td>1.11</td>\n",
              "      <td>1.87</td>\n",
              "      <td>87.79</td>\n",
              "      <td>1.81</td>\n",
              "      <td>4.25</td>\n",
              "      <td>3.34</td>\n",
              "      <td>2.70</td>\n",
              "      <td>2.83</td>\n",
              "      <td>1.96</td>\n",
              "      <td>89.03</td>\n",
              "      <td>1.01</td>\n",
              "      <td>23.60</td>\n",
              "      <td>3</td>\n",
              "      <td>240</td>\n",
              "      <td>97.15</td>\n",
              "      <td>84.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>18.33</td>\n",
              "      <td>1958</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.14</td>\n",
              "      <td>136300</td>\n",
              "      <td>164200</td>\n",
              "      <td>199900</td>\n",
              "      <td>63600</td>\n",
              "      <td>467</td>\n",
              "      <td>560</td>\n",
              "      <td>672</td>\n",
              "      <td>205</td>\n",
              "      <td>627</td>\n",
              "      <td>27.6</td>\n",
              "      <td>20.7</td>\n",
              "      <td>12.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.30</td>\n",
              "      <td>77.17</td>\n",
              "      <td>71.27</td>\n",
              "      <td>90.22</td>\n",
              "      <td>96.12</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>10.6</td>\n",
              "      <td>2186.7</td>\n",
              "      <td>3.84</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>4.25</td>\n",
              "      <td>5</td>\n",
              "      <td>21.26</td>\n",
              "      <td>24</td>\n",
              "      <td>102.05</td>\n",
              "      <td>57</td>\n",
              "      <td>242.37</td>\n",
              "      <td>376</td>\n",
              "      <td>1598.78</td>\n",
              "      <td>26</td>\n",
              "      <td>110.55</td>\n",
              "      <td>1</td>\n",
              "      <td>4.25</td>\n",
              "      <td>127.56</td>\n",
              "      <td>1955.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Tigardcity</td>\n",
              "      <td>OR</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>1</td>\n",
              "      <td>29344</td>\n",
              "      <td>2.43</td>\n",
              "      <td>0.74</td>\n",
              "      <td>94.33</td>\n",
              "      <td>3.43</td>\n",
              "      <td>2.35</td>\n",
              "      <td>11.36</td>\n",
              "      <td>25.88</td>\n",
              "      <td>11.01</td>\n",
              "      <td>10.28</td>\n",
              "      <td>29344</td>\n",
              "      <td>100.00</td>\n",
              "      <td>35669</td>\n",
              "      <td>82.00</td>\n",
              "      <td>1.15</td>\n",
              "      <td>55.73</td>\n",
              "      <td>22.25</td>\n",
              "      <td>2.94</td>\n",
              "      <td>14.56</td>\n",
              "      <td>42112</td>\n",
              "      <td>16946</td>\n",
              "      <td>17103</td>\n",
              "      <td>16644</td>\n",
              "      <td>21606</td>\n",
              "      <td>15528</td>\n",
              "      <td>5954</td>\n",
              "      <td>8405</td>\n",
              "      <td>1389</td>\n",
              "      <td>4.75</td>\n",
              "      <td>2.80</td>\n",
              "      <td>9.09</td>\n",
              "      <td>30.13</td>\n",
              "      <td>4.01</td>\n",
              "      <td>69.80</td>\n",
              "      <td>15.95</td>\n",
              "      <td>21.52</td>\n",
              "      <td>8.79</td>\n",
              "      <td>32.48</td>\n",
              "      <td>10.10</td>\n",
              "      <td>25.78</td>\n",
              "      <td>14.76</td>\n",
              "      <td>12.55</td>\n",
              "      <td>2.95</td>\n",
              "      <td>78.54</td>\n",
              "      <td>78.85</td>\n",
              "      <td>92.37</td>\n",
              "      <td>75.72</td>\n",
              "      <td>66.08</td>\n",
              "      <td>74.19</td>\n",
              "      <td>164</td>\n",
              "      <td>0.88</td>\n",
              "      <td>1468</td>\n",
              "      <td>16.42</td>\n",
              "      <td>23.98</td>\n",
              "      <td>32.08</td>\n",
              "      <td>35.63</td>\n",
              "      <td>0.82</td>\n",
              "      <td>1.20</td>\n",
              "      <td>1.61</td>\n",
              "      <td>1.78</td>\n",
              "      <td>93.11</td>\n",
              "      <td>1.14</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.05</td>\n",
              "      <td>2.42</td>\n",
              "      <td>2.69</td>\n",
              "      <td>2.06</td>\n",
              "      <td>64.18</td>\n",
              "      <td>2.03</td>\n",
              "      <td>47.46</td>\n",
              "      <td>3</td>\n",
              "      <td>544</td>\n",
              "      <td>95.68</td>\n",
              "      <td>57.79</td>\n",
              "      <td>0.92</td>\n",
              "      <td>7.54</td>\n",
              "      <td>1976</td>\n",
              "      <td>1.55</td>\n",
              "      <td>0.12</td>\n",
              "      <td>74700</td>\n",
              "      <td>90400</td>\n",
              "      <td>112000</td>\n",
              "      <td>37300</td>\n",
              "      <td>370</td>\n",
              "      <td>428</td>\n",
              "      <td>520</td>\n",
              "      <td>150</td>\n",
              "      <td>484</td>\n",
              "      <td>24.1</td>\n",
              "      <td>21.7</td>\n",
              "      <td>11.6</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>5.00</td>\n",
              "      <td>44.77</td>\n",
              "      <td>36.60</td>\n",
              "      <td>61.26</td>\n",
              "      <td>82.85</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>10.6</td>\n",
              "      <td>2780.9</td>\n",
              "      <td>4.37</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>3</td>\n",
              "      <td>8.30</td>\n",
              "      <td>6</td>\n",
              "      <td>16.6</td>\n",
              "      <td>56</td>\n",
              "      <td>154.95</td>\n",
              "      <td>14</td>\n",
              "      <td>38.74</td>\n",
              "      <td>274</td>\n",
              "      <td>758.14</td>\n",
              "      <td>1797</td>\n",
              "      <td>4972.19</td>\n",
              "      <td>136</td>\n",
              "      <td>376.3</td>\n",
              "      <td>22</td>\n",
              "      <td>60.87</td>\n",
              "      <td>218.59</td>\n",
              "      <td>6167.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Gloversvillecity</td>\n",
              "      <td>NY</td>\n",
              "      <td>35</td>\n",
              "      <td>29443</td>\n",
              "      <td>1</td>\n",
              "      <td>16656</td>\n",
              "      <td>2.40</td>\n",
              "      <td>1.70</td>\n",
              "      <td>97.35</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.70</td>\n",
              "      <td>12.55</td>\n",
              "      <td>25.20</td>\n",
              "      <td>12.19</td>\n",
              "      <td>17.57</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>20580</td>\n",
              "      <td>68.15</td>\n",
              "      <td>0.24</td>\n",
              "      <td>38.95</td>\n",
              "      <td>39.48</td>\n",
              "      <td>11.71</td>\n",
              "      <td>18.33</td>\n",
              "      <td>26501</td>\n",
              "      <td>10810</td>\n",
              "      <td>10909</td>\n",
              "      <td>9984</td>\n",
              "      <td>4941</td>\n",
              "      <td>3541</td>\n",
              "      <td>2451</td>\n",
              "      <td>4391</td>\n",
              "      <td>2831</td>\n",
              "      <td>17.23</td>\n",
              "      <td>11.05</td>\n",
              "      <td>33.68</td>\n",
              "      <td>10.81</td>\n",
              "      <td>9.86</td>\n",
              "      <td>54.74</td>\n",
              "      <td>31.22</td>\n",
              "      <td>27.43</td>\n",
              "      <td>26.76</td>\n",
              "      <td>22.71</td>\n",
              "      <td>10.98</td>\n",
              "      <td>28.15</td>\n",
              "      <td>14.47</td>\n",
              "      <td>12.91</td>\n",
              "      <td>2.98</td>\n",
              "      <td>64.02</td>\n",
              "      <td>62.36</td>\n",
              "      <td>65.38</td>\n",
              "      <td>67.43</td>\n",
              "      <td>59.59</td>\n",
              "      <td>70.27</td>\n",
              "      <td>561</td>\n",
              "      <td>3.84</td>\n",
              "      <td>339</td>\n",
              "      <td>13.86</td>\n",
              "      <td>13.86</td>\n",
              "      <td>15.34</td>\n",
              "      <td>15.34</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>94.98</td>\n",
              "      <td>0.56</td>\n",
              "      <td>3.93</td>\n",
              "      <td>2.56</td>\n",
              "      <td>2.37</td>\n",
              "      <td>2.51</td>\n",
              "      <td>2.20</td>\n",
              "      <td>58.18</td>\n",
              "      <td>1.21</td>\n",
              "      <td>45.66</td>\n",
              "      <td>3</td>\n",
              "      <td>669</td>\n",
              "      <td>91.19</td>\n",
              "      <td>54.89</td>\n",
              "      <td>2.54</td>\n",
              "      <td>57.85</td>\n",
              "      <td>1939</td>\n",
              "      <td>7.00</td>\n",
              "      <td>0.87</td>\n",
              "      <td>36400</td>\n",
              "      <td>49600</td>\n",
              "      <td>66500</td>\n",
              "      <td>30100</td>\n",
              "      <td>195</td>\n",
              "      <td>250</td>\n",
              "      <td>309</td>\n",
              "      <td>114</td>\n",
              "      <td>333</td>\n",
              "      <td>28.7</td>\n",
              "      <td>20.6</td>\n",
              "      <td>14.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.04</td>\n",
              "      <td>88.71</td>\n",
              "      <td>56.70</td>\n",
              "      <td>90.17</td>\n",
              "      <td>96.24</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>5.2</td>\n",
              "      <td>3217.7</td>\n",
              "      <td>3.31</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>10</td>\n",
              "      <td>57.86</td>\n",
              "      <td>10</td>\n",
              "      <td>57.86</td>\n",
              "      <td>33</td>\n",
              "      <td>190.93</td>\n",
              "      <td>225</td>\n",
              "      <td>1301.78</td>\n",
              "      <td>716</td>\n",
              "      <td>4142.56</td>\n",
              "      <td>47</td>\n",
              "      <td>271.93</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>306.64</td>\n",
              "      <td>?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bemidjicity</td>\n",
              "      <td>MN</td>\n",
              "      <td>7</td>\n",
              "      <td>5068</td>\n",
              "      <td>1</td>\n",
              "      <td>11245</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.53</td>\n",
              "      <td>89.16</td>\n",
              "      <td>1.17</td>\n",
              "      <td>0.52</td>\n",
              "      <td>24.46</td>\n",
              "      <td>40.53</td>\n",
              "      <td>28.69</td>\n",
              "      <td>12.65</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>17390</td>\n",
              "      <td>69.33</td>\n",
              "      <td>0.55</td>\n",
              "      <td>42.82</td>\n",
              "      <td>32.16</td>\n",
              "      <td>11.21</td>\n",
              "      <td>14.43</td>\n",
              "      <td>24018</td>\n",
              "      <td>8483</td>\n",
              "      <td>9009</td>\n",
              "      <td>887</td>\n",
              "      <td>4425</td>\n",
              "      <td>3352</td>\n",
              "      <td>3000</td>\n",
              "      <td>1328</td>\n",
              "      <td>2855</td>\n",
              "      <td>29.99</td>\n",
              "      <td>12.15</td>\n",
              "      <td>23.06</td>\n",
              "      <td>25.28</td>\n",
              "      <td>9.08</td>\n",
              "      <td>52.44</td>\n",
              "      <td>6.89</td>\n",
              "      <td>36.54</td>\n",
              "      <td>10.94</td>\n",
              "      <td>27.80</td>\n",
              "      <td>7.51</td>\n",
              "      <td>50.66</td>\n",
              "      <td>11.64</td>\n",
              "      <td>9.73</td>\n",
              "      <td>2.98</td>\n",
              "      <td>58.59</td>\n",
              "      <td>55.20</td>\n",
              "      <td>66.51</td>\n",
              "      <td>79.17</td>\n",
              "      <td>61.22</td>\n",
              "      <td>68.94</td>\n",
              "      <td>402</td>\n",
              "      <td>4.70</td>\n",
              "      <td>196</td>\n",
              "      <td>46.94</td>\n",
              "      <td>56.12</td>\n",
              "      <td>67.86</td>\n",
              "      <td>69.90</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.98</td>\n",
              "      <td>1.18</td>\n",
              "      <td>1.22</td>\n",
              "      <td>94.64</td>\n",
              "      <td>0.39</td>\n",
              "      <td>5.23</td>\n",
              "      <td>3.11</td>\n",
              "      <td>2.35</td>\n",
              "      <td>2.55</td>\n",
              "      <td>2.12</td>\n",
              "      <td>58.13</td>\n",
              "      <td>2.94</td>\n",
              "      <td>55.64</td>\n",
              "      <td>2</td>\n",
              "      <td>333</td>\n",
              "      <td>92.45</td>\n",
              "      <td>53.57</td>\n",
              "      <td>3.90</td>\n",
              "      <td>42.64</td>\n",
              "      <td>1958</td>\n",
              "      <td>7.45</td>\n",
              "      <td>0.82</td>\n",
              "      <td>30600</td>\n",
              "      <td>43200</td>\n",
              "      <td>59500</td>\n",
              "      <td>28900</td>\n",
              "      <td>202</td>\n",
              "      <td>283</td>\n",
              "      <td>362</td>\n",
              "      <td>160</td>\n",
              "      <td>332</td>\n",
              "      <td>32.2</td>\n",
              "      <td>23.2</td>\n",
              "      <td>12.9</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.74</td>\n",
              "      <td>73.75</td>\n",
              "      <td>42.22</td>\n",
              "      <td>60.34</td>\n",
              "      <td>89.02</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>11.5</td>\n",
              "      <td>974.2</td>\n",
              "      <td>0.38</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>4</td>\n",
              "      <td>32.04</td>\n",
              "      <td>14</td>\n",
              "      <td>112.14</td>\n",
              "      <td>91</td>\n",
              "      <td>728.93</td>\n",
              "      <td>1060</td>\n",
              "      <td>8490.87</td>\n",
              "      <td>91</td>\n",
              "      <td>728.93</td>\n",
              "      <td>5</td>\n",
              "      <td>40.05</td>\n",
              "      <td>?</td>\n",
              "      <td>9988.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2210</th>\n",
              "      <td>Mercedcity</td>\n",
              "      <td>CA</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>10</td>\n",
              "      <td>56216</td>\n",
              "      <td>3.07</td>\n",
              "      <td>6.87</td>\n",
              "      <td>61.68</td>\n",
              "      <td>15.23</td>\n",
              "      <td>29.86</td>\n",
              "      <td>15.46</td>\n",
              "      <td>30.16</td>\n",
              "      <td>14.34</td>\n",
              "      <td>8.08</td>\n",
              "      <td>56216</td>\n",
              "      <td>100.00</td>\n",
              "      <td>24727</td>\n",
              "      <td>75.05</td>\n",
              "      <td>1.12</td>\n",
              "      <td>31.42</td>\n",
              "      <td>21.45</td>\n",
              "      <td>19.98</td>\n",
              "      <td>14.41</td>\n",
              "      <td>27388</td>\n",
              "      <td>10237</td>\n",
              "      <td>13041</td>\n",
              "      <td>8344</td>\n",
              "      <td>8590</td>\n",
              "      <td>3399</td>\n",
              "      <td>6470</td>\n",
              "      <td>6644</td>\n",
              "      <td>13804</td>\n",
              "      <td>25.06</td>\n",
              "      <td>17.12</td>\n",
              "      <td>30.87</td>\n",
              "      <td>15.79</td>\n",
              "      <td>9.99</td>\n",
              "      <td>55.53</td>\n",
              "      <td>13.47</td>\n",
              "      <td>27.18</td>\n",
              "      <td>16.38</td>\n",
              "      <td>25.02</td>\n",
              "      <td>10.22</td>\n",
              "      <td>31.91</td>\n",
              "      <td>16.28</td>\n",
              "      <td>13.34</td>\n",
              "      <td>3.56</td>\n",
              "      <td>67.04</td>\n",
              "      <td>64.81</td>\n",
              "      <td>76.19</td>\n",
              "      <td>72.78</td>\n",
              "      <td>47.24</td>\n",
              "      <td>55.38</td>\n",
              "      <td>1960</td>\n",
              "      <td>4.49</td>\n",
              "      <td>10623</td>\n",
              "      <td>22.97</td>\n",
              "      <td>35.12</td>\n",
              "      <td>42.10</td>\n",
              "      <td>60.31</td>\n",
              "      <td>4.34</td>\n",
              "      <td>6.64</td>\n",
              "      <td>7.96</td>\n",
              "      <td>11.40</td>\n",
              "      <td>65.33</td>\n",
              "      <td>11.87</td>\n",
              "      <td>13.49</td>\n",
              "      <td>9.91</td>\n",
              "      <td>3.03</td>\n",
              "      <td>2.83</td>\n",
              "      <td>3.19</td>\n",
              "      <td>41.69</td>\n",
              "      <td>16.89</td>\n",
              "      <td>57.23</td>\n",
              "      <td>2</td>\n",
              "      <td>683</td>\n",
              "      <td>96.40</td>\n",
              "      <td>44.63</td>\n",
              "      <td>1.46</td>\n",
              "      <td>13.18</td>\n",
              "      <td>1973</td>\n",
              "      <td>4.91</td>\n",
              "      <td>0.55</td>\n",
              "      <td>71200</td>\n",
              "      <td>91100</td>\n",
              "      <td>118900</td>\n",
              "      <td>47700</td>\n",
              "      <td>298</td>\n",
              "      <td>374</td>\n",
              "      <td>455</td>\n",
              "      <td>157</td>\n",
              "      <td>438</td>\n",
              "      <td>29.8</td>\n",
              "      <td>22.6</td>\n",
              "      <td>11.7</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>18.90</td>\n",
              "      <td>52.67</td>\n",
              "      <td>39.19</td>\n",
              "      <td>74.58</td>\n",
              "      <td>85.88</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>16.7</td>\n",
              "      <td>3365.4</td>\n",
              "      <td>0.59</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>10</td>\n",
              "      <td>16.49</td>\n",
              "      <td>30</td>\n",
              "      <td>49.46</td>\n",
              "      <td>121</td>\n",
              "      <td>199.5</td>\n",
              "      <td>170</td>\n",
              "      <td>280.29</td>\n",
              "      <td>1376</td>\n",
              "      <td>2268.72</td>\n",
              "      <td>2563</td>\n",
              "      <td>4225.82</td>\n",
              "      <td>489</td>\n",
              "      <td>806.25</td>\n",
              "      <td>34</td>\n",
              "      <td>56.06</td>\n",
              "      <td>545.75</td>\n",
              "      <td>7356.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2211</th>\n",
              "      <td>Pinevillecity</td>\n",
              "      <td>LA</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>10</td>\n",
              "      <td>12251</td>\n",
              "      <td>2.68</td>\n",
              "      <td>21.18</td>\n",
              "      <td>76.65</td>\n",
              "      <td>1.52</td>\n",
              "      <td>1.29</td>\n",
              "      <td>17.36</td>\n",
              "      <td>31.23</td>\n",
              "      <td>16.97</td>\n",
              "      <td>12.57</td>\n",
              "      <td>12251</td>\n",
              "      <td>100.00</td>\n",
              "      <td>20321</td>\n",
              "      <td>75.06</td>\n",
              "      <td>0.47</td>\n",
              "      <td>33.25</td>\n",
              "      <td>27.63</td>\n",
              "      <td>8.85</td>\n",
              "      <td>18.23</td>\n",
              "      <td>25000</td>\n",
              "      <td>9995</td>\n",
              "      <td>11353</td>\n",
              "      <td>5768</td>\n",
              "      <td>10910</td>\n",
              "      <td>1718</td>\n",
              "      <td>11471</td>\n",
              "      <td>4883</td>\n",
              "      <td>2364</td>\n",
              "      <td>20.79</td>\n",
              "      <td>12.51</td>\n",
              "      <td>27.71</td>\n",
              "      <td>19.28</td>\n",
              "      <td>7.90</td>\n",
              "      <td>54.64</td>\n",
              "      <td>7.81</td>\n",
              "      <td>37.06</td>\n",
              "      <td>10.37</td>\n",
              "      <td>28.73</td>\n",
              "      <td>10.86</td>\n",
              "      <td>29.51</td>\n",
              "      <td>16.12</td>\n",
              "      <td>13.77</td>\n",
              "      <td>3.12</td>\n",
              "      <td>68.57</td>\n",
              "      <td>63.66</td>\n",
              "      <td>80.29</td>\n",
              "      <td>73.68</td>\n",
              "      <td>64.20</td>\n",
              "      <td>66.67</td>\n",
              "      <td>277</td>\n",
              "      <td>2.98</td>\n",
              "      <td>275</td>\n",
              "      <td>2.91</td>\n",
              "      <td>45.09</td>\n",
              "      <td>65.45</td>\n",
              "      <td>75.64</td>\n",
              "      <td>0.07</td>\n",
              "      <td>1.01</td>\n",
              "      <td>1.47</td>\n",
              "      <td>1.70</td>\n",
              "      <td>92.78</td>\n",
              "      <td>0.86</td>\n",
              "      <td>5.03</td>\n",
              "      <td>3.37</td>\n",
              "      <td>2.49</td>\n",
              "      <td>2.58</td>\n",
              "      <td>2.39</td>\n",
              "      <td>56.06</td>\n",
              "      <td>3.99</td>\n",
              "      <td>54.48</td>\n",
              "      <td>2</td>\n",
              "      <td>523</td>\n",
              "      <td>89.72</td>\n",
              "      <td>54.24</td>\n",
              "      <td>4.59</td>\n",
              "      <td>46.08</td>\n",
              "      <td>1966</td>\n",
              "      <td>7.56</td>\n",
              "      <td>0.12</td>\n",
              "      <td>33600</td>\n",
              "      <td>52000</td>\n",
              "      <td>72700</td>\n",
              "      <td>39100</td>\n",
              "      <td>176</td>\n",
              "      <td>248</td>\n",
              "      <td>297</td>\n",
              "      <td>121</td>\n",
              "      <td>330</td>\n",
              "      <td>23.8</td>\n",
              "      <td>17.3</td>\n",
              "      <td>14.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.24</td>\n",
              "      <td>75.16</td>\n",
              "      <td>49.12</td>\n",
              "      <td>78.79</td>\n",
              "      <td>92.85</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>7.3</td>\n",
              "      <td>1682.8</td>\n",
              "      <td>1.15</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4</td>\n",
              "      <td>33.09</td>\n",
              "      <td>1</td>\n",
              "      <td>8.27</td>\n",
              "      <td>10</td>\n",
              "      <td>82.73</td>\n",
              "      <td>104</td>\n",
              "      <td>860.43</td>\n",
              "      <td>574</td>\n",
              "      <td>4748.9</td>\n",
              "      <td>24</td>\n",
              "      <td>198.56</td>\n",
              "      <td>2</td>\n",
              "      <td>16.55</td>\n",
              "      <td>124.1</td>\n",
              "      <td>5824.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2212</th>\n",
              "      <td>Yucaipacity</td>\n",
              "      <td>CA</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>10</td>\n",
              "      <td>32824</td>\n",
              "      <td>2.46</td>\n",
              "      <td>0.52</td>\n",
              "      <td>92.62</td>\n",
              "      <td>0.98</td>\n",
              "      <td>11.00</td>\n",
              "      <td>11.81</td>\n",
              "      <td>20.96</td>\n",
              "      <td>9.53</td>\n",
              "      <td>20.73</td>\n",
              "      <td>32824</td>\n",
              "      <td>100.00</td>\n",
              "      <td>27182</td>\n",
              "      <td>59.79</td>\n",
              "      <td>0.51</td>\n",
              "      <td>44.72</td>\n",
              "      <td>43.40</td>\n",
              "      <td>9.01</td>\n",
              "      <td>23.56</td>\n",
              "      <td>34973</td>\n",
              "      <td>14131</td>\n",
              "      <td>14416</td>\n",
              "      <td>13630</td>\n",
              "      <td>13197</td>\n",
              "      <td>17313</td>\n",
              "      <td>8532</td>\n",
              "      <td>9398</td>\n",
              "      <td>2460</td>\n",
              "      <td>7.56</td>\n",
              "      <td>7.82</td>\n",
              "      <td>26.14</td>\n",
              "      <td>12.42</td>\n",
              "      <td>5.18</td>\n",
              "      <td>50.54</td>\n",
              "      <td>9.34</td>\n",
              "      <td>23.36</td>\n",
              "      <td>13.53</td>\n",
              "      <td>23.54</td>\n",
              "      <td>9.89</td>\n",
              "      <td>20.56</td>\n",
              "      <td>12.38</td>\n",
              "      <td>11.23</td>\n",
              "      <td>2.99</td>\n",
              "      <td>76.77</td>\n",
              "      <td>74.20</td>\n",
              "      <td>76.92</td>\n",
              "      <td>82.42</td>\n",
              "      <td>55.73</td>\n",
              "      <td>62.62</td>\n",
              "      <td>434</td>\n",
              "      <td>1.60</td>\n",
              "      <td>2414</td>\n",
              "      <td>6.63</td>\n",
              "      <td>9.03</td>\n",
              "      <td>17.15</td>\n",
              "      <td>26.72</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.66</td>\n",
              "      <td>1.26</td>\n",
              "      <td>1.97</td>\n",
              "      <td>88.95</td>\n",
              "      <td>1.70</td>\n",
              "      <td>5.10</td>\n",
              "      <td>3.50</td>\n",
              "      <td>2.44</td>\n",
              "      <td>2.37</td>\n",
              "      <td>2.67</td>\n",
              "      <td>74.61</td>\n",
              "      <td>4.39</td>\n",
              "      <td>61.03</td>\n",
              "      <td>2</td>\n",
              "      <td>957</td>\n",
              "      <td>93.30</td>\n",
              "      <td>76.81</td>\n",
              "      <td>0.84</td>\n",
              "      <td>29.47</td>\n",
              "      <td>1967</td>\n",
              "      <td>2.51</td>\n",
              "      <td>0.27</td>\n",
              "      <td>91700</td>\n",
              "      <td>123900</td>\n",
              "      <td>164000</td>\n",
              "      <td>72300</td>\n",
              "      <td>347</td>\n",
              "      <td>451</td>\n",
              "      <td>551</td>\n",
              "      <td>204</td>\n",
              "      <td>514</td>\n",
              "      <td>30.5</td>\n",
              "      <td>23.9</td>\n",
              "      <td>13.1</td>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>7.35</td>\n",
              "      <td>48.66</td>\n",
              "      <td>46.73</td>\n",
              "      <td>75.54</td>\n",
              "      <td>92.30</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>27.5</td>\n",
              "      <td>1195.2</td>\n",
              "      <td>0.12</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>5</td>\n",
              "      <td>13.61</td>\n",
              "      <td>5</td>\n",
              "      <td>13.61</td>\n",
              "      <td>24</td>\n",
              "      <td>65.32</td>\n",
              "      <td>96</td>\n",
              "      <td>261.29</td>\n",
              "      <td>628</td>\n",
              "      <td>1709.26</td>\n",
              "      <td>895</td>\n",
              "      <td>2435.97</td>\n",
              "      <td>179</td>\n",
              "      <td>487.19</td>\n",
              "      <td>8</td>\n",
              "      <td>21.77</td>\n",
              "      <td>353.83</td>\n",
              "      <td>4654.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2213</th>\n",
              "      <td>Beevillecity</td>\n",
              "      <td>TX</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>10</td>\n",
              "      <td>13547</td>\n",
              "      <td>2.89</td>\n",
              "      <td>3.37</td>\n",
              "      <td>69.91</td>\n",
              "      <td>0.90</td>\n",
              "      <td>62.11</td>\n",
              "      <td>17.16</td>\n",
              "      <td>30.01</td>\n",
              "      <td>14.73</td>\n",
              "      <td>10.42</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>19899</td>\n",
              "      <td>71.67</td>\n",
              "      <td>1.70</td>\n",
              "      <td>21.94</td>\n",
              "      <td>26.44</td>\n",
              "      <td>13.05</td>\n",
              "      <td>10.29</td>\n",
              "      <td>22103</td>\n",
              "      <td>8100</td>\n",
              "      <td>9555</td>\n",
              "      <td>6437</td>\n",
              "      <td>8271</td>\n",
              "      <td>171</td>\n",
              "      <td>4436</td>\n",
              "      <td>5338</td>\n",
              "      <td>4021</td>\n",
              "      <td>30.32</td>\n",
              "      <td>24.37</td>\n",
              "      <td>39.63</td>\n",
              "      <td>12.40</td>\n",
              "      <td>12.12</td>\n",
              "      <td>52.53</td>\n",
              "      <td>8.22</td>\n",
              "      <td>25.16</td>\n",
              "      <td>10.63</td>\n",
              "      <td>20.87</td>\n",
              "      <td>10.35</td>\n",
              "      <td>29.18</td>\n",
              "      <td>14.36</td>\n",
              "      <td>12.48</td>\n",
              "      <td>3.46</td>\n",
              "      <td>67.76</td>\n",
              "      <td>63.45</td>\n",
              "      <td>87.82</td>\n",
              "      <td>74.12</td>\n",
              "      <td>50.57</td>\n",
              "      <td>60.14</td>\n",
              "      <td>279</td>\n",
              "      <td>2.35</td>\n",
              "      <td>309</td>\n",
              "      <td>4.85</td>\n",
              "      <td>8.09</td>\n",
              "      <td>11.00</td>\n",
              "      <td>20.71</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.47</td>\n",
              "      <td>48.92</td>\n",
              "      <td>6.66</td>\n",
              "      <td>9.83</td>\n",
              "      <td>7.10</td>\n",
              "      <td>2.84</td>\n",
              "      <td>2.93</td>\n",
              "      <td>2.73</td>\n",
              "      <td>60.11</td>\n",
              "      <td>9.64</td>\n",
              "      <td>50.28</td>\n",
              "      <td>2</td>\n",
              "      <td>802</td>\n",
              "      <td>85.39</td>\n",
              "      <td>58.39</td>\n",
              "      <td>5.61</td>\n",
              "      <td>67.21</td>\n",
              "      <td>1964</td>\n",
              "      <td>15.91</td>\n",
              "      <td>0.87</td>\n",
              "      <td>26000</td>\n",
              "      <td>37800</td>\n",
              "      <td>52100</td>\n",
              "      <td>26100</td>\n",
              "      <td>135</td>\n",
              "      <td>227</td>\n",
              "      <td>317</td>\n",
              "      <td>182</td>\n",
              "      <td>316</td>\n",
              "      <td>26.2</td>\n",
              "      <td>23.3</td>\n",
              "      <td>14.1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.28</td>\n",
              "      <td>82.26</td>\n",
              "      <td>54.05</td>\n",
              "      <td>79.72</td>\n",
              "      <td>94.06</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>6.3</td>\n",
              "      <td>2142.2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>15.71</td>\n",
              "      <td>7</td>\n",
              "      <td>54.98</td>\n",
              "      <td>79</td>\n",
              "      <td>620.48</td>\n",
              "      <td>192</td>\n",
              "      <td>1508.01</td>\n",
              "      <td>474</td>\n",
              "      <td>3722.9</td>\n",
              "      <td>13</td>\n",
              "      <td>102.1</td>\n",
              "      <td>1</td>\n",
              "      <td>7.85</td>\n",
              "      <td>691.17</td>\n",
              "      <td>5340.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2214</th>\n",
              "      <td>WestSacramentocity</td>\n",
              "      <td>CA</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>10</td>\n",
              "      <td>28898</td>\n",
              "      <td>2.61</td>\n",
              "      <td>2.39</td>\n",
              "      <td>71.27</td>\n",
              "      <td>9.09</td>\n",
              "      <td>24.43</td>\n",
              "      <td>12.99</td>\n",
              "      <td>25.21</td>\n",
              "      <td>11.63</td>\n",
              "      <td>12.12</td>\n",
              "      <td>28664</td>\n",
              "      <td>99.19</td>\n",
              "      <td>23287</td>\n",
              "      <td>68.89</td>\n",
              "      <td>1.20</td>\n",
              "      <td>27.54</td>\n",
              "      <td>28.62</td>\n",
              "      <td>19.05</td>\n",
              "      <td>17.29</td>\n",
              "      <td>27897</td>\n",
              "      <td>11510</td>\n",
              "      <td>13074</td>\n",
              "      <td>8163</td>\n",
              "      <td>9874</td>\n",
              "      <td>6827</td>\n",
              "      <td>7540</td>\n",
              "      <td>8275</td>\n",
              "      <td>5287</td>\n",
              "      <td>18.50</td>\n",
              "      <td>13.93</td>\n",
              "      <td>33.68</td>\n",
              "      <td>8.86</td>\n",
              "      <td>9.27</td>\n",
              "      <td>53.35</td>\n",
              "      <td>9.44</td>\n",
              "      <td>17.75</td>\n",
              "      <td>19.61</td>\n",
              "      <td>17.44</td>\n",
              "      <td>15.77</td>\n",
              "      <td>29.72</td>\n",
              "      <td>18.84</td>\n",
              "      <td>17.31</td>\n",
              "      <td>3.20</td>\n",
              "      <td>62.64</td>\n",
              "      <td>60.23</td>\n",
              "      <td>72.43</td>\n",
              "      <td>66.15</td>\n",
              "      <td>50.44</td>\n",
              "      <td>57.32</td>\n",
              "      <td>1152</td>\n",
              "      <td>4.85</td>\n",
              "      <td>4765</td>\n",
              "      <td>31.54</td>\n",
              "      <td>39.50</td>\n",
              "      <td>46.32</td>\n",
              "      <td>55.97</td>\n",
              "      <td>5.20</td>\n",
              "      <td>6.51</td>\n",
              "      <td>7.64</td>\n",
              "      <td>9.23</td>\n",
              "      <td>74.98</td>\n",
              "      <td>7.76</td>\n",
              "      <td>8.58</td>\n",
              "      <td>5.70</td>\n",
              "      <td>2.58</td>\n",
              "      <td>2.45</td>\n",
              "      <td>2.76</td>\n",
              "      <td>52.72</td>\n",
              "      <td>11.31</td>\n",
              "      <td>61.63</td>\n",
              "      <td>2</td>\n",
              "      <td>600</td>\n",
              "      <td>94.85</td>\n",
              "      <td>55.68</td>\n",
              "      <td>3.67</td>\n",
              "      <td>28.67</td>\n",
              "      <td>1960</td>\n",
              "      <td>7.37</td>\n",
              "      <td>1.43</td>\n",
              "      <td>68400</td>\n",
              "      <td>87600</td>\n",
              "      <td>114900</td>\n",
              "      <td>46500</td>\n",
              "      <td>272</td>\n",
              "      <td>369</td>\n",
              "      <td>449</td>\n",
              "      <td>177</td>\n",
              "      <td>426</td>\n",
              "      <td>30.9</td>\n",
              "      <td>21.2</td>\n",
              "      <td>11.6</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>16.49</td>\n",
              "      <td>55.29</td>\n",
              "      <td>48.74</td>\n",
              "      <td>66.20</td>\n",
              "      <td>89.08</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>21.7</td>\n",
              "      <td>1331.0</td>\n",
              "      <td>1.39</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>5</td>\n",
              "      <td>16.53</td>\n",
              "      <td>19</td>\n",
              "      <td>62.8</td>\n",
              "      <td>102</td>\n",
              "      <td>337.15</td>\n",
              "      <td>152</td>\n",
              "      <td>502.41</td>\n",
              "      <td>791</td>\n",
              "      <td>2614.53</td>\n",
              "      <td>1458</td>\n",
              "      <td>4819.2</td>\n",
              "      <td>405</td>\n",
              "      <td>1338.67</td>\n",
              "      <td>20</td>\n",
              "      <td>66.11</td>\n",
              "      <td>918.89</td>\n",
              "      <td>8838.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2215 rows × 147 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                communityname state countyCode communityCode  fold  \\\n",
              "0     BerkeleyHeightstownship    NJ         39          5320     1   \n",
              "1              Marpletownship    PA         45         47616     1   \n",
              "2                  Tigardcity    OR          ?             ?     1   \n",
              "3            Gloversvillecity    NY         35         29443     1   \n",
              "4                 Bemidjicity    MN          7          5068     1   \n",
              "...                       ...   ...        ...           ...   ...   \n",
              "2210               Mercedcity    CA          ?             ?    10   \n",
              "2211            Pinevillecity    LA          ?             ?    10   \n",
              "2212              Yucaipacity    CA          ?             ?    10   \n",
              "2213             Beevillecity    TX          ?             ?    10   \n",
              "2214       WestSacramentocity    CA          ?             ?    10   \n",
              "\n",
              "      population  householdsize  racepctblack  racePctWhite  racePctAsian  \\\n",
              "0          11980           3.10          1.37         91.78          6.50   \n",
              "1          23123           2.82          0.80         95.57          3.44   \n",
              "2          29344           2.43          0.74         94.33          3.43   \n",
              "3          16656           2.40          1.70         97.35          0.50   \n",
              "4          11245           2.76          0.53         89.16          1.17   \n",
              "...          ...            ...           ...           ...           ...   \n",
              "2210       56216           3.07          6.87         61.68         15.23   \n",
              "2211       12251           2.68         21.18         76.65          1.52   \n",
              "2212       32824           2.46          0.52         92.62          0.98   \n",
              "2213       13547           2.89          3.37         69.91          0.90   \n",
              "2214       28898           2.61          2.39         71.27          9.09   \n",
              "\n",
              "      racePctHisp  agePct12t21  agePct12t29  agePct16t24  agePct65up  \\\n",
              "0            1.88        12.47        21.44        10.93       11.33   \n",
              "1            0.85        11.01        21.30        10.48       17.18   \n",
              "2            2.35        11.36        25.88        11.01       10.28   \n",
              "3            0.70        12.55        25.20        12.19       17.57   \n",
              "4            0.52        24.46        40.53        28.69       12.65   \n",
              "...           ...          ...          ...          ...         ...   \n",
              "2210        29.86        15.46        30.16        14.34        8.08   \n",
              "2211         1.29        17.36        31.23        16.97       12.57   \n",
              "2212        11.00        11.81        20.96         9.53       20.73   \n",
              "2213        62.11        17.16        30.01        14.73       10.42   \n",
              "2214        24.43        12.99        25.21        11.63       12.12   \n",
              "\n",
              "      numbUrban  pctUrban  medIncome  pctWWage  pctWFarmSelf  pctWInvInc  \\\n",
              "0         11980    100.00      75122     89.24          1.55       70.20   \n",
              "1         23123    100.00      47917     78.99          1.11       64.11   \n",
              "2         29344    100.00      35669     82.00          1.15       55.73   \n",
              "3             0      0.00      20580     68.15          0.24       38.95   \n",
              "4             0      0.00      17390     69.33          0.55       42.82   \n",
              "...         ...       ...        ...       ...           ...         ...   \n",
              "2210      56216    100.00      24727     75.05          1.12       31.42   \n",
              "2211      12251    100.00      20321     75.06          0.47       33.25   \n",
              "2212      32824    100.00      27182     59.79          0.51       44.72   \n",
              "2213          0      0.00      19899     71.67          1.70       21.94   \n",
              "2214      28664     99.19      23287     68.89          1.20       27.54   \n",
              "\n",
              "      pctWSocSec  pctWPubAsst  pctWRetire  medFamInc  perCapInc  whitePerCap  \\\n",
              "0          23.62         1.03       18.39      79584      29711        30233   \n",
              "1          35.50         2.75       22.85      55323      20148        20191   \n",
              "2          22.25         2.94       14.56      42112      16946        17103   \n",
              "3          39.48        11.71       18.33      26501      10810        10909   \n",
              "4          32.16        11.21       14.43      24018       8483         9009   \n",
              "...          ...          ...         ...        ...        ...          ...   \n",
              "2210       21.45        19.98       14.41      27388      10237        13041   \n",
              "2211       27.63         8.85       18.23      25000       9995        11353   \n",
              "2212       43.40         9.01       23.56      34973      14131        14416   \n",
              "2213       26.44        13.05       10.29      22103       8100         9555   \n",
              "2214       28.62        19.05       17.29      27897      11510        13074   \n",
              "\n",
              "      blackPerCap  indianPerCap  AsianPerCap OtherPerCap  HispPerCap  \\\n",
              "0           13600          5725        27101        5115       22838   \n",
              "1           18137             0        20074        5250       12222   \n",
              "2           16644         21606        15528        5954        8405   \n",
              "3            9984          4941         3541        2451        4391   \n",
              "4             887          4425         3352        3000        1328   \n",
              "...           ...           ...          ...         ...         ...   \n",
              "2210         8344          8590         3399        6470        6644   \n",
              "2211         5768         10910         1718       11471        4883   \n",
              "2212        13630         13197        17313        8532        9398   \n",
              "2213         6437          8271          171        4436        5338   \n",
              "2214         8163          9874         6827        7540        8275   \n",
              "\n",
              "      NumUnderPov  PctPopUnderPov  PctLess9thGrade  PctNotHSGrad  PctBSorMore  \\\n",
              "0             227            1.96             5.81          9.90        48.18   \n",
              "1             885            3.98             5.61         13.72        29.89   \n",
              "2            1389            4.75             2.80          9.09        30.13   \n",
              "3            2831           17.23            11.05         33.68        10.81   \n",
              "4            2855           29.99            12.15         23.06        25.28   \n",
              "...           ...             ...              ...           ...          ...   \n",
              "2210        13804           25.06            17.12         30.87        15.79   \n",
              "2211         2364           20.79            12.51         27.71        19.28   \n",
              "2212         2460            7.56             7.82         26.14        12.42   \n",
              "2213         4021           30.32            24.37         39.63        12.40   \n",
              "2214         5287           18.50            13.93         33.68         8.86   \n",
              "\n",
              "      PctUnemployed  PctEmploy  PctEmplManu  PctEmplProfServ  PctOccupManu  \\\n",
              "0              2.70      64.55        14.65            28.82          5.49   \n",
              "1              2.43      61.96        12.26            29.28          6.39   \n",
              "2              4.01      69.80        15.95            21.52          8.79   \n",
              "3              9.86      54.74        31.22            27.43         26.76   \n",
              "4              9.08      52.44         6.89            36.54         10.94   \n",
              "...             ...        ...          ...              ...           ...   \n",
              "2210           9.99      55.53        13.47            27.18         16.38   \n",
              "2211           7.90      54.64         7.81            37.06         10.37   \n",
              "2212           5.18      50.54         9.34            23.36         13.53   \n",
              "2213          12.12      52.53         8.22            25.16         10.63   \n",
              "2214           9.27      53.35         9.44            17.75         19.61   \n",
              "\n",
              "      PctOccupMgmtProf  MalePctDivorce  MalePctNevMarr  FemalePctDiv  \\\n",
              "0                50.73            3.67           26.38          5.22   \n",
              "1                37.64            4.23           27.99          6.45   \n",
              "2                32.48           10.10           25.78         14.76   \n",
              "3                22.71           10.98           28.15         14.47   \n",
              "4                27.80            7.51           50.66         11.64   \n",
              "...                ...             ...             ...           ...   \n",
              "2210             25.02           10.22           31.91         16.28   \n",
              "2211             28.73           10.86           29.51         16.12   \n",
              "2212             23.54            9.89           20.56         12.38   \n",
              "2213             20.87           10.35           29.18         14.36   \n",
              "2214             17.44           15.77           29.72         18.84   \n",
              "\n",
              "      TotalPctDiv  PersPerFam  PctFam2Par  PctKids2Par  PctYoungKids2Par  \\\n",
              "0            4.47        3.22       91.43        90.17             95.78   \n",
              "1            5.42        3.11       86.91        85.33             96.82   \n",
              "2           12.55        2.95       78.54        78.85             92.37   \n",
              "3           12.91        2.98       64.02        62.36             65.38   \n",
              "4            9.73        2.98       58.59        55.20             66.51   \n",
              "...           ...         ...         ...          ...               ...   \n",
              "2210        13.34        3.56       67.04        64.81             76.19   \n",
              "2211        13.77        3.12       68.57        63.66             80.29   \n",
              "2212        11.23        2.99       76.77        74.20             76.92   \n",
              "2213        12.48        3.46       67.76        63.45             87.82   \n",
              "2214        17.31        3.20       62.64        60.23             72.43   \n",
              "\n",
              "      PctTeen2Par  PctWorkMomYoungKids  PctWorkMom  NumKidsBornNeverMar  \\\n",
              "0           95.81                44.56       58.88                   31   \n",
              "1           86.46                51.14       62.43                   43   \n",
              "2           75.72                66.08       74.19                  164   \n",
              "3           67.43                59.59       70.27                  561   \n",
              "4           79.17                61.22       68.94                  402   \n",
              "...           ...                  ...         ...                  ...   \n",
              "2210        72.78                47.24       55.38                 1960   \n",
              "2211        73.68                64.20       66.67                  277   \n",
              "2212        82.42                55.73       62.62                  434   \n",
              "2213        74.12                50.57       60.14                  279   \n",
              "2214        66.15                50.44       57.32                 1152   \n",
              "\n",
              "      PctKidsBornNeverMar  NumImmig  PctImmigRecent  PctImmigRec5  \\\n",
              "0                    0.36      1277            8.69         13.00   \n",
              "1                    0.24      1920            5.21          8.65   \n",
              "2                    0.88      1468           16.42         23.98   \n",
              "3                    3.84       339           13.86         13.86   \n",
              "4                    4.70       196           46.94         56.12   \n",
              "...                   ...       ...             ...           ...   \n",
              "2210                 4.49     10623           22.97         35.12   \n",
              "2211                 2.98       275            2.91         45.09   \n",
              "2212                 1.60      2414            6.63          9.03   \n",
              "2213                 2.35       309            4.85          8.09   \n",
              "2214                 4.85      4765           31.54         39.50   \n",
              "\n",
              "      PctImmigRec8  PctImmigRec10  PctRecentImmig  PctRecImmig5  PctRecImmig8  \\\n",
              "0            20.99          30.93            0.93          1.39          2.24   \n",
              "1            13.33          22.50            0.43          0.72          1.11   \n",
              "2            32.08          35.63            0.82          1.20          1.61   \n",
              "3            15.34          15.34            0.28          0.28          0.31   \n",
              "4            67.86          69.90            0.82          0.98          1.18   \n",
              "...            ...            ...             ...           ...           ...   \n",
              "2210         42.10          60.31            4.34          6.64          7.96   \n",
              "2211         65.45          75.64            0.07          1.01          1.47   \n",
              "2212         17.15          26.72            0.49          0.66          1.26   \n",
              "2213         11.00          20.71            0.11          0.18          0.25   \n",
              "2214         46.32          55.97            5.20          6.51          7.64   \n",
              "\n",
              "      PctRecImmig10  PctSpeakEnglOnly  PctNotSpeakEnglWell  PctLargHouseFam  \\\n",
              "0              3.30             85.68                 1.37             4.81   \n",
              "1              1.87             87.79                 1.81             4.25   \n",
              "2              1.78             93.11                 1.14             2.97   \n",
              "3              0.31             94.98                 0.56             3.93   \n",
              "4              1.22             94.64                 0.39             5.23   \n",
              "...             ...               ...                  ...              ...   \n",
              "2210          11.40             65.33                11.87            13.49   \n",
              "2211           1.70             92.78                 0.86             5.03   \n",
              "2212           1.97             88.95                 1.70             5.10   \n",
              "2213           0.47             48.92                 6.66             9.83   \n",
              "2214           9.23             74.98                 7.76             8.58   \n",
              "\n",
              "      PctLargHouseOccup  PersPerOccupHous  PersPerOwnOccHous  \\\n",
              "0                  4.17              2.99               3.00   \n",
              "1                  3.34              2.70               2.83   \n",
              "2                  2.05              2.42               2.69   \n",
              "3                  2.56              2.37               2.51   \n",
              "4                  3.11              2.35               2.55   \n",
              "...                 ...               ...                ...   \n",
              "2210               9.91              3.03               2.83   \n",
              "2211               3.37              2.49               2.58   \n",
              "2212               3.50              2.44               2.37   \n",
              "2213               7.10              2.84               2.93   \n",
              "2214               5.70              2.58               2.45   \n",
              "\n",
              "      PersPerRentOccHous  PctPersOwnOccup  PctPersDenseHous  PctHousLess3BR  \\\n",
              "0                   2.84            91.46              0.39           11.06   \n",
              "1                   1.96            89.03              1.01           23.60   \n",
              "2                   2.06            64.18              2.03           47.46   \n",
              "3                   2.20            58.18              1.21           45.66   \n",
              "4                   2.12            58.13              2.94           55.64   \n",
              "...                  ...              ...               ...             ...   \n",
              "2210                3.19            41.69             16.89           57.23   \n",
              "2211                2.39            56.06              3.99           54.48   \n",
              "2212                2.67            74.61              4.39           61.03   \n",
              "2213                2.73            60.11              9.64           50.28   \n",
              "2214                2.76            52.72             11.31           61.63   \n",
              "\n",
              "      MedNumBR  HousVacant  PctHousOccup  PctHousOwnOcc  PctVacantBoarded  \\\n",
              "0            3          64         98.37          91.01              3.12   \n",
              "1            3         240         97.15          84.88              0.00   \n",
              "2            3         544         95.68          57.79              0.92   \n",
              "3            3         669         91.19          54.89              2.54   \n",
              "4            2         333         92.45          53.57              3.90   \n",
              "...        ...         ...           ...            ...               ...   \n",
              "2210         2         683         96.40          44.63              1.46   \n",
              "2211         2         523         89.72          54.24              4.59   \n",
              "2212         2         957         93.30          76.81              0.84   \n",
              "2213         2         802         85.39          58.39              5.61   \n",
              "2214         2         600         94.85          55.68              3.67   \n",
              "\n",
              "      PctVacMore6Mos  MedYrHousBuilt  PctHousNoPhone  PctWOFullPlumb  \\\n",
              "0              37.50            1959            0.00            0.28   \n",
              "1              18.33            1958            0.31            0.14   \n",
              "2               7.54            1976            1.55            0.12   \n",
              "3              57.85            1939            7.00            0.87   \n",
              "4              42.64            1958            7.45            0.82   \n",
              "...              ...             ...             ...             ...   \n",
              "2210           13.18            1973            4.91            0.55   \n",
              "2211           46.08            1966            7.56            0.12   \n",
              "2212           29.47            1967            2.51            0.27   \n",
              "2213           67.21            1964           15.91            0.87   \n",
              "2214           28.67            1960            7.37            1.43   \n",
              "\n",
              "      OwnOccLowQuart  OwnOccMedVal  OwnOccHiQuart  OwnOccQrange  RentLowQ  \\\n",
              "0             215900        262600         326900        111000       685   \n",
              "1             136300        164200         199900         63600       467   \n",
              "2              74700         90400         112000         37300       370   \n",
              "3              36400         49600          66500         30100       195   \n",
              "4              30600         43200          59500         28900       202   \n",
              "...              ...           ...            ...           ...       ...   \n",
              "2210           71200         91100         118900         47700       298   \n",
              "2211           33600         52000          72700         39100       176   \n",
              "2212           91700        123900         164000         72300       347   \n",
              "2213           26000         37800          52100         26100       135   \n",
              "2214           68400         87600         114900         46500       272   \n",
              "\n",
              "      RentMedian  RentHighQ  RentQrange  MedRent  MedRentPctHousInc  \\\n",
              "0           1001       1001         316     1001               23.8   \n",
              "1            560        672         205      627               27.6   \n",
              "2            428        520         150      484               24.1   \n",
              "3            250        309         114      333               28.7   \n",
              "4            283        362         160      332               32.2   \n",
              "...          ...        ...         ...      ...                ...   \n",
              "2210         374        455         157      438               29.8   \n",
              "2211         248        297         121      330               23.8   \n",
              "2212         451        551         204      514               30.5   \n",
              "2213         227        317         182      316               26.2   \n",
              "2214         369        449         177      426               30.9   \n",
              "\n",
              "      MedOwnCostPctInc  MedOwnCostPctIncNoMtg  NumInShelters  NumStreet  \\\n",
              "0                 21.1                   14.0             11          0   \n",
              "1                 20.7                   12.5              0          0   \n",
              "2                 21.7                   11.6             16          0   \n",
              "3                 20.6                   14.5              0          0   \n",
              "4                 23.2                   12.9              2          0   \n",
              "...                ...                    ...            ...        ...   \n",
              "2210              22.6                   11.7             64          0   \n",
              "2211              17.3                   14.4              0          0   \n",
              "2212              23.9                   13.1             44          0   \n",
              "2213              23.3                   14.1              0          0   \n",
              "2214              21.2                   11.6             10          2   \n",
              "\n",
              "      PctForeignBorn  PctBornSameState  PctSameHouse85  PctSameCity85  \\\n",
              "0              10.66             53.72           65.29          78.09   \n",
              "1               8.30             77.17           71.27          90.22   \n",
              "2               5.00             44.77           36.60          61.26   \n",
              "3               2.04             88.71           56.70          90.17   \n",
              "4               1.74             73.75           42.22          60.34   \n",
              "...              ...               ...             ...            ...   \n",
              "2210           18.90             52.67           39.19          74.58   \n",
              "2211            2.24             75.16           49.12          78.79   \n",
              "2212            7.35             48.66           46.73          75.54   \n",
              "2213            2.28             82.26           54.05          79.72   \n",
              "2214           16.49             55.29           48.74          66.20   \n",
              "\n",
              "      PctSameState85 LemasSwornFT LemasSwFTPerPop LemasSwFTFieldOps  \\\n",
              "0              89.14            ?               ?                 ?   \n",
              "1              96.12            ?               ?                 ?   \n",
              "2              82.85            ?               ?                 ?   \n",
              "3              96.24            ?               ?                 ?   \n",
              "4              89.02            ?               ?                 ?   \n",
              "...              ...          ...             ...               ...   \n",
              "2210           85.88            ?               ?                 ?   \n",
              "2211           92.85            ?               ?                 ?   \n",
              "2212           92.30            ?               ?                 ?   \n",
              "2213           94.06            ?               ?                 ?   \n",
              "2214           89.08            ?               ?                 ?   \n",
              "\n",
              "     LemasSwFTFieldPerPop LemasTotalReq LemasTotReqPerPop PolicReqPerOffic  \\\n",
              "0                       ?             ?                 ?                ?   \n",
              "1                       ?             ?                 ?                ?   \n",
              "2                       ?             ?                 ?                ?   \n",
              "3                       ?             ?                 ?                ?   \n",
              "4                       ?             ?                 ?                ?   \n",
              "...                   ...           ...               ...              ...   \n",
              "2210                    ?             ?                 ?                ?   \n",
              "2211                    ?             ?                 ?                ?   \n",
              "2212                    ?             ?                 ?                ?   \n",
              "2213                    ?             ?                 ?                ?   \n",
              "2214                    ?             ?                 ?                ?   \n",
              "\n",
              "     PolicPerPop RacialMatchCommPol PctPolicWhite PctPolicBlack PctPolicHisp  \\\n",
              "0              ?                  ?             ?             ?            ?   \n",
              "1              ?                  ?             ?             ?            ?   \n",
              "2              ?                  ?             ?             ?            ?   \n",
              "3              ?                  ?             ?             ?            ?   \n",
              "4              ?                  ?             ?             ?            ?   \n",
              "...          ...                ...           ...           ...          ...   \n",
              "2210           ?                  ?             ?             ?            ?   \n",
              "2211           ?                  ?             ?             ?            ?   \n",
              "2212           ?                  ?             ?             ?            ?   \n",
              "2213           ?                  ?             ?             ?            ?   \n",
              "2214           ?                  ?             ?             ?            ?   \n",
              "\n",
              "     PctPolicAsian PctPolicMinor OfficAssgnDrugUnits NumKindsDrugsSeiz  \\\n",
              "0                ?             ?                   ?                 ?   \n",
              "1                ?             ?                   ?                 ?   \n",
              "2                ?             ?                   ?                 ?   \n",
              "3                ?             ?                   ?                 ?   \n",
              "4                ?             ?                   ?                 ?   \n",
              "...            ...           ...                 ...               ...   \n",
              "2210             ?             ?                   ?                 ?   \n",
              "2211             ?             ?                   ?                 ?   \n",
              "2212             ?             ?                   ?                 ?   \n",
              "2213             ?             ?                   ?                 ?   \n",
              "2214             ?             ?                   ?                 ?   \n",
              "\n",
              "     PolicAveOTWorked  LandArea  PopDens  PctUsePubTrans PolicCars  \\\n",
              "0                   ?       6.5   1845.9            9.63         ?   \n",
              "1                   ?      10.6   2186.7            3.84         ?   \n",
              "2                   ?      10.6   2780.9            4.37         ?   \n",
              "3                   ?       5.2   3217.7            3.31         ?   \n",
              "4                   ?      11.5    974.2            0.38         ?   \n",
              "...               ...       ...      ...             ...       ...   \n",
              "2210                ?      16.7   3365.4            0.59         ?   \n",
              "2211                ?       7.3   1682.8            1.15         ?   \n",
              "2212                ?      27.5   1195.2            0.12         ?   \n",
              "2213                ?       6.3   2142.2            0.00         ?   \n",
              "2214                ?      21.7   1331.0            1.39         ?   \n",
              "\n",
              "     PolicOperBudg LemasPctPolicOnPatr LemasGangUnitDeploy  \\\n",
              "0                ?                   ?                   ?   \n",
              "1                ?                   ?                   ?   \n",
              "2                ?                   ?                   ?   \n",
              "3                ?                   ?                   ?   \n",
              "4                ?                   ?                   ?   \n",
              "...            ...                 ...                 ...   \n",
              "2210             ?                   ?                   ?   \n",
              "2211             ?                   ?                   ?   \n",
              "2212             ?                   ?                   ?   \n",
              "2213             ?                   ?                   ?   \n",
              "2214             ?                   ?                   ?   \n",
              "\n",
              "      LemasPctOfficDrugUn PolicBudgPerPop  murders  murdPerPop rapes  \\\n",
              "0                     0.0               ?        0        0.00     0   \n",
              "1                     0.0               ?        0        0.00     1   \n",
              "2                     0.0               ?        3        8.30     6   \n",
              "3                     0.0               ?        0        0.00    10   \n",
              "4                     0.0               ?        0        0.00     ?   \n",
              "...                   ...             ...      ...         ...   ...   \n",
              "2210                  0.0               ?       10       16.49    30   \n",
              "2211                  0.0               ?        0        0.00     4   \n",
              "2212                  0.0               ?        5       13.61     5   \n",
              "2213                  0.0               ?        0        0.00     2   \n",
              "2214                  0.0               ?        5       16.53    19   \n",
              "\n",
              "     rapesPerPop robberies robbbPerPop assaults assaultPerPop burglaries  \\\n",
              "0              0         1         8.2        4         32.81         14   \n",
              "1           4.25         5       21.26       24        102.05         57   \n",
              "2           16.6        56      154.95       14         38.74        274   \n",
              "3          57.86        10       57.86       33        190.93        225   \n",
              "4              ?         4       32.04       14        112.14         91   \n",
              "...          ...       ...         ...      ...           ...        ...   \n",
              "2210       49.46       121       199.5      170        280.29       1376   \n",
              "2211       33.09         1        8.27       10         82.73        104   \n",
              "2212       13.61        24       65.32       96        261.29        628   \n",
              "2213       15.71         7       54.98       79        620.48        192   \n",
              "2214        62.8       102      337.15      152        502.41        791   \n",
              "\n",
              "     burglPerPop larcenies larcPerPop autoTheft autoTheftPerPop arsons  \\\n",
              "0         114.85       138    1132.08        16          131.26      2   \n",
              "1         242.37       376    1598.78        26          110.55      1   \n",
              "2         758.14      1797    4972.19       136           376.3     22   \n",
              "3        1301.78       716    4142.56        47          271.93      ?   \n",
              "4         728.93      1060    8490.87        91          728.93      5   \n",
              "...          ...       ...        ...       ...             ...    ...   \n",
              "2210     2268.72      2563    4225.82       489          806.25     34   \n",
              "2211      860.43       574     4748.9        24          198.56      2   \n",
              "2212     1709.26       895    2435.97       179          487.19      8   \n",
              "2213     1508.01       474     3722.9        13           102.1      1   \n",
              "2214     2614.53      1458     4819.2       405         1338.67     20   \n",
              "\n",
              "     arsonsPerPop ViolentCrimesPerPop nonViolPerPop  \n",
              "0           16.41               41.02       1394.59  \n",
              "1            4.25              127.56       1955.95  \n",
              "2           60.87              218.59       6167.51  \n",
              "3               ?              306.64             ?  \n",
              "4           40.05                   ?       9988.79  \n",
              "...           ...                 ...           ...  \n",
              "2210        56.06              545.75       7356.84  \n",
              "2211        16.55               124.1       5824.44  \n",
              "2212        21.77              353.83        4654.2  \n",
              "2213         7.85              691.17       5340.87  \n",
              "2214        66.11              918.89        8838.5  \n",
              "\n",
              "[2215 rows x 147 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPgz9d88m7uk",
        "colab_type": "text"
      },
      "source": [
        "### Initial Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0o5hcj5R_Iv",
        "colab_type": "text"
      },
      "source": [
        "Drop Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAUHyexwSBCL",
        "colab_type": "text"
      },
      "source": [
        "Replace values with NaNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvejj-DXdB4x",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEHAb7dqE2wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_communities.info)\n",
        "print(df_communities.describe())\n",
        "print(df_communities.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rh9V3tHEqyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label = \"Risk\" # 1 = \"Good\"; 0 = \"Bad\"\n",
        "\n",
        "eda_descr_stats(df_communities, disc_feature=\"Sex\", disc_min_value=\"female\", label = \"Risk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8f4lMHddc9c",
        "colab_type": "text"
      },
      "source": [
        "### Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff0bvUYDdk5i",
        "colab_type": "text"
      },
      "source": [
        "#### 0) Preprocess data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhjE6ELVFJFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute preprocessing function\n",
        "df_3_adult_train_input, df_3_adult_train_label = fair_preprocess(data = df_3_adult, \n",
        "                                                                 label = \"Over-50K\", \n",
        "                                                                 neg_class = \"<=50K\", \n",
        "                                                                 pos_class = \">50K\")\n",
        "\n",
        "# Check whether binary encoding was successful and seperate datasets were created\n",
        "print(df_3_adult.groupby([\"Over-50K\"]).agg({\"Over-50K\": 'count'}))\n",
        "print(df_3_adult_train_input.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd0EZIuxd4aO",
        "colab_type": "text"
      },
      "source": [
        "#### 1) Define Hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2LXwjKNF33U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_classifier_compas = hyperparameter_tuning(df_compas_train_input, df_compas_train_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJgdMZOueEaV",
        "colab_type": "text"
      },
      "source": [
        "#### 2) Create datasets with diff. minority group sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E8VoqTFQ-R7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Creating dfs for minority and majority group\n",
        "is_black = df_compas[\"race\"].isin([\"African-American\"])\n",
        "is_white = df_compas[\"race\"].isin([\"Caucasian\"])\n",
        "df_compas_black = df_compas[is_black]  # Minority\n",
        "df_compas_white = df_compas[is_white]  # Majority\n",
        "\n",
        "list_dfs_compas = create_datasets(min_data = df_compas_black, maj_data = df_compas_white, training_sizes=training_sizes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXtqNpMaeCVz",
        "colab_type": "text"
      },
      "source": [
        "#### 3) Create dataframes with diff. metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2kvEbOCRFc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_dfs = list_dfs_compas\n",
        "label = \"two_year_recid\"\n",
        "model = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
        "                              criterion='gini', max_depth=25, max_features='auto',\n",
        "                              max_leaf_nodes=None, max_samples=None,\n",
        "                              min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                              min_samples_leaf=1, min_samples_split=5,\n",
        "                              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
        "                              n_jobs=None, oob_score=False, random_state=None,\n",
        "                              verbose=0, warm_start=False)\n",
        "\n",
        "# model = grid_rf_class # <- Best model from hyperparameter tuning\n",
        "\n",
        "cv = 5 # To Do: For final calculations, change to cv = 10\n",
        "discr_feature = \"race\"\n",
        "min_value = \"African-American\"\n",
        "maj_value = \"Caucasian\"\n",
        "\n",
        "results_df_compas = metrics_to_df(list_dfs=list_dfs_compas, label = label, model = model, \n",
        "                                  cv = cv, discr_feature = \"race\", min_value = min_value,\n",
        "                                  maj_value = maj_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYIcGDseeUin",
        "colab_type": "text"
      },
      "source": [
        "#### 4) Create visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF3SKKuaRPGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# B: Learning Curve Function from scikit learn\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "# Define arguments\n",
        "f1 = make_scorer(f1_score) \n",
        "\n",
        "df_3_adult_train_input = pd.get_dummies(df_3_adult_train_input)\n",
        "\n",
        "# Plot actual learning curve\n",
        "plot_learning_curve(estimator = model, \n",
        "                    title = \"Communities and Crime Dataset - Random Forest Learning Curve\", \n",
        "                    X = df_3_adult_train_input, y = df_3_adult_train_label, \n",
        "                    cv = 5, \n",
        "                    scoring = f1, \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = np.linspace(.1, 1.0, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCvuD9nZRU1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_metrics_line_chart(results_df_compas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtBH_by9RaCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maj_min_metrics_line_chart(results_df_compas) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbLR_jzFLgHY",
        "colab_type": "text"
      },
      "source": [
        "Ideas\n",
        "- Individual Metric dfs zusammenmergen\n",
        "- Rows Minority als key dafür nehmen\n",
        "- Auf Kernmetriken fokussieren, diese dann umbenennen\n",
        "- Kernmetriken von verschiedenen Datasets in zentralem Line Chart zeigen\n",
        "- Dann Korridor einzeichnen, wo die Knickpunkte sich jeweils befinden\n",
        "- Überlappungen von Korridoren zeigen, indem sich durch alpha die Farbe gegenseitig verstärkt\n",
        "- Dann Tabelle rechts daneben platzieren, die Zusatzinformationen bietet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUBpyP0PcQZX",
        "colab_type": "text"
      },
      "source": [
        "# Deprecated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqOWar36y6bd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Random Forest Hyperparameter Tuning Results\n",
        "\n",
        "# model = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
        "#                               criterion='gini', max_depth=25, max_features='auto',\n",
        "#                               max_leaf_nodes=None, max_samples=None,\n",
        "#                               min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "#                               min_samples_leaf=1, min_samples_split=5,\n",
        "#                               min_weight_fraction_leaf=0.0, n_estimators=100,\n",
        "#                               n_jobs=None, oob_score=False, random_state=None,\n",
        "#                               verbose=0, warm_start=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP9y_W-gJcV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RF Hyperparameter Tuning\n",
        "\n",
        "# HYPERPARAMETER TUNING\n",
        "\n",
        "def hyperparameter_tuning(df_train_input, df_train_label):\n",
        "\n",
        "  from sklearn import model_selection\n",
        "\n",
        "  # Create the hyperparameter grid\n",
        "  # Important: Keys in the dictionary must be valid hyperparameters \n",
        "  param_grid = {\"learning_rate\": [0.3, 0.1],\n",
        "                \"max_depth\": [3, 6, 9], \n",
        "                \"min_child_weight\": [1, 3, 5],       \n",
        "                \"reg_lambda\": [2, 5, 10, 15, 100]}\n",
        "\n",
        "  # 1. Fundamental hyperparameters\n",
        "  # learning rate & number of trees\n",
        "\n",
        "  xgb1 = XGBClassifier(learning_rate =0.1, n_estimators=1000, max_depth=5,\n",
        "                     min_child_weight=1, gamma=0, subsample=0.8,\n",
        "                     colsample_bytree=0.8, objective= 'binary:logistic', \n",
        "                     scale_pos_weight=1, seed=27)\n",
        "\n",
        "\n",
        "  # n_estimators = number of trees in the foreset\n",
        "  # max_features = max number of features considered for splitting a node\n",
        "  # max_depth = max number of levels in each decision tree\n",
        "  # min_samples_split = min number of data points placed in a node before the node is split\n",
        "  # min_samples_leaf = min number of data points allowed in a leaf node\n",
        "  # bootstrap = method for sampling data points (with or without replacement)\n",
        "\n",
        "  # Define classifier\n",
        "  rf_grid_search = RandomForestClassifier(criterion= \"gini\",\n",
        "                                          max_features = \"auto\")\n",
        "\n",
        "  # Learning Curve for Slice \n",
        "  from sklearn.metrics import make_scorer\n",
        "  from sklearn.metrics import f1_score\n",
        "\n",
        "  # Define model\n",
        "  f1 = make_scorer(f1_score)\n",
        "\n",
        "  # Dummy Coding\n",
        "  df_train_input_dummy = pd.get_dummies(df_train_input)\n",
        "\n",
        "  grid_rf_class = sklearn.model_selection.GridSearchCV(estimator = rf_grid_search,\n",
        "                                                      param_grid = param_grid, \n",
        "                                                      scoring= f1,\n",
        "                                                      n_jobs = 2, \n",
        "                                                      cv = 5,\n",
        "                                                      refit = True,\n",
        "                                                      return_train_score = True)\n",
        "  \n",
        "  # Fit model\n",
        "  grid_rf_class.fit(df_train_input_dummy, df_train_label)\n",
        "\n",
        "  return(grid_rf_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWQHazJKBKYc",
        "colab_type": "text"
      },
      "source": [
        "**Steps**\n",
        "1. Specify hyperparameters for the chosen classification algorithm by means of grid search using standard ranges. Specify by means of on the dataset as a whole (full min. and maj.).\n",
        "2. Specify what is the majority and what is the minority class based on a value of a feature that identifies groups that are at risk of being discriminated. \n",
        "3. First, take the whole dataset with the majority and the minority class. Then, filter the dataset such that the majority class is fixed in size and only a certain number of training examples that are considered for the minority class is considered. Change this number step-by-step. For each step, train a random forest and test by mans of 10-fold cross validation.\n",
        "4. Attach the final outcome predictions  to the individual dataframes of the different datsets with diff. min. group sizes as seperate feature. -> *Open questions: is that manageable because we have different models (e.g. by means of majority vote of the 10 models with regards to classification)? If not possible then necessary to store the metric information directly without data frame as intermediate step (for that check calculation of fairness metric) (-> then also easier to store validation score to have an underfit/overfit check) OR use train/test split instead of k-fold cv (least prefered)*\n",
        "- a) Then filter that dataset so that only the minority group is displayed. Then, calculate  the confusion matrix and the F1-score. \n",
        "- b) Calculate the fairness metric by means of comparing the predictions for the minority class with the prediction for the majority class (check: https://aif360.readthedocs.io/en/latest/modules/metrics.html#binary-label-dataset-metric).\n",
        "5. Store the F1-score and the fairness metric for each training set size in a data frame. The data frame should have the following features: \n",
        "- a) Absolute number of training examples for the minority group, \n",
        "- b) percentage share of the minority group in the majority, \n",
        "- c) F-1 score for the min. group (maybe also seperately including std. dev. upper and lower bound), \n",
        "- d) fairness metric based on the min. & maj. group prediction comparisons. **Extended**: in case the fairness metric is not in the range of [0, 1] like the F1-score, normalize/standardize it (make sure that this is not biasing/altering the metrics results).\n",
        "6. Based on the data frame from step 5, plot a learning curve that shows the relationship between the different absolute sizes of training examples and the performance and fairness metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIkI8slaAokF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Age Boxplot\n",
        "\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "\n",
        "fig = make_subplots(rows=1, cols=3)\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_3_adult[\"Age\"])\n",
        ")  \n",
        "\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_3_adult[\"Hours-per-week\"])\n",
        ")\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_3_adult[\"Capital-Gain\"])\n",
        ")\n",
        "\n",
        "fig.update_layout(height=600, width=800, title_text=\"Boxplots for Numeric Features\")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "# Age Boxplot\n",
        "\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = px.box(df_3_adult, y=\"Age\")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY4OQL4mAOv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bar Plot and Pie Plot for Discriminatory Feature \n",
        "\n",
        "def target_distribution(y_var, data):\n",
        "    val = data[y_var]\n",
        "\n",
        "    plt.style.use('seaborn-whitegrid')\n",
        "    plt.rcParams.update({'font.size': 13})\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
        "\n",
        "    cnt = val.value_counts().sort_values(ascending=True)\n",
        "    labels = cnt.index.values\n",
        "\n",
        "    sizes = cnt.values\n",
        "    colors = sns.color_palette(\"PuBu\", len(labels))\n",
        "\n",
        "    #------------COUNT-----------------------\n",
        "    ax1.barh(cnt.index.values, cnt.values, color=colors)\n",
        "    ax1.set_title('Count plot of '+y_var)\n",
        "\n",
        "    #------------PERCENTAGE-------------------\n",
        "    ax2.pie(sizes, labels=labels, colors=colors,autopct='%1.0f%%', shadow=True, startangle=130)\n",
        "    ax2.axis('equal')\n",
        "    ax2.set_title('Distribution of '+y_var)\n",
        "    plt.show()\n",
        "\n",
        "# Distribution for Race\n",
        "\n",
        "target_distribution(y_var=\"Race\", data=df_3_adult)\n",
        "\n",
        "# Absolute number of members of different \"races\"\n",
        "print(df_3_adult.Race.value_counts(dropna=False, sort=True)) # Learning: würde genauso mit df_3_adult[\"Race\"].value_counts(dropna=False) funktionieren\n",
        "\n",
        "# Percentage of members of different \"races\"\n",
        "print(df_3_adult.Race.value_counts(normalize=True, dropna=False, sort=True))\n",
        "\n",
        "# Define Histogram Function\n",
        "def plot_histo(data, col, Y_columns):\n",
        "    df = data.copy()\n",
        "    fig, axs = plt.subplots(1,2,figsize=(20,6))\n",
        "    \n",
        "    for i in range(0,2):\n",
        "        cnt = []; y_col = Y_columns[i]\n",
        "        Y_values = df[y_col].dropna().drop_duplicates().values\n",
        "        for val in Y_values:\n",
        "            cnt += [df[df[y_col] == val][col].values]\n",
        "        bins = df[col].nunique()\n",
        "\n",
        "        axs[i].hist(cnt, bins=bins, stacked=True)\n",
        "        axs[i].legend(Y_values,loc='upper right')\n",
        "        axs[i].set_title(\"Histogram of the \"+col+\" column by \"+y_col)\n",
        "        \n",
        "    plt.show()\n",
        "\n",
        "Y_columns = [\"Race\", \"Sex\"]\n",
        "\n",
        "print(plot_histo(data = df_3_adult, col='Capital-Gain',Y_columns=Y_columns))\n",
        "print(plot_histo(data = df_3_adult, col='Capital-Loss',Y_columns=Y_columns))\n",
        "print(plot_histo(data = df_3_adult, col='Hours-per-week',Y_columns=Y_columns))\n",
        "\n",
        "# Define Function for Bar Plot\n",
        "\n",
        "def plot_bar(data, col, Y_columns, max_cat=10):\n",
        "    df = data.copy()\n",
        "    \n",
        "    fig, axs = plt.subplots(1,2,figsize=(20,6))\n",
        "    cat_val = df[col].value_counts()[0:max_cat].index.values\n",
        "    df = df[df[col].isin(cat_val)]\n",
        "\n",
        "    for i in range(0,2):\n",
        "        y_col = Y_columns[i]\n",
        "        Y_values = df[y_col].dropna().drop_duplicates().values\n",
        "        for val in Y_values:\n",
        "            cnt = df[df[y_col] == val][col].value_counts().sort_index()\n",
        "            axs[i].barh(cnt.index.values, cnt.values)\n",
        "        axs[i].legend(Y_values,loc='upper right')\n",
        "        axs[i].set_title(\"Bar plot of the \"+col+\" column by \"+y_col)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "print(df_3_adult.columns)\n",
        "\n",
        "Y_columns = [\"Race\"]\n",
        "\n",
        "print(plot_bar(data = df_3_adult, col='Sex',Y_columns=Y_columns)) # Error because values for two subplots are expected"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QthjEWnrFWCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Column names\n",
        "# 'f1_complete', \"f1_complete_train\", 'f1_minority', 'f1_majority', \"tpr_complete\", \n",
        "# 'tpr_minority', \"tpr_majority\", \"fpr_minority\", \"fpr_majority\", \"prob_yhat_1_minority\", \"prob_yhat_1_majority\"\n",
        "\n",
        "# results_df[\"rel_share_min_of_maj\"]\n",
        "# results_df[\"aver_abs_odds_diff\"]  \n",
        "# results_df[\"stat_parity_diff\"] \n",
        "# results_df[\"equal_opport_dist\"]  \n",
        "# results_df[\"disparate_impact\"] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEP61EfQhFyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eda_descr_stats(data, disc_feature, disc_min_value, label, second_disc_feature=\"\"):\n",
        "\n",
        "  # 1. Sensitive Feature\n",
        "  print(f\"1. Sensitive Attribute: One or more of the following features are sensitive ones: {data.columns}.\")\n",
        "  print(f\"1. Sensitive Attribute: These are the individual values for the sensitive attribute: {data[disc_feature].unique()}.\")\n",
        "\n",
        "  # 2. Binary Target Feature\n",
        "  print(\"2. Binary Target Variable: The Binary Target Feature has the following values and counts:\")\n",
        "  print(data.groupby([label]).agg({label: 'count'}))\n",
        "\n",
        "  # 3. Total Number of Predictor Features\n",
        "  print(f\"3. The Total Number of Predictor Features is: {data.shape[1]}.\")\n",
        "\n",
        "  # 4. Total Number of Training Examples\n",
        "  print(f\"4. The Total Number of Training Examples is: {data.shape[0]}.\")\n",
        "\n",
        "  # 5. Total Number of Training Examples in the Minority Group \n",
        "  is_min = data[disc_feature].isin([disc_min_value])\n",
        "  print(f\"5. The Total Number of Training Examples in the Minority Group is: {len(data[is_min].index)}.\")\n",
        "\n",
        "  # 6. Sample Size Disparity\n",
        "  # Absolute number of members of different \"races\"\n",
        "  print(f\"6. Sample Size Disparity: The Absolute numbers of members of different races are as follows:  {data[disc_feature].value_counts(dropna=False, sort=True)}.\")\n",
        "  # Percentage of members of different \"races\"\n",
        "  print(f\"6. Sample Size Disparity: The Percentages of the number of members of different races are as follows: {data[disc_feature].value_counts(normalize=True, dropna=False, sort=True)}.\")\n",
        "\n",
        "  # 7. Class Balance\n",
        "  print(data[label].value_counts(dropna=False))\n",
        "  print(data[label].value_counts(normalize=True, dropna=False))\n",
        "\n",
        "  # 8. Coarseness of Features\n",
        "  print(\"8. Coarseness of Features: Details on missing values of features in the dataset:\")\n",
        "  print(data.isna().any())\n",
        "  print(data.isna().sum())\n",
        "\n",
        "  # 9. Severity of Outliers for Numeric Features\n",
        "  print(\"9. Severity of Outliers for Numeric Features\")\n",
        "  ax = sns.boxplot(data=data, orient=\"h\", palette=\"Set2\")\n",
        "  print(ax)\n",
        "\n",
        "  # (10. Cross-sectional sample size disparity)\n",
        "  if second_disc_feature:\n",
        "    print(\"10. Cross-sectional sample size disparity\")\n",
        "    data.groupby([second_disc_feature, disc_feature]).agg({label: 'count'})\n",
        "\n",
        "  # (11. Feature Importance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH6m4e6TWiwZ",
        "colab_type": "text"
      },
      "source": [
        "*Outcomes*:\n",
        "1. Understanding of the general structure of the dataset\n",
        "2. Understanding of basic statistics of the features of the dataset\n",
        "3. Identification of **features** on the basis of which subpopulations can be identified and discrimination can happen\n",
        "4. Identification of **values** of these features that are connected to certain subpopulations\n",
        "5. Identification of the percentage of certain subpopulation based on the values of these features -> *Sample Size Disparity I*\n",
        "6. Identification of the percentage of certain cross-sectional subpopulations based on the overall dataset -> *Sample Size Disparity II*\n",
        "7. Identification of the target feature and the type of class balance\n",
        "8. Identification of percentage of missing values per feature\n",
        "9. Identification of severity of outliers per feature\n",
        "10. Identification of the distribution of class values (for majority and minority)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si8Ss0hAWdO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Sensitive Feature\n",
        "print(f\"1. Sensitive Attribute: One or more of the following features are sensitive ones: {df_3_adult.columns}.\")\n",
        "print(f\"1. Sensitive Attribute: These are the individual values for the sensitive attribute: {df_3_adult.Race.unique()}.\")\n",
        "\n",
        "# 2. Binary Target Feature\n",
        "print(\"2. Binary Target Variable: The Binary Target Feature has the following values and counts:\")\n",
        "print(df_3_adult.groupby([\"Over-50K\"]).agg({\"Over-50K\": 'count'}))\n",
        "\n",
        "# 3. Total Number of Predictor Features\n",
        "print(f\"3. The Total Number of Predictor Features is: {df_3_adult.shape[1]}.\")\n",
        "\n",
        "# 4. Total Number of Training Examples\n",
        "df_3_adult.shape[0]\n",
        "print(f\"4. The Total Number of Training Examples is: {df_3_adult.shape[0]}.\")\n",
        "\n",
        "# 5. Total Number of Training Examples in the Minority Group \n",
        "is_black = df_3_adult[\"Race\"].isin([\"Black\"])\n",
        "print(f\"5. The Total Number of Training Examples in the Minority Group is: {len(df_3_adult[is_black].index)}.\")\n",
        "\n",
        "# 6. Sample Size Disparity\n",
        "# Absolute number of members of different \"races\"\n",
        "print(f\"6. Sample Size Disparity: The Absolute numbers of members of different races are as follows:  {df_3_adult.Race.value_counts(dropna=False, sort=True)}.\")\n",
        "# Percentage of members of different \"races\"\n",
        "print(f\"6. Sample Size Disparity: The Percentages of the number of members of different races are as follows: {df_3_adult.Race.value_counts(normalize=True, dropna=False, sort=True)}.\")\n",
        "\n",
        "# 7. Class Balance\n",
        "print(df_3_adult[\"Over-50K\"].value_counts(dropna=False))\n",
        "print(df_3_adult[\"Over-50K\"].value_counts(normalize=True, dropna=False))\n",
        "\n",
        "# 8. Coarseness of Features\n",
        "print(\"8. Coarseness of Features: Details on missing values of features in the dataset:\")\n",
        "print(df_3_adult.isna().any())\n",
        "print(df_3_adult.isna().sum())\n",
        "\n",
        "# 9. Severity of Outliers for Numeric Features\n",
        "print(\"9. Severity of Outliers for Numeric Features\")\n",
        "ax = sns.boxplot(data=df_3_adult, orient=\"h\", palette=\"Set2\")\n",
        "print(ax)\n",
        "\n",
        "# (10. Cross-sectional sample size disparity)\n",
        "print(\"10. Cross-sectional sample size disparity\")\n",
        "df_3_adult.groupby(['Sex', 'Race']).agg({\"Over-50K\": 'count'})\n",
        "\n",
        "# (11. Feature Importance)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJdQ2el7iHnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Basic Performance Statistics\n",
        "\n",
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix \n",
        "print(confusion_matrix(y_train, y_train_pred))\n",
        "\n",
        "# ROC Curve\n",
        "from sklearn.metrics import roc_curve\n",
        "fpr, tpr, thresholds = roc_curve(y_train, y_train_pred) # Validate if the correct data was used\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr, tpr, label='k-NN')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('k-NN ROC Curve')\n",
        "plt.show();\n",
        "\n",
        "# AUC \n",
        "from sklearn.metrics import roc_auc_score\n",
        "print(roc_auc_score(y_train, y_train_pred)) # Validate if the correct data was used\n",
        "\n",
        "# Full Classification Metrics Report\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_train, y_train_pred))\n",
        "\n",
        "# Visual Model Evaluation\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import plot_roc_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpK9B42QzITz",
        "colab_type": "text"
      },
      "source": [
        "## Test Metric function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t4W0BvffrcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Libraries to study\n",
        "from aif360.datasets import StandardDataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
        "from aif360.algorithms.preprocessing import LFR, Reweighing\n",
        "from aif360.algorithms.inprocessing import AdversarialDebiasing, PrejudiceRemover\n",
        "from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing, EqOddsPostprocessing, RejectOptionClassification\n",
        "\n",
        "# This DataFrame is created to stock differents models and fair metrics that we produce in this notebook\n",
        "algo_metrics = pd.DataFrame(columns=['model', 'fair_metrics', 'prediction', 'probs'])\n",
        "\n",
        "def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n",
        "    return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], \n",
        "                                            columns=['model', 'fair_metrics', 'prediction', 'probs'], index=[name]))\n",
        "\n",
        "def get_fair_metrics_and_plot(data, model, plot=True, model_aif=False):\n",
        "    pred = model.predict(data).labels if model_aif else model.predict(data.features)\n",
        "    # fair_metrics function available in the metrics.py file\n",
        "    fair = fair_metrics(data, pred)\n",
        "\n",
        "    if plot:\n",
        "        # plot_fair_metrics function available in the visualisations.py file\n",
        "        # The visualisation of this function is inspired by the dashboard on the demo of IBM aif360 \n",
        "        plot_fair_metrics(fair)\n",
        "        display(fair)\n",
        "    \n",
        "    return fair\n",
        "\n",
        "display(Markdown('### Bias metrics for the Sex model'))\n",
        "fair = get_fair_metrics_and_plot(data_orig_sex_test, rf_orig_sex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGgBB2XQFSKa",
        "colab_type": "text"
      },
      "source": [
        "Overfitting / underfitting check by means of plotly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3vxb42sFUIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A: Custom plotly visualization - Function definition\n",
        "\n",
        "def compl_fitting_line_chart(metric_df):\n",
        "\n",
        "  import plotly.graph_objects as go\n",
        "\n",
        "  # Create traces\n",
        "\n",
        "  # Performance Metrics\n",
        "  fig = go.Figure()\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"f1_complete\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='F1 Minority'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"f1_complete_train\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='TPR/Recall Minority'))\n",
        "\n",
        "  # Edit the layout\n",
        "  fig.update_layout(title={'text': \"Learning Curve for the Complete Dataset\",\n",
        "                           'y':0.9,\n",
        "                           'x':0.5,\n",
        "                           'xanchor': 'center',\n",
        "                           'yanchor': 'top'},\n",
        "                    xaxis_title='Rows Complete',\n",
        "                    yaxis_title='Metric Score', \n",
        "                    font=dict(size=14))\n",
        "  \n",
        "  fig.show()\n",
        "\n",
        "\n",
        "# B: Custom plotly visualization - Function Execution\n",
        "compl_fitting_line_chart(results_df)  #Change into results_df_adult"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf2CHWVDzHxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST FUNCTION \n",
        "\n",
        "# Import relevant modules\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Get specific Dataframe from list of dataframes\n",
        "df_check = list_dfs[5]\n",
        "\n",
        "# Define Input and target columns\n",
        "label = \"Over-50K\"\n",
        "df_train_input = df_check.drop(columns=[label])  # Input\n",
        "df_train_label = df_check[label]                 # Target\n",
        "\n",
        "# Apply dummy coding\n",
        "df_train_input = pd.get_dummies(df_train_input)\n",
        "\n",
        "## TRAIN & TEST\n",
        "# Define model\n",
        "rf_test = RandomForestClassifier(criterion= \"gini\", \n",
        "                                 max_features = \"auto\",\n",
        "                                 max_depth = 4,\n",
        "                                 min_samples_leaf = 4,\n",
        "                                 n_estimators = 100)\n",
        "# Predict \n",
        "y_train_pred = cross_val_predict(rf_test,\n",
        "                                 df_train_input, \n",
        "                                 df_train_label, \n",
        "                                 cv = 10)\n",
        "\n",
        "# Append prediction labels to original dataset\n",
        "df_check['y_pred'] = y_train_pred\n",
        "\n",
        "# Create dataset version for minority class \n",
        "is_black = df_check[\"Race\"].isin([\"Black\"]) \n",
        "df_check_black = df_check[is_black]  # Minority group\n",
        "# Create dataset version for majority class\n",
        "is_white = df_check[\"Race\"].isin([\"White\"])\n",
        "df_check_white = df_check[is_white] # Majority group\n",
        "\n",
        "\n",
        "## METRICS\n",
        "# Get metrics for the COMPLETE dataset\n",
        "rows_compl_list = [] \n",
        "rows_compl = len(df_check.index)\n",
        "rows_compl_list.append(rows_compl)\n",
        "\n",
        "f1_compl_list = []\n",
        "f1_compl = f1_score(df_train_label, y_train_pred) \n",
        "f1_compl_list.append(f1_compl) \n",
        "\n",
        "# metrics.f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)) \n",
        "# -> https://stackoverflow.com/questions/43162506/undefinedmetricwarning-f-score-is-ill-defined-and-being-set-to-0-0-in-labels-wi\n",
        "\n",
        "# Get metrics for the MINORITY group\n",
        "# NUMBER OF ROWS\n",
        "rows_min_list = [] \n",
        "rows_min = len(df_check_black.index)\n",
        "rows_min_list.append(rows_min)\n",
        "# F1\n",
        "f1_min_list = []\n",
        "f1_min = f1_score(df_check_black[\"Over-50K\"], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))  # or labels = [0,1]\n",
        "f1_min_list.append(f1_min)\n",
        "# TPR, RECALL\n",
        "tpr_min_list = []\n",
        "tpr_min = recall_score(df_check_black[\"Over-50K\"], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "tpr_min_list.append(tpr_min)\n",
        "# FPR, SPECIFICITY\n",
        "fpr_min_list = []\n",
        "tn_min, fp_min, fn_min, tp_min = confusion_matrix(df_check_black[\"Over-50K\"], df_check_black[\"y_pred\"], labels=[0,1]).ravel()\n",
        "fpr_min = tn_min / (tn_min+fp_min)\n",
        "fpr_min_list.append(fpr_min)\n",
        "\n",
        "# Get metrics for the MAJORITY group\n",
        "# NUMBER OF ROWS\n",
        "rows_maj_list = [] \n",
        "rows_maj = len(df_check_white.index)\n",
        "rows_maj_list.append(rows_maj)\n",
        "# F1\n",
        "f1_maj_list = []\n",
        "f1_maj = f1_score(df_check_white[\"Over-50K\"], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "f1_maj_list.append(f1_maj)\n",
        "# TPR, RECALL\n",
        "tpr_maj_list = []\n",
        "tpr_maj = recall_score(df_check_white[\"Over-50K\"], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"])) #average='weighted'\n",
        "tpr_maj_list.append(tpr_maj)\n",
        "# FPR, SPECIFICITY\n",
        "fpr_maj_list = []\n",
        "tn_maj, fp_maj, fn_maj, tp_maj = confusion_matrix(df_check_white[\"Over-50K\"], df_check_white[\"y_pred\"], labels=[0,1]).ravel()\n",
        "fpr_maj = tn_maj / (tn_maj+fp_maj)\n",
        "fpr_maj_list.append(fpr_maj)\n",
        "\n",
        "\n",
        "# Store metrics for different iterations in Data Frame\n",
        "results_df = pd.DataFrame({'rows_complete': rows_compl_list,\n",
        "                           \"rows_minority\": rows_min_list,\n",
        "                           \"rows_majority\": rows_maj_list, \n",
        "                           'f1_complete': f1_compl_list,\n",
        "                           'f1_minority': f1_min_list,\n",
        "                           'f1_majority': f1_maj_list,\n",
        "                           'tpr_minority': tpr_min_list,\n",
        "                           \"tpr_majority\": tpr_maj_list,\n",
        "                           \"fpr_min\": fpr_min_list,\n",
        "                           \"fpr_maj\": fpr_maj_list})\n",
        "\n",
        "# Calculate new columns and append to df \n",
        "results_df[\"rel_share_min_of_maj\"] = (results_df[\"rows_minority\"] / results_df[\"rows_majority\"]) \n",
        "# TBD if this calculation is correct that way or if (min/(maj+min))*100\n",
        "\n",
        "# results_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DceolfFEzQiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get train scores\n",
        "\n",
        "f1 = make_scorer(f1_score)\n",
        "\n",
        "cross_val_results = cross_validate(estimator = rf_test, \n",
        "                                  X = df_train_input, y = df_train_label, cv = 10, \n",
        "                                  scoring = f1, return_train_score=True)\n",
        "print(cross_val_results)\n",
        "f1_avg_train_score = np.mean(cross_val_results[\"train_score\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJmXzP_tdJXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Join iso alpha codes \n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW_4b7rBdJ7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Store data in dataframe\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "csv_columns = [\n",
        "  \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital-Status\",\n",
        "  \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital-Gain\", \"Capital-Loss\",\n",
        "  \"Hours-per-week\", \"Country\", \"Over-50K\"]\n",
        "\n",
        "df_3_adult = pd.read_csv(io.BytesIO(uploaded['adult (1).data']), names = csv_columns, skipinitialspace=True)# Dataset is now stored in a Pandas Dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDpY_z3mW4kb",
        "colab_type": "text"
      },
      "source": [
        "Variant A: ggplot2\n",
        "\n",
        "- http://www.sthda.com/english/wiki/ggplot2-line-plot-quick-start-guide-r-software-and-data-visualization\n",
        "- https://www.datanovia.com/en/blog/how-to-create-a-ggplot-with-multiple-lines/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjg9oXQ_WyjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Visualization with ggplot2\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from plotnine import *\n",
        "from plotnine.data import mpg\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "(ggplot(data = results_df)\n",
        "  + aes(x='rows_minority') \n",
        "  + geom_line(aes(y = \"f1_minority\"), color =\"darkred\") \n",
        "  + geom_line(aes(y = \"aver_abs_odds_diff\"), color = \"steelblue\") \n",
        "  + ggtitle(\"Minority Metrics\")\n",
        "  # + scale_color_identity(guide = legend()) # Add legend based on colours\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5YkTvP4Aa4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Groupy by for conditional probabilities \n",
        "\n",
        "# Prepare groupby data \n",
        "filter_race_black_y_1 = df_check[\"Race\"].isin([\"Black\"]) & df_check[\"y_pred\"].isin([1])\n",
        "filter_race_white_y_1 = df_check[\"Race\"].isin([\"White\"]) & df_check[\"y_pred\"].isin([1])\n",
        "\n",
        "filter_race_black = df_check[\"Race\"].isin([\"Black\"])\n",
        "filter_race_white = df_check[\"Race\"].isin([\"White\"])\n",
        "\n",
        "prob_race_black_y_1 = len(df_check[filter_race_black_y_1].index) / len(df_check[filter_race_black].index)\n",
        "prob_race_white_y_1 = len(df_check[filter_race_white_y_1].index) / len(df_check[filter_race_white].index)\n",
        "\n",
        "print(prob_race_black_y_1)\n",
        "print(prob_race_white_y_1)\n",
        "\n",
        "# rating_probs = df_check.groupby(\"Race\").size().div(len(df_check))\n",
        "# groupby_probs = df_check.groupby(['y_pred', \"Race\"]).size().div(len(df_check)).div(rating_probs, axis=0, level=\"Race\")\n",
        "# print(groupby_probs)\n",
        "\n",
        "# Experiments identify how to subset groupby in order to get right conditional probability \n",
        "rating_probs = df_check.groupby('Race').size().div(len(df_check))\n",
        "print(df_check.groupby(['y_pred', 'Race']).size().div(len(df_check)).div(rating_probs, axis=0, level='Race'))\n",
        "group_df_1 = df_check.groupby(['y_pred', 'Race']).size().div(len(df_check)).div(rating_probs, axis=0, level='Race')\n",
        "# list_groupby = list(group_df_1)\n",
        "# print(list_groupby)\n",
        "# print(list_groupby[2])\n",
        "# print(list_groupby[3])\n",
        "\n",
        "# print(group_df_1[[2]])\n",
        "# print(group_df_1[[3]])\n",
        "# test = group_df_1[[3]]\n",
        "# print(test)\n",
        "\n",
        "# # Create list\n",
        "# list_groupby_probs = list(groupby_probs)\n",
        "# print(list_groupby_probs)\n",
        "# prob_y_1_min = list_groupby_probs[2]\n",
        "# print(prob_y_1_min)\n",
        "\n",
        "# prob_y_1_maj = list_groupby_probs[3]\n",
        "# print(prob_y_1_maj)\n",
        "\n",
        "# # Cond. Prob. P(y=1|minority)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1zs8SxSqZGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learning_curve(estimator, X, y, groups=None,\n",
        "                   train_sizes=np.linspace(0.1, 1.0, 5), cv=None,\n",
        "                   scoring=None, exploit_incremental_learning=False,\n",
        "                   n_jobs=None, pre_dispatch=\"all\", verbose=0, shuffle=False,\n",
        "                   random_state=None, error_score=np.nan, return_times=False):\n",
        "  \n",
        "    \"\"\"Learning curve.\n",
        "\n",
        "    Determines cross-validated training and test scores for different training\n",
        "    set sizes.\n",
        "\n",
        "    A cross-validation generator splits the whole dataset k times in training\n",
        "    and test data. Subsets of the training set with varying sizes will be used\n",
        "    to train the estimator and a score for each training subset size and the\n",
        "    test set will be computed. Afterwards, the scores will be averaged over\n",
        "    all k runs for each training subset size.\n",
        "\n",
        "    Read more in the :ref:`User Guide <learning_curve>`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    groups : array-like, with shape (n_samples,), optional\n",
        "        Group labels for the samples used while splitting the dataset into\n",
        "        train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
        "        instance (e.g., :class:`GroupKFold`).\n",
        "\n",
        "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
        "        Relative or absolute numbers of training examples that will be used to\n",
        "        generate the learning curve. If the dtype is float, it is regarded as a\n",
        "        fraction of the maximum size of the training set (that is determined\n",
        "        by the selected validation method), i.e. it has to be within (0, 1].\n",
        "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
        "        Note that for classification the number of samples usually have to\n",
        "        be big enough to contain at least one sample from each class.\n",
        "        (default: np.linspace(0.1, 1.0, 5))\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "\n",
        "        - None, to use the default 5-fold cross validation,\n",
        "        - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
        "        - :term:`CV splitter`,\n",
        "        - An iterable yielding (train, test) splits as arrays of indices.\n",
        "\n",
        "        For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
        "        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
        "        other cases, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validation strategies that can be used here.\n",
        "\n",
        "        .. versionchanged:: 0.22\n",
        "            ``cv`` default value if None changed from 3-fold to 5-fold.\n",
        "\n",
        "    scoring : string, callable or None, optional, default: None\n",
        "        A string (see model evaluation documentation) or\n",
        "        a scorer callable object / function with signature\n",
        "        ``scorer(estimator, X, y)``.\n",
        "\n",
        "    exploit_incremental_learning : boolean, optional, default: False\n",
        "        If the estimator supports incremental learning, this will be\n",
        "        used to speed up fitting for different training set sizes.\n",
        "\n",
        "    n_jobs : int or None, optional (default=None)\n",
        "        Number of jobs to run in parallel.\n",
        "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
        "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
        "        for more details.\n",
        "\n",
        "    pre_dispatch : integer or string, optional\n",
        "        Number of predispatched jobs for parallel execution (default is\n",
        "        all). The option can reduce the allocated memory. The string can\n",
        "        be an expression like '2*n_jobs'.\n",
        "\n",
        "    verbose : integer, optional\n",
        "        Controls the verbosity: the higher, the more messages.\n",
        "\n",
        "    shuffle : boolean, optional\n",
        "        Whether to shuffle training data before taking prefixes of it\n",
        "        based on``train_sizes``.\n",
        "\n",
        "    random_state : int, RandomState instance or None, optional (default=None)\n",
        "        If int, random_state is the seed used by the random number generator;\n",
        "        If RandomState instance, random_state is the random number generator;\n",
        "        If None, the random number generator is the RandomState instance used\n",
        "        by `np.random`. Used when ``shuffle`` is True.\n",
        "\n",
        "    error_score : 'raise' or numeric\n",
        "        Value to assign to the score if an error occurs in estimator fitting.\n",
        "        If set to 'raise', the error is raised.\n",
        "        If a numeric value is given, FitFailedWarning is raised. This parameter\n",
        "        does not affect the refit step, which will always raise the error.\n",
        "\n",
        "    return_times : boolean, optional (default: False)\n",
        "        Whether to return the fit and score times.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    train_sizes_abs : array, shape (n_unique_ticks,), dtype int\n",
        "        Numbers of training examples that has been used to generate the\n",
        "        learning curve. Note that the number of ticks might be less\n",
        "        than n_ticks because duplicate entries will be removed.\n",
        "\n",
        "    train_scores : array, shape (n_ticks, n_cv_folds)\n",
        "        Scores on training sets.\n",
        "\n",
        "    test_scores : array, shape (n_ticks, n_cv_folds)\n",
        "        Scores on test set.\n",
        "\n",
        "    fit_times : array, shape (n_ticks, n_cv_folds)\n",
        "        Times spent for fitting in seconds. Only present if ``return_times``\n",
        "        is True.\n",
        "\n",
        "    score_times : array, shape (n_ticks, n_cv_folds)\n",
        "        Times spent for scoring in seconds. Only present if ``return_times``\n",
        "        is True.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    See :ref:`examples/model_selection/plot_learning_curve.py\n",
        "    <sphx_glr_auto_examples_model_selection_plot_learning_curve.py>`\n",
        "    \"\"\"\n",
        "    if exploit_incremental_learning and not hasattr(estimator, \"partial_fit\"):\n",
        "        raise ValueError(\"An estimator must support the partial_fit interface \"\n",
        "                         \"to exploit incremental learning\")\n",
        "    X, y, groups = indexable(X, y, groups)\n",
        "\n",
        "    cv = check_cv(cv, y, classifier=is_classifier(estimator))\n",
        "    # Store it as list as we will be iterating over the list multiple times\n",
        "    cv_iter = list(cv.split(X, y, groups))\n",
        "\n",
        "    scorer = check_scoring(estimator, scoring=scoring)\n",
        "\n",
        "    n_max_training_samples = len(cv_iter[0][0])\n",
        "    # Because the lengths of folds can be significantly different, it is\n",
        "    # not guaranteed that we use all of the available training data when we\n",
        "    # use the first 'n_max_training_samples' samples.\n",
        "    train_sizes_abs = _translate_train_sizes(train_sizes,\n",
        "                                             n_max_training_samples)\n",
        "    n_unique_ticks = train_sizes_abs.shape[0]\n",
        "    if verbose > 0:\n",
        "        print(\"[learning_curve] Training set sizes: \" + str(train_sizes_abs))\n",
        "\n",
        "    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,\n",
        "                        verbose=verbose)\n",
        "\n",
        "    if shuffle:\n",
        "        rng = check_random_state(random_state)\n",
        "        cv_iter = ((rng.permutation(train), test) for train, test in cv_iter)\n",
        "\n",
        "    if exploit_incremental_learning:\n",
        "        classes = np.unique(y) if is_classifier(estimator) else None\n",
        "        out = parallel(delayed(_incremental_fit_estimator)(\n",
        "            clone(estimator), X, y, classes, train, test, train_sizes_abs,\n",
        "            scorer, verbose, return_times) for train, test in cv_iter)\n",
        "    else:\n",
        "        train_test_proportions = []\n",
        "        for train, test in cv_iter:\n",
        "            for n_train_samples in train_sizes_abs:\n",
        "                train_test_proportions.append((train[:n_train_samples], test))\n",
        "\n",
        "        out = parallel(delayed(_fit_and_score)(\n",
        "            clone(estimator), X, y, scorer, train, test, verbose,\n",
        "            parameters=None, fit_params=None, return_train_score=True,\n",
        "            error_score=error_score, return_times=return_times)\n",
        "            for train, test in train_test_proportions)\n",
        "        out = np.array(out)\n",
        "        n_cv_folds = out.shape[0] // n_unique_ticks\n",
        "        dim = 4 if return_times else 2\n",
        "        out = out.reshape(n_cv_folds, n_unique_ticks, dim)\n",
        "\n",
        "    out = np.asarray(out).transpose((2, 1, 0))\n",
        "\n",
        "    ret = train_sizes_abs, out[0], out[1]\n",
        "\n",
        "    if return_times:\n",
        "        ret = ret + (out[2], out[3])\n",
        "\n",
        "    return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtMtLvu3b7Yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics_to_df(list_dfs, label, model, cv, discr_feature, min_value, maj_value):\n",
        "\n",
        "  # Import relevant modules\n",
        "  from sklearn.model_selection import cross_val_predict\n",
        "  import sklearn.metrics\n",
        "  from sklearn.metrics import make_scorer\n",
        "  from sklearn.metrics import f1_score\n",
        "  from sklearn.metrics import recall_score\n",
        "  from sklearn.metrics import confusion_matrix\n",
        "  from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "  for dataset_var in list_dfs:\n",
        "\n",
        "      # Define Input and target columns\n",
        "      df_train_input = dataset_var.drop(columns=[label])  # Input\n",
        "      df_train_label = dataset_var[label]                 # Target\n",
        "\n",
        "      # Apply dummy coding\n",
        "      df_train_input = pd.get_dummies(df_train_input)\n",
        "\n",
        "      ## TRAIN & TEST\n",
        "      # Predict \n",
        "      y_train_pred = cross_val_predict(model,\n",
        "                                       df_train_input,\n",
        "                                       df_train_label,\n",
        "                                       cv = cv)\n",
        "\n",
        "      # Append prediction labels to original dataset\n",
        "      dataset_var['y_pred'] = y_train_pred\n",
        "\n",
        "      # Create dataset for MINORITY group \n",
        "      is_black = dataset_var[discr_feature].isin([min_value])\n",
        "      df_check_black = dataset_var[is_black] \n",
        "\n",
        "      # Create dataset for MAJORITY group\n",
        "      is_white = dataset_var[discr_feature].isin([maj_value])\n",
        "      df_check_white = dataset_var[is_white] \n",
        "\n",
        "      ## METRICS\n",
        "      # Get metrics for the COMPLETE dataset\n",
        "      rows_compl_list = [] \n",
        "      rows_compl = len(dataset_var.index)\n",
        "      rows_compl_list.append(rows_compl)\n",
        "\n",
        "      f1_compl_list = []\n",
        "      f1_compl = f1_score(df_train_label, y_train_pred) \n",
        "      f1_compl_list.append(f1_compl) \n",
        "\n",
        "      tpr_compl = \n",
        "\n",
        "      # Get metrics for the MINORITY group\n",
        "      # NUMBER OF ROWS\n",
        "      rows_min_list = [] \n",
        "      rows_min = len(df_check_black.index)\n",
        "      rows_min_list.append(rows_min)\n",
        "      # F1\n",
        "      f1_min_list = []\n",
        "      f1_min = f1_score(df_check_black[label], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))  # or labels = [0,1]\n",
        "      f1_min_list.append(f1_min)\n",
        "      # TPR/RECALL\n",
        "      tpr_min_list = []\n",
        "      tpr_min = recall_score(df_check_black[label], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "      tpr_min_list.append(tpr_min)\n",
        "      # FPR/SPECIFICITY\n",
        "      fpr_min_list = []\n",
        "      tn_min, fp_min, fn_min, tp_min = confusion_matrix(df_check_black[label], df_check_black[\"y_pred\"], labels=[0,1]).ravel()\n",
        "      fpr_min = tn_min / (tn_min+fp_min)\n",
        "      fpr_min_list.append(fpr_min)\n",
        "\n",
        "      # Get metrics for the MAJORITY group\n",
        "      # NUMBER OF ROWS\n",
        "      rows_maj_list = [] \n",
        "      rows_maj = len(df_check_white.index)\n",
        "      rows_maj_list.append(rows_maj)\n",
        "      # F1\n",
        "      f1_maj_list = []\n",
        "      f1_maj = f1_score(df_check_white[label], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "      f1_maj_list.append(f1_maj)\n",
        "      # TPR/RECALL\n",
        "      tpr_maj_list = []\n",
        "      tpr_maj = recall_score(df_check_white[label], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"])) #average='weighted'\n",
        "      tpr_maj_list.append(tpr_maj)\n",
        "      # FPR/SPECIFICITY\n",
        "      fpr_maj_list = []\n",
        "      tn_maj, fp_maj, fn_maj, tp_maj = confusion_matrix(df_check_white[label], df_check_white[\"y_pred\"], labels=[0,1]).ravel()\n",
        "      fpr_maj = tn_maj / (tn_maj+fp_maj)\n",
        "      fpr_maj_list.append(fpr_maj)\n",
        "\n",
        "  # Store metrics for different iterations in Data Frame\n",
        "  results_df = pd.DataFrame({'rows_complete': rows_compl_list,\n",
        "                              \"rows_minority\": rows_min_list,\n",
        "                              \"rows_majority\": rows_maj_list, \n",
        "                              'f1_complete': f1_compl_list,\n",
        "                              'f1_minority': f1_min_list,\n",
        "                              'f1_majority': f1_maj_list,\n",
        "                              'tpr_minority': tpr_min_list,\n",
        "                              \"tpr_majority\": tpr_maj_list,\n",
        "                              \"fpr_min\": fpr_min_list,\n",
        "                              \"fpr_maj\": fpr_maj_list})\n",
        "\n",
        "  # Calculate new metric columns and append to df \n",
        "  results_df[\"rel_share_min_of_maj\"] = (results_df[\"rows_minority\"] / results_df[\"rows_majority\"]) \n",
        "\n",
        "  return(results_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcvZqMvgw-XH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATA PREPROCESSING manually\n",
        "# Encoding Binary \n",
        "\n",
        "df_3_adult[\"Over-50K\"] = df_3_adult[\"Over-50K\"].replace({'<=50K': 0, '>50K': 1})\n",
        "\n",
        "# Check if encoding was successful \n",
        "df_3_adult[\"Over-50K\"].dtypes\n",
        "\n",
        "# Input features\n",
        "df_3_adult_train_input = df_3_adult.drop(columns=[\"Over-50K\"])\n",
        "\n",
        "# Target feature\n",
        "df_3_adult_train_label = df_3_adult[\"Over-50K\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc12Lw3VC-_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoding Binary \n",
        "# df_3_adult[\"Over-50K\"] = df_3_adult[\"Over-50K\"].apply(lambda val: 1 if val == \">50K\" else val == 0)\n",
        "# df_3_adult[\"Over-50K\"] = df_3_adult[\"Over-50K\"].replace(to_replace=['<=50K', '>50K'], value=[0, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diJsTc_t6-_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## OLD FUNCTION\n",
        "\n",
        "label = \"Over-50K\"\n",
        "model = grid_rf_class # <- Best model from hyperparameter tuning\n",
        "list_dfs = list_dfs\n",
        "\n",
        "def metrics_to_df(list_dfs, label, model, cv = 10):\n",
        "\n",
        "  for dataset_var in list_dfs:\n",
        "  \n",
        "    ## DATA PREPROCESSING\n",
        "      # Seperate dataset by input features and labels\n",
        "      df_train_input = dataset_var.drop(columns=[label])  # Input\n",
        "      df_train_label = dataset_var[label]]                # Target\n",
        "\n",
        "      # Apply dummy coding\n",
        "      df_train_input = pd.get_dummies(df_train_input)\n",
        "\n",
        "    ## TRAIN & TEST\n",
        "      # Predict \n",
        "      y_train_pred = cross_val_predict(model,\n",
        "                                       df_train_input, \n",
        "                                       df_train_label, \n",
        "                                       cv = cv)\n",
        "    \n",
        "    ## METRICS\n",
        "      # Get metrics for the COMPLETE dataset\n",
        "\n",
        "      # confusion_matrix(df_train_label, y_train_pred) \n",
        "\n",
        "      rows_compl_list = [] \n",
        "      rows_compl = len(dataset_var.index)\n",
        "      rows_compl_list.append(rows_compl)\n",
        "\n",
        "      f1_compl_list = []\n",
        "      f1_compl = f1_score(df_train_label, y_train_pred)\n",
        "      f1_compl_list.append(f1_compl) \n",
        "\n",
        "      # Band for difference in Training and Validation Set\n",
        "        # train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, scoring=scoring, \n",
        "                                                                # n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "        train_scores_mean = np.mean(train_scores, axis=1)\n",
        "        train_scores_std  = np.std(train_scores, axis=1)\n",
        "        test_scores_mean  = np.mean(test_scores, axis=1)\n",
        "        test_scores_std   = np.std(test_scores, axis=1)\n",
        "        plt.grid()\n",
        "\n",
        "      # Get metrics for the MINORITY group\n",
        "        \n",
        "        row_min_list = []\n",
        "        rows_min = len(dataset_min.index)\n",
        "        row_min_list.append(rows_min)\n",
        "\n",
        "        # TPR\n",
        "        # FPR\n",
        "        # TNR\n",
        "        # FNR\n",
        "\n",
        "        f1_min_list = []\n",
        "        f1_min = f1_score(df_train_label, y_train_pred)\n",
        "        f1_min_list.append()\n",
        "\n",
        "      # Get metrics for the MAJORITY group\n",
        "\n",
        "        # Number of rows\n",
        "        row_maj_list = []\n",
        "        rows_maj = len(dataset_maj.index)\n",
        "        row_maj_list.append(rows_maj)\n",
        "\n",
        "        # TPR\n",
        "        # FPR\n",
        "        # TNR\n",
        "        # FNR\n",
        "      \n",
        "        f1_maj_list = []\n",
        "        f1_maj = f1_score(df_train_label, y_train_pred)\n",
        "        f1_maj_list.append()\n",
        "\n",
        "\n",
        "  # Store metrics for different iterations in Data Frame\n",
        "  results_df = pd.DataFrame({'Total_rows':rows_compl_list, \n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list})\n",
        "  \n",
        "  # Setting a column as an index\n",
        "  # dogs_ind = dogs.set_index(\"name\")\n",
        "\n",
        "  return(results_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWFpvWx8bRWi",
        "colab_type": "text"
      },
      "source": [
        "Calculate Confusion Matrix Cell Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-VTE1ryDbAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## DEPRECATED\n",
        "\n",
        "  # neighbors_list = list(range(5,500, 5))\n",
        "  # accuracy_list = []\n",
        "  # for test_number in neighbors_list:\n",
        "  # model = KNeighborsClassifier(n_neighbors=test_number)\n",
        "  # predictions = model.fit(X_train, y_train).predict(X_test)\n",
        "  # accuracy = accuracy_score(y_test, predictions)\n",
        "  # accuracy_list.append(accuracy)\n",
        "  # results_df = pd.DataFrame({'neighbors':neighbors_list, 'accuracy':accuracy_list})\n",
        "\n",
        "  # # Initialize df \n",
        "  # algo_metrics = pd.DataFrame(columns=['model', 'fair_metrics', 'prediction', 'probs']) # To Do: Change metrics here\n",
        "\n",
        "  # def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n",
        "  #     return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], \n",
        "  #                                             columns=['model', 'fair_metrics', 'prediction', 'probs'], \n",
        "  #                                             index=[name]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdV1izcpevA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This DataFrame is created to stock differents models and fair metrics that we produce in this notebook\n",
        "algo_metrics = pd.DataFrame(columns=['number_training_examples', 'rel_share_min', 'min_f1', \"total_f1\", 'fairness metric'])\n",
        "\n",
        "def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n",
        "    return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], \n",
        "                                            columns=['model', 'fair_metrics', 'prediction', 'probs'], \n",
        "                                            index=[name]))\n",
        "    \n",
        "def get_fair_metrics_and_plot(data, model, plot=True, model_aif=False):\n",
        "    pred = model.predict(data).labels if model_aif else model.predict(data.features)\n",
        "    # fair_metrics function available in the metrics.py file\n",
        "    fair = fair_metrics(data, pred)\n",
        "    \n",
        "    if plot:\n",
        "        # plot_fair_metrics function available in the visualisations.py file\n",
        "        # The visualisation of this function is inspired by the dashboard on the demo of IBM aif360 \n",
        "        plot_fair_metrics(fair)\n",
        "        display(fair)\n",
        "    \n",
        "    return fair\n",
        "\n",
        "# Fairness Metric\n",
        "import aif360\n",
        "import aequitas\n",
        "import auditai\n",
        "\n",
        "# 1: Average Absolute Odd Difference\n",
        "average_abs_odds_difference()\n",
        "# https://aif360.readthedocs.io/en/latest/modules/metrics.html#aif360.metrics.ClassificationMetric.average_abs_odds_difference\n",
        "    def average_odds_difference(self):\n",
        "        r\"\"\"Average of difference in FPR and TPR for unprivileged and privileged\n",
        "        groups:\n",
        "\n",
        "        .. math::\n",
        "\n",
        "           \\tfrac{1}{2}\\left[(FPR_{D = \\text{unprivileged}} - FPR_{D = \\text{privileged}})\n",
        "           + (TPR_{D = \\text{unprivileged}} - TPR_{D = \\text{privileged}}))\\right]\n",
        "\n",
        "        A value of 0 indicates equality of odds.\n",
        "        \"\"\"\n",
        "        return 0.5 * (self.difference(self.false_positive_rate)\n",
        "                    + self.difference(self.true_positive_rate))\n",
        "\n",
        "# This metric's scale would need to be \"reversed\", presumably.\n",
        "\n",
        "# 2: Equal Opportunity Distance\n",
        "equal_opportunity_difference()\n",
        "# https://aif360.readthedocs.io/en/latest/modules/metrics.html#aif360.metrics.ClassificationMetric.equal_opportunity_difference\n",
        "\n",
        "    def equal_opportunity_difference(self):\n",
        "        \"\"\"Alias of :meth:`true_positive_rate_difference`.\"\"\"\n",
        "        return self.true_positive_rate_difference()\n",
        "\n",
        "# ClassificationMetric and BinaryLabelDatasetMetric.\n",
        "\n",
        "# Template:\n",
        "\n",
        "# This DataFrame is created to stock differents models and fair metrics that we produce in this notebook\n",
        "# algo_metrics = pd.DataFrame(columns=['model', 'fair_metrics', 'prediction', 'probs'])\n",
        "# def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n",
        "#     return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], columns=['model', 'fair_metrics', 'prediction', 'probs'], index=[name]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DkTibSibQO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def get_tpr(y_actual, y_hat): # Two lists: original outcomes and prediction outcomes\n",
        "#     TP = 0\n",
        "#     FN = 0\n",
        "#     for i in range(len(y_hat)): \n",
        "#         if y_actual[i]==y_hat[i]==1:\n",
        "#            TP += 1\n",
        "#         if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
        "#            FN += 1\n",
        "#     TPR = TP/(TP+FN)\n",
        "\n",
        "# Calculation of confusion matrix rates\n",
        "\n",
        "# # Sensitivity, hit rate, recall, or true positive rate\n",
        "# TPR = TP/(TP+FN)\n",
        "# # Specificity or true negative rate\n",
        "# TNR = TN/(TN+FP) \n",
        "# # Precision or positive predictive value\n",
        "# PPV = TP/(TP+FP)\n",
        "# # Negative predictive value\n",
        "# NPV = TN/(TN+FN)\n",
        "# # Fall out or false positive rate\n",
        "# FPR = FP/(FP+TN)\n",
        "# # False negative rate\n",
        "# FNR = FN/(TP+FN)\n",
        "# # False discovery rate\n",
        "# FDR = FP/(TP+FP)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAkh5YSJaZej",
        "colab_type": "text"
      },
      "source": [
        "Approach 2: Take slices of dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weKws3DBaYHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Slice code for minority example\n",
        "\n",
        "# Setup slices of the dataset\n",
        "is_black = df_3_adult[\"Race\"].isin([\"Black\"])\n",
        "is_white = df_3_adult[\"Race\"].isin([\"White\"])\n",
        "is_female = df_3_adult[\"Sex\"].isin([\"Female\"])\n",
        "is_male = df_3_adult[\"Sex\"].isin([\"Male\"])\n",
        "\n",
        "# Create filtered version of the dataset\n",
        "# Minority group\n",
        "df_3_adult_black = df_3_adult[is_black]\n",
        "df_3_adult_female = df_3_adult[is_female]\n",
        "# Majority group\n",
        "df_3_adult_white = df_3_adult[is_white]\n",
        "df_3_adult_male = df_3_adult[is_male]\n",
        "\n",
        "\n",
        "# Dummy coding for slice\n",
        "\n",
        "df_3_adult_black_dummies = pd.get_dummies(df_3_adult_black)\n",
        "\n",
        "# Features of complete dataset\n",
        "print(df_3_adult.columns)\n",
        "\n",
        "# Input features\n",
        "df_3_adult_black_input = df_3_adult_black_dummies.drop(columns=[\"Over-50K\"])\n",
        "print(df_3_adult_black_input.columns)\n",
        "\n",
        "# Target feature\n",
        "df_3_adult_black_label = df_3_adult_black_dummies[\"Over-50K\"]\n",
        "print(df_3_adult_black_label)\n",
        "\n",
        "# Learning Curve for Slice \n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define arguments\n",
        "f1 = make_scorer(f1_score) # theoretically, set (zero_division=1)\n",
        "random_forest = RandomForestClassifier(n_estimators = 100, max_leaf_nodes = 12)\n",
        "sizes_minority = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300,\n",
        "                  350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750,\n",
        "                  2000, 2250, 2500, 2750] \n",
        "\n",
        "# Plot actual learning curve\n",
        "plot_learning_curve(estimator = random_forest, \n",
        "                    title = \"Random Forest Learning Curve\", \n",
        "                    X = df_3_adult_black_input, y = df_3_adult_black_label, \n",
        "                    cv = 10, \n",
        "                    scoring = f1, \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = sizes_minority)\n",
        "\n",
        "from matplotlib.pyplot import figure\n",
        "# plt.figure(num=None, figsize=(10, 8), dpi=80, facecolor='w', edgecolor='k')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F41F18rCYhL1",
        "colab_type": "text"
      },
      "source": [
        "Fairness Metric per Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0dZ5qd1YrYS",
        "colab_type": "text"
      },
      "source": [
        "https://nbviewer.jupyter.org/github/IBM/AIF360/blob/master/examples/tutorial_credit_scoring.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0E7etjXYgGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load all necessary packages\n",
        "import sys\n",
        "sys.path.insert(1, \"../\")  \n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "from aif360.datasets import GermanDataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# Define protected class\n",
        "dataset_orig = df_3_adult(\n",
        "    protected_attribute_names=['Race'],           \n",
        "    privileged_classes=\"White\",     \n",
        "    features_to_drop=['personal_status', 'sex'] \n",
        ")\n",
        "\n",
        "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
        "\n",
        "privileged_groups = [{'Race': \"White\"}]\n",
        "unprivileged_groups = [{'Race': \"Black\"}]\n",
        "\n",
        "\n",
        "# Convert dataset into aif360 adequate form\n",
        "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "display(Markdown(\"#### Original training dataset\"))\n",
        "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6XdytLDYXUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(regr.predict(diabetes_X_test))\n",
        "\n",
        "rfc_model_3 = RandomForestClassifier(n_estimators=200)\n",
        "rfc_model_3.predict(X_test)\n",
        "\n",
        "X_test['survived'] = rfc_model_3.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKx-nFqaTvsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inspect original Learning Curve function\n",
        "import inspect\n",
        "from sklearn.model_selection import learning_curve\n",
        "lines = inspect.getsource(learning_curve)\n",
        "print(lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOolonVAnCke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Function to get TPR, FPR, usw.\n",
        "\n",
        "# # https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
        "\n",
        "# def perf_measures(y_actual, y_hat): # Two lists: original outcomes and prediction outcomes\n",
        "#     TP = 0\n",
        "#     FP = 0\n",
        "#     TN = 0\n",
        "#     FN = 0\n",
        "\n",
        "#     for i in range(len(y_hat)): \n",
        "#         if y_actual[i]==y_hat[i]==1:\n",
        "#            TP += 1\n",
        "#         if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
        "#            FP += 1\n",
        "#         if y_actual[i]==y_hat[i]==0:\n",
        "#            TN += 1\n",
        "#         if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
        "#            FN += 1\n",
        "\n",
        "#     # Calculation of confusion matrix rates\n",
        "#     # Sensitivity, hit rate, recall, or true positive rate\n",
        "#     TPR = TP/(TP+FN)\n",
        "#     # Specificity or true negative rate\n",
        "#     TNR = TN/(TN+FP) \n",
        "#     # Precision or positive predictive value\n",
        "#     PPV = TP/(TP+FP)\n",
        "#     # Negative predictive value\n",
        "#     NPV = TN/(TN+FN)\n",
        "#     # Fall out or false positive rate\n",
        "#     FPR = FP/(FP+TN)\n",
        "#     # False negative rate\n",
        "#     FNR = FN/(TP+FN)\n",
        "#     # False discovery rate\n",
        "#     FDR = FP/(TP+FP)\n",
        "\n",
        "#     return TPR, FPR, TNR, FNR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTYvv_KIktiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Deprecated CONFUSION MATRIX RATES\n",
        "\n",
        "# tpr_min_list = []\n",
        "# tpr_min = get_tpr(df_check_black[\"Over-50K\"], df_check_black[\"y_pred\"])\n",
        "# tpr_min_list.append(tpr_min)\n",
        "\n",
        "# tpr_min_list = [] \n",
        "# fpr_min_list = []\n",
        "# tnr_min_list = []\n",
        "# fnr_min_list = []\n",
        "# tpr_min, fpr_min, tnr_min, fnr_min = perf_measures(df_check_black[\"Over-50K\"], df_check_black[\"y_pred\"])\n",
        "# tpr_min_list.append(tpr_min)\n",
        "# fpr_min_list.append(fpr_min)\n",
        "# tnr_min_list.append(tnr_min)\n",
        "# fnr_min_list.append(fnr_min)\n",
        "\n",
        "# tpr_maj_list = []\n",
        "# fpr_maj_list = []\n",
        "# tnr_maj_list = []\n",
        "# fnr_maj_list = []\n",
        "# tpr_maj, fpr_maj, tnr_maj, fnr_maj = perf_measures(df_check_white[\"Over-50K\"], df_check_white[\"y_pred\"])\n",
        "# tpr_maj_list.append(tpr_maj)\n",
        "# fpr_maj_list.append(fpr_maj)\n",
        "# tnr_maj_list.append(tnr_maj)\n",
        "# fnr_maj_list.append(fnr_maj)\n",
        "\n",
        "#  \"tpr_majority\": tpr_maj_list\n",
        "#  \"tpr_minority\": tpr_min_list\n",
        "#  \"fpr_majority\": fpr_maj_list\n",
        "#  \"fpr_minority\": fpr_min_list\n",
        "#  \"tnr_majority\": tnr_maj_list\n",
        "#  \"tnr_minority\": tnr_min_list\n",
        "#  \"fnr_majority\": fnr_maj_list\n",
        "#  \"fnr_minority\": fnr_min_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUuG9uvV5U19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def create_datasets_new(min_data: pd.DataFrame, maj_data: pd.DataFrame, training_sizes: list):\n",
        "#     datasets = []\n",
        "#     for training_size in training_sizes:\n",
        "#         \n",
        "#             dataset_min_sample = min_data.sample(n=training_size, random_state=1)\n",
        "#             dataset = pd.concat((dataset_min_sample, maj_data))\n",
        "#             datasets.append(dataset)\n",
        "#     return datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_FwySPCFNK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import inspect\n",
        "lines = inspect.getsource(learning_curve)\n",
        "print(lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTUaC5dzFQcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learning_curve(estimator, X, y, groups=None,\n",
        "                   train_sizes=np.linspace(0.1, 1.0, 5), cv=None,\n",
        "                   scoring=None, exploit_incremental_learning=False,\n",
        "                   n_jobs=None, pre_dispatch=\"all\", verbose=0, shuffle=False,\n",
        "                   random_state=None, error_score=np.nan, return_times=False):\n",
        "    \"\"\"Learning curve.\n",
        "\n",
        "    Determines cross-validated training and test scores for different training\n",
        "    set sizes.\n",
        "\n",
        "    A cross-validation generator splits the whole dataset k times in training\n",
        "    and test data. Subsets of the training set with varying sizes will be used\n",
        "    to train the estimator and a score for each training subset size and the\n",
        "    test set will be computed. Afterwards, the scores will be averaged over\n",
        "    all k runs for each training subset size.\n",
        "\n",
        "    Read more in the :ref:`User Guide <learning_curve>`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    groups : array-like, with shape (n_samples,), optional\n",
        "        Group labels for the samples used while splitting the dataset into\n",
        "        train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
        "        instance (e.g., :class:`GroupKFold`).\n",
        "\n",
        "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
        "        Relative or absolute numbers of training examples that will be used to\n",
        "        generate the learning curve. If the dtype is float, it is regarded as a\n",
        "        fraction of the maximum size of the training set (that is determined\n",
        "        by the selected validation method), i.e. it has to be within (0, 1].\n",
        "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
        "        Note that for classification the number of samples usually have to\n",
        "        be big enough to contain at least one sample from each class.\n",
        "        (default: np.linspace(0.1, 1.0, 5))\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "\n",
        "        - None, to use the default 5-fold cross validation,\n",
        "        - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
        "        - :term:`CV splitter`,\n",
        "        - An iterable yielding (train, test) splits as arrays of indices.\n",
        "\n",
        "        For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
        "        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
        "        other cases, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validation strategies that can be used here.\n",
        "\n",
        "        .. versionchanged:: 0.22\n",
        "            ``cv`` default value if None changed from 3-fold to 5-fold.\n",
        "\n",
        "    scoring : string, callable or None, optional, default: None\n",
        "        A string (see model evaluation documentation) or\n",
        "        a scorer callable object / function with signature\n",
        "        ``scorer(estimator, X, y)``.\n",
        "\n",
        "    exploit_incremental_learning : boolean, optional, default: False\n",
        "        If the estimator supports incremental learning, this will be\n",
        "        used to speed up fitting for different training set sizes.\n",
        "\n",
        "    n_jobs : int or None, optional (default=None)\n",
        "        Number of jobs to run in parallel.\n",
        "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
        "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
        "        for more details.\n",
        "\n",
        "    pre_dispatch : integer or string, optional\n",
        "        Number of predispatched jobs for parallel execution (default is\n",
        "        all). The option can reduce the allocated memory. The string can\n",
        "        be an expression like '2*n_jobs'.\n",
        "\n",
        "    verbose : integer, optional\n",
        "        Controls the verbosity: the higher, the more messages.\n",
        "\n",
        "    shuffle : boolean, optional\n",
        "        Whether to shuffle training data before taking prefixes of it\n",
        "        based on``train_sizes``.\n",
        "\n",
        "    random_state : int, RandomState instance or None, optional (default=None)\n",
        "        If int, random_state is the seed used by the random number generator;\n",
        "        If RandomState instance, random_state is the random number generator;\n",
        "        If None, the random number generator is the RandomState instance used\n",
        "        by `np.random`. Used when ``shuffle`` is True.\n",
        "\n",
        "    error_score : 'raise' or numeric\n",
        "        Value to assign to the score if an error occurs in estimator fitting.\n",
        "        If set to 'raise', the error is raised.\n",
        "        If a numeric value is given, FitFailedWarning is raised. This parameter\n",
        "        does not affect the refit step, which will always raise the error.\n",
        "\n",
        "    return_times : boolean, optional (default: False)\n",
        "        Whether to return the fit and score times.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    train_sizes_abs : array, shape (n_unique_ticks,), dtype int\n",
        "        Numbers of training examples that has been used to generate the\n",
        "        learning curve. Note that the number of ticks might be less\n",
        "        than n_ticks because duplicate entries will be removed.\n",
        "\n",
        "    train_scores : array, shape (n_ticks, n_cv_folds)\n",
        "        Scores on training sets.\n",
        "\n",
        "    test_scores : array, shape (n_ticks, n_cv_folds)\n",
        "        Scores on test set.\n",
        "\n",
        "    fit_times : array, shape (n_ticks, n_cv_folds)\n",
        "        Times spent for fitting in seconds. Only present if ``return_times``\n",
        "        is True.\n",
        "\n",
        "    score_times : array, shape (n_ticks, n_cv_folds)\n",
        "        Times spent for scoring in seconds. Only present if ``return_times``\n",
        "        is True.\n",
        "    \"\"\"\n",
        "    \n",
        "    if exploit_incremental_learning and not hasattr(estimator, \"partial_fit\"):\n",
        "        raise ValueError(\"An estimator must support the partial_fit interface \"\n",
        "                         \"to exploit incremental learning\")\n",
        "    X, y, groups = indexable(X, y, groups)\n",
        "\n",
        "    cv = check_cv(cv, y, classifier=is_classifier(estimator))\n",
        "    # Store it as list as we will be iterating over the list multiple times\n",
        "    cv_iter = list(cv.split(X, y, groups))\n",
        "\n",
        "    scorer = check_scoring(estimator, scoring=scoring)\n",
        "\n",
        "    n_max_training_samples = len(cv_iter[0][0])\n",
        "    # Because the lengths of folds can be significantly different, it is\n",
        "    # not guaranteed that we use all of the available training data when we\n",
        "    # use the first 'n_max_training_samples' samples.\n",
        "    train_sizes_abs = _translate_train_sizes(train_sizes,\n",
        "                                             n_max_training_samples)\n",
        "    n_unique_ticks = train_sizes_abs.shape[0]\n",
        "    if verbose > 0:\n",
        "        print(\"[learning_curve] Training set sizes: \" + str(train_sizes_abs))\n",
        "\n",
        "    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,\n",
        "                        verbose=verbose)\n",
        "\n",
        "    if shuffle:\n",
        "        rng = check_random_state(random_state)\n",
        "        cv_iter = ((rng.permutation(train), test) for train, test in cv_iter)\n",
        "\n",
        "    if exploit_incremental_learning:\n",
        "        classes = np.unique(y) if is_classifier(estimator) else None\n",
        "        out = parallel(delayed(_incremental_fit_estimator)(\n",
        "            clone(estimator), X, y, classes, train, test, train_sizes_abs,\n",
        "            scorer, verbose, return_times) for train, test in cv_iter)\n",
        "    else:\n",
        "        train_test_proportions = []\n",
        "        for train, test in cv_iter:\n",
        "            for n_train_samples in train_sizes_abs:\n",
        "                train_test_proportions.append((train[:n_train_samples], test))\n",
        "\n",
        "        out = parallel(delayed(_fit_and_score)(\n",
        "            clone(estimator), X, y, scorer, train, test, verbose,\n",
        "            parameters=None, fit_params=None, return_train_score=True,\n",
        "            error_score=error_score, return_times=return_times)\n",
        "            for train, test in train_test_proportions)\n",
        "        out = np.array(out)\n",
        "        n_cv_folds = out.shape[0] // n_unique_ticks\n",
        "        dim = 4 if return_times else 2\n",
        "        out = out.reshape(n_cv_folds, n_unique_ticks, dim)\n",
        "\n",
        "    out = np.asarray(out).transpose((2, 1, 0))\n",
        "\n",
        "    ret = train_sizes_abs, out[0], out[1]\n",
        "\n",
        "    if return_times:\n",
        "        ret = ret + (out[2], out[3])\n",
        "\n",
        "    return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqnx5tPkCpWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# min_dataset_list = list()\n",
        "# compl_dataset_list = list()\n",
        "\n",
        "# is_black = df_3_adult[\"Race\"].isin([\"Black\"])\n",
        "# is_white = df_3_adult[\"Race\"].isin([\"White\"])\n",
        "# def create_datasets(min_data, maj_data, training_sizes):\n",
        "#     for training_size in training_sizes:\n",
        "#         while len(min_data.index) >= training_size: # Code should stop when number of rows of df with min. group is smaller than training size iteration\n",
        "#             dataset_min_slice = min_data.sample(n = training_size, random_state = 1) # Get a random subset of the minority group\n",
        "#             min_dataset_list.append(dataset_min_slice) # Create list of data frames that include observations of minority group of different sizes\n",
        "#             for min_dataset_component in min_dataset_list:\n",
        "#                 dataset_list = maj_data.append(min_dataset_component) # Merge observations of min. group of different sizes with observations from maj. group\n",
        "#                 compl_dataset_list.append(dataset_list) # Create list of data frames that include majority group (fixed size) and minority group (different sizes)\n",
        "#                 return compl_dataset_list\n",
        "\n",
        "# training_sizes_2 = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, \n",
        "                      300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750]\n",
        "\n",
        "# list_dfs_2 = create_datasets_new(min_data = df_3_adult_black, maj_data = df_3_adult_white, training_sizes = training_sizes_2)\n",
        "# print(len(list_dfs_2))\n",
        "# print([df.shape[0] for df in list_dfs_2])\n",
        "\n",
        "# training_sizes_3 =  [2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000, 4250, 4500]\n",
        "\n",
        "# list_dfs_3 = create_datasets_new(min_data = df_3_adult_black, maj_data = df_3_adult_white, training_sizes = training_sizes_3)\n",
        "# print(len(list_dfs_3))\n",
        "# print([df.shape[0] for df in list_dfs_3])\n",
        "\n",
        "# training_sizes_4 = [4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000, 20000, 25000, 30000]\n",
        "\n",
        "# list_dfs_4 = create_datasets_new(min_data = df_3_adult_black, maj_data = df_3_adult_white, training_sizes = training_sizes_4)\n",
        "# print(len(list_dfs_4))\n",
        "# print([df.shape[0] for df in list_dfs_4])\n",
        "\n",
        "# # Execute function\n",
        "# training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300,\n",
        "#                   350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750,\n",
        "#                   2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000, 4250, 4500,\n",
        "#                   4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000, 20000, 25000, 30000]\n",
        "\n",
        "# df_3_adult_black = df_3_adult[is_black]  # Define minority group based on original data frame\n",
        "# df_3_adult_white = df_3_adult[is_white]  # Define majority group based on original data frame\n",
        "\n",
        "# list_dfs = create_datasets_new(min_data = df_3_adult_black, maj_data = df_3_adult_white, training_sizes = training_sizes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfhAaGcytjPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_3_adult_train_input\n",
        "# df_3_adult_train_label\n",
        "\n",
        "# # Row bind\n",
        "# df1.append(df2)\n",
        "\n",
        "# def get_min_datasets(data, ):\n",
        "#   for specific_size in min_sizes:\n",
        "#     if n_row(dataset) >= specific_size:\n",
        "#     elif n_row(dataset) < specific_size:\n",
        "#       stop\n",
        "#     dataset_min_slice = training_examples.sample(frac=1) # shuffle dataset with minority group randomely before slicing\n",
        "#     dataset_min_slice = dataset_min_slice.slice[min_sizes]\n",
        "#     diff_min_datasets.append(dataset_min) #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb_a7-0Dg1bS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ggf. feature importance für jede Iteration auch noch berechnen und dann ans DF anhängen\n",
        "\n",
        "# Sample Code\n",
        "\n",
        "# neighbors_list = [3,5,10,20,50,75]      # use either the one or the other neighbors_list \n",
        "# neighbors_list = list(range(5,500, 5))\n",
        "# print(np.linspace(1,2,5))\n",
        "\n",
        "# accuracy_list = []\n",
        "# for test_number in neighbors_list:\n",
        "#   model = KNeighborsClassifier(n_neighbors=test_number)\n",
        "#   predictions = model.fit(X_train, y_train).predict(X_test)\n",
        "#   accuracy = accuracy_score(y_test, predictions)\n",
        "#   accuracy_list.append(accuracy)\n",
        "\n",
        "  # Because I will be working with k-fold cv, most likely I will need to take the average \n",
        "  # and define the standard deviation (which then can also be shown in the plot as bands)\n",
        "\n",
        "  # Important: Get score both training and testing \n",
        "\n",
        "# results_df = pd.DataFrame({'neighbors':neighbors_list, 'accuracy':accuracy_list})\n",
        "# print(np.linspace(1,2,5))\n",
        "\n",
        "# Matplotlib plotting code -> Alternatively, use plotly\n",
        "\n",
        "# plt.plot(results_df['neighbors'],\n",
        "# results_df['accuracy'])\n",
        "# # Add the labels and title\n",
        "# plt.gca().set(xlabel='n_neighbors', ylabel='Accuracy',\n",
        "# title='Accuracy for different n_neighbors')\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# Dataframe \n",
        "\n",
        "# results_df = pd.DataFrame(results_list, columns=['learning_rate', 'max_depth', 'accuracy'])\n",
        "# print(results_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZywZ_O294Rcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# II. Model setup\n",
        "# a) Linear\n",
        "\n",
        "# b) Nonlinear\n",
        "# K-NN\n",
        "# knn = KNeighborsClassifier(n_neighbors=6) # To Do: Check appropriatness of hyperparameter\n",
        "# knn.fit(df_3_adult_train_input, df_3_adult_train_label) \n",
        "# To Do: Probably wrong because whole dataset and not only training set is used as input training\n",
        "\n",
        "# c) Others\n",
        "# Random Forest\n",
        "# random_forest = RandomForestClassifier(n_estimators = 500, max_leaf_nodes = 16, n_jobs =-1)\n",
        "#  random_forest.fit(df_3_adult_train_input, df_3_adult_train_label)\n",
        "# To Do: Probably wrong because whole dataset and not only training set is used as input training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLHQERS8pV32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute train/test split.\n",
        "\n",
        "# Check: Probably not necessary \n",
        "\n",
        "# Execute Train/test split\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df_3_adult_train_input, \n",
        "#                                                     df_3_adult_train_label, \n",
        "#                                                     test_size=0.3, \n",
        "#                                                     random_state=21) # seed for random number generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYMoVDkicTDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot Alternative 2\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Male', x=df_3_adult_aggr[\"Race\"], y=df_3_adult_aggr[\"counting\"]),\n",
        "    go.Bar(name='Female', x=df_3_adult_aggr[\"Sex\"], y=df_3_adult_aggr[\"counting\"])\n",
        "])\n",
        "# Change the bar mode\n",
        "fig.update_layout(barmode='group')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91Yvu-jJcado",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot Alternative 1\n",
        "# fig = px.bar(df_3_adult_aggr, x=\"Race\", y=\"counting\", color='Sex', barmode='group')\n",
        "# fig.show()\n",
        "\n",
        "# Hint: Does not work properly.\n",
        "\n",
        "# Plot Alternative 2\n",
        "# import plotly.graph_objects as go\n",
        "\n",
        "# fig = go.Figure(data=[\n",
        "#     go.Bar(name='Male', x=df_3_adult_aggr[\"Race\"], y=df_3_adult_aggr[\"counting\"]),\n",
        "#     go.Bar(name='Female', x=df_3_adult_aggr[\"Sex\"], y=df_3_adult_aggr[\"counting\"])\n",
        "# ])\n",
        "# # Change the bar mode\n",
        "# fig.update_layout(barmode='group')\n",
        "# fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q7546kms4na",
        "colab_type": "text"
      },
      "source": [
        "## Learning Curve Variants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOPJKpqPtfVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Learning Curve Function\n",
        "\n",
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    \"\"\"\n",
        "    Generate 3 plots: the test and training learning curve, the training\n",
        "    samples vs fit times curve, the fit times vs score curve.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    title : string\n",
        "        Title for the chart.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    axes : array of 3 axes, optional (default=None)\n",
        "        Axes to use for plotting the curves.\n",
        "\n",
        "    ylim : tuple, shape (ymin, ymax), optional\n",
        "        Defines minimum and maximum yvalues plotted.\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "          - None, to use the default 5-fold cross-validation,\n",
        "          - integer, to specify the number of folds.\n",
        "          - :term:`CV splitter`,\n",
        "          - An iterable yielding (train, test) splits as arrays of indices.\n",
        "\n",
        "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
        "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
        "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validators that can be used here.\n",
        "\n",
        "    n_jobs : int or None, optional (default=None)\n",
        "        Number of jobs to run in parallel.\n",
        "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
        "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
        "        for more details.\n",
        "\n",
        "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
        "        Relative or absolute numbers of training examples that will be used to\n",
        "        generate the learning curve. If the dtype is float, it is regarded as a\n",
        "        fraction of the maximum size of the training set (that is determined\n",
        "        by the selected validation method), i.e. it has to be within (0, 1].\n",
        "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
        "        Note that for classification the number of samples usually have to\n",
        "        be big enough to contain at least one sample from each class.\n",
        "        (default: np.linspace(0.1, 1.0, 5))\n",
        "    \"\"\"\n",
        "    if axes is None:\n",
        "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "    axes[0].set_title(title)\n",
        "    if ylim is not None:\n",
        "        axes[0].set_ylim(*ylim)\n",
        "    axes[0].set_xlabel(\"Training examples\")\n",
        "    axes[0].set_ylabel(\"Score\")\n",
        "\n",
        "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
        "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
        "                       train_sizes=train_sizes,\n",
        "                       return_times=True)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    fit_times_mean = np.mean(fit_times, axis=1)\n",
        "    fit_times_std = np.std(fit_times, axis=1)\n",
        "\n",
        "    # Plot learning curve\n",
        "    axes[0].grid()\n",
        "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                         color=\"r\")\n",
        "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
        "                         color=\"g\")\n",
        "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "                 label=\"Training score\")\n",
        "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "                 label=\"Cross-validation score\")\n",
        "    axes[0].legend(loc=\"best\")\n",
        "\n",
        "    # Plot n_samples vs fit_times\n",
        "    axes[1].grid()\n",
        "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
        "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
        "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
        "    axes[1].set_xlabel(\"Training examples\")\n",
        "    axes[1].set_ylabel(\"fit_times\")\n",
        "    axes[1].set_title(\"Scalability of the model\")\n",
        "\n",
        "    # Plot fit_time vs score\n",
        "    axes[2].grid()\n",
        "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
        "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
        "    axes[2].set_xlabel(\"fit_times\")\n",
        "    axes[2].set_ylabel(\"Score\")\n",
        "    axes[2].set_title(\"Performance of the model\")\n",
        "\n",
        "    return plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POhGUR1hti1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_learning_curve(estimator = random_forest, title = \"Random Forest Learning Curve\", \n",
        "                    X = df_3_adult_train_input, y = df_3_adult_train_label, \n",
        "                    axes=None, ylim=None, cv=10, train_sizes=sizes)\n",
        " \n",
        "# models = []\n",
        "\n",
        "# for model in models:\n",
        "#   plot_learning_curve(estimator=model, title=\"k-nn Learning Curve\", X = df_3_adult_train_input, y = df_3_adult_train_label, \n",
        "#                     axes=None, ylim=None, cv=10,\n",
        "#                     n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pzmnhnP22Je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install yellowbrick"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_Pq7eJW2xjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.scikit-yb.org/en/latest/api/model_selection/learning_curve.html\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "\n",
        "from yellowbrick.datasets import load_game\n",
        "from yellowbrick.model_selection import LearningCurve\n",
        "\n",
        "# Encode the categorical data\n",
        "X = df_3_adult_train_input\n",
        "y = df_3_adult_train_label\n",
        "\n",
        "# Create the learning curve visualizer\n",
        "cv = StratifiedKFold(n_splits=12)\n",
        "# sizes = np.linspace(0.1, 1.0, 10)\n",
        "sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300,\n",
        "             350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750,\n",
        "             2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000, 4250, 4500,\n",
        "             4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Instantiate the classification model and visualizer\n",
        "visualizer = LearningCurve(\n",
        "    model = random_forest, cv=cv, scoring='f1_weighted', train_sizes=sizes, n_jobs=4\n",
        ")\n",
        "\n",
        "visualizer.fit(X, y)        # Fit the data to the visualizer\n",
        "visualizer.show()           # Finalize and render the figure"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24uAPV7p-tja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.dataquest.io/blog/learning-curves-machine-learning/\n",
        "\n",
        "# train_sizes = [1, 100, 500, 1000, 1500, 2000, 2500, 3000, 5000, 7500, 10000]\n",
        "\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "# features = df_3_adult_train_input.columns\n",
        "# target = df_3_adult_train_label\n",
        "\n",
        "train_sizes, train_scores, validation_scores = learning_curve(estimator = random_forest, \n",
        "                                                              X = df_3_adult_train_input, \n",
        "                                                              y = df_3_adult_train_label, \n",
        "                                                              cv = 5)\n",
        "\n",
        "train_scores_mean = train_scores.mean(axis = 1)\n",
        "validation_scores_mean = validation_scores.mean(axis = 1)\n",
        "print('Mean training scores\\n\\n', pd.Series(train_scores_mean, index = train_sizes))\n",
        "print('\\n', '-' * 20) # separator\n",
        "print('\\nMean validation scores\\n\\n',pd.Series(validation_scores_mean, index = train_sizes))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "plt.plot(train_sizes, train_scores_mean, label = 'Training error')\n",
        "plt.plot(train_sizes, validation_scores_mean, label = 'Validation error')\n",
        "plt.ylabel('Score', fontsize = 14)\n",
        "plt.xlabel('Training set size', fontsize = 14)\n",
        "plt.title('Learning curves for a knn model', fontsize = 18, y = 1.03)\n",
        "plt.legend()\n",
        "plt.ylim(0,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxOS8Pdw-z6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Bundling our previous work into a function ###\n",
        "\n",
        "def learning_curves(estimator, data, features, target, train_sizes, cv, scoring):\n",
        "    train_sizes, train_scores, validation_scores = learning_curve(\n",
        "        estimator, data[features], data[target], train_sizes = train_sizes,\n",
        "        cv = cv, scoring = scoring)\n",
        "    \n",
        "    train_scores_mean = train_scores.mean(axis = 1)\n",
        "    validation_scores_mean = validation_scores.mean(axis = 1)\n",
        "\n",
        "    plt.plot(train_sizes, train_scores_mean, label = 'Training error')\n",
        "    plt.plot(train_sizes, validation_scores_mean, label = 'Validation error')\n",
        "\n",
        "    plt.ylabel('Score', fontsize = 14)\n",
        "    plt.xlabel('Training set size', fontsize = 14)\n",
        "    title = 'Learning curves for a ' + str(estimator).split('(')[0] + ' model'\n",
        "    plt.title(title, fontsize = 18, y = 1.03)\n",
        "    plt.legend()\n",
        "    plt.ylim(0,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgoEC9cb-27p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Plotting the two learning curves ###\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "plt.figure(figsize = (16,5))\n",
        "\n",
        "for model, i in [(RandomForestClassifier(), 1)]:\n",
        "    plt.subplot(1,2,i)\n",
        "    learning_curves(estimator = random_forest, \n",
        "                    data = df_3_adult_dummies, \n",
        "                    features = df_3_adult_train_input.columns, \n",
        "                    target= \"Over-50K\", \n",
        "                    train_sizes = sizes,\n",
        "                    scoring = f1, \n",
        "                    cv= 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTO7VG99DTcp",
        "colab_type": "text"
      },
      "source": [
        "### What-If Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGGOb9ACbyEZ",
        "colab_type": "text"
      },
      "source": [
        "Guide: https://colab.research.google.com/github/pair-code/what-if-tool/blob/master/xgboost_caip.ipynb?hl=de\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ5JGFSwCCMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install current tensorflow version\n",
        "# To determine which version you're using:\n",
        "!pip show tensorflow\n",
        "\n",
        "# For the current version: \n",
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JlJ_VWqARH_X",
        "colab": {}
      },
      "source": [
        "# Install the What-If Tool\n",
        "try:\n",
        "  import google.colab\n",
        "  !pip install --upgrade witwidget\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoYDH1TSBqO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Helper Functions for the What-If tool\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import witwidget\n",
        "\n",
        "from witwidget.notebook.visualization import WitWidget, WitConfigBuilder\n",
        "\n",
        "import tensorflow as tf\n",
        "import functools\n",
        "\n",
        "# Creates a tf feature spec from the dataframe and columns specified.\n",
        "def create_feature_spec(df, columns=None):\n",
        "    feature_spec = {}\n",
        "    if columns == None:\n",
        "        columns = df.columns.values.tolist()\n",
        "    for f in columns:\n",
        "        if df[f].dtype is np.dtype(np.int64):\n",
        "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.int64)\n",
        "        elif df[f].dtype is np.dtype(np.float64):\n",
        "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.float32)\n",
        "        else:\n",
        "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.string)\n",
        "    return feature_spec\n",
        "\n",
        "# Creates simple numeric and categorical feature columns from a feature spec and a\n",
        "# list of columns from that spec to use.\n",
        "#\n",
        "# NOTE: Models might perform better with some feature engineering such as bucketed\n",
        "# numeric columns and hash-bucket/embedding columns for categorical features.\n",
        "def create_feature_columns(columns, feature_spec):\n",
        "    ret = []\n",
        "    for col in columns:\n",
        "        if feature_spec[col].dtype is tf.int64 or feature_spec[col].dtype is tf.float32:\n",
        "            ret.append(tf.feature_column.numeric_column(col))\n",
        "        else:\n",
        "            ret.append(tf.feature_column.indicator_column(\n",
        "                tf.feature_column.categorical_column_with_vocabulary_list(col, list(df[col].unique()))))\n",
        "    return ret\n",
        "\n",
        "# An input function for providing input to a model from tf.Examples\n",
        "def tfexamples_input_fn(examples, feature_spec, label, mode=tf.estimator.ModeKeys.EVAL,\n",
        "                       num_epochs=None, \n",
        "                       batch_size=64):\n",
        "    def ex_generator():\n",
        "        for i in range(len(examples)):\n",
        "            yield examples[i].SerializeToString()\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "      ex_generator, tf.dtypes.string, tf.TensorShape([]))\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(lambda tf_example: parse_tf_example(tf_example, label, feature_spec))\n",
        "    dataset = dataset.repeat(num_epochs)\n",
        "    return dataset\n",
        "\n",
        "# Parses Tf.Example protos into features for the input function.\n",
        "def parse_tf_example(example_proto, label, feature_spec):\n",
        "    parsed_features = tf.io.parse_example(serialized=example_proto, features=feature_spec)\n",
        "    target = parsed_features.pop(label)\n",
        "    return parsed_features, target\n",
        "\n",
        "# Converts a dataframe into a list of tf.Example protos.\n",
        "def df_to_examples(df, columns=None):\n",
        "    examples = []\n",
        "    if columns == None:\n",
        "        columns = df.columns.values.tolist()\n",
        "    for index, row in df.iterrows():\n",
        "        example = tf.train.Example()\n",
        "        for col in columns:\n",
        "            if df[col].dtype is np.dtype(np.int64):\n",
        "                example.features.feature[col].int64_list.value.append(int(row[col]))\n",
        "            elif df[col].dtype is np.dtype(np.float64):\n",
        "                example.features.feature[col].float_list.value.append(row[col])\n",
        "            elif row[col] == row[col]:\n",
        "                example.features.feature[col].bytes_list.value.append(row[col].encode('utf-8'))\n",
        "        examples.append(example)\n",
        "    return examples\n",
        "\n",
        "# Converts a dataframe column into a column of 0's and 1's based on the provided test.\n",
        "# Used to force label columns to be numeric for binary classification using a TF estimator.\n",
        "def make_label_column_numeric(df, label_column, test):\n",
        "  df[label_column] = np.where(test(df[label_column]), 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuZMXEnMdNe5",
        "colab_type": "text"
      },
      "source": [
        "**Specify feature and label columns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhbMGWyF0EcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set the column in the dataset you wish for the model to predict\n",
        "label_column = 'Over-50K'\n",
        "\n",
        "# Set list of all columns from the dataset we will use for model input.\n",
        "input_features = ['Age', 'Workclass', 'fnlwgt', 'Education', 'Education-Num',\n",
        "                  'Marital-Status', 'Occupation', 'Relationship', 'Race', 'Sex',\n",
        "                  'Capital-Gain', 'Capital-Loss', 'Hours-per-week', 'Country']\n",
        "\n",
        "# Create a list containing all input features and the label column\n",
        "features_and_labels = input_features + [label_column]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQm_vccX1Y4V",
        "colab_type": "text"
      },
      "source": [
        "**Convert dataset to tf.Example protos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyjPGPeI1asw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "examples = df_to_examples(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlBwOj9R3PZw",
        "colab_type": "text"
      },
      "source": [
        "**Create and train the classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlJH-6Tf2t5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_steps = 200  #@param {type: \"number\"}\n",
        "\n",
        "# Create a feature spec for the classifier\n",
        "feature_spec = create_feature_spec(df_what_if, features_and_labels)\n",
        "\n",
        "# Define and train the classifier\n",
        "train_inpf = functools.partial(tfexamples_input_fn, examples, feature_spec, label_column)\n",
        "classifier = tf.estimator.LinearClassifier(\n",
        "    feature_columns=create_feature_columns(input_features, feature_spec))\n",
        "classifier.train(train_inpf, steps=num_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnHVwhOw3ORW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Invoke What-If Tool for test data and the trained models {display-mode: \"form\"}\n",
        "\n",
        "num_datapoints = 2000  #@param {type: \"number\"}\n",
        "tool_height_in_px = 1000  #@param {type: \"number\"}\n",
        "\n",
        "from witwidget.notebook.visualization import WitConfigBuilder\n",
        "from witwidget.notebook.visualization import WitWidget\n",
        "\n",
        "# Load up the test dataset\n",
        "test_csv_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
        "test_df = pd.read_csv(test_csv_path, names=csv_columns, skipinitialspace=True,\n",
        "  skiprows=1)\n",
        "make_label_column_numeric(test_df, label_column, lambda val: val == '>50K.')\n",
        "test_examples = df_to_examples(test_df[0:num_datapoints])\n",
        "\n",
        "# Setup the tool with the test examples and the trained classifier\n",
        "config_builder = WitConfigBuilder(test_examples[0:num_datapoints]).set_estimator_and_feature_spec(\n",
        "    classifier, feature_spec).set_label_vocab(['Under 50K', 'Over 50K'])\n",
        "a = WitWidget(config_builder, height=tool_height_in_px)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}